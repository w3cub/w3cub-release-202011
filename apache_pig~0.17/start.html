
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Getting Started - Apache Pig 0.17 - W3cubDocs</title>
  
  <meta name="description" content=" Mandatory ">
  <meta name="keywords" content="getting, started, apache, pig, apache_pig~0.17">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/apache_pig~0.17/start.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/apache_pig~0.17.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/apache_pig~0.17/" class="_nav-link" title="" style="margin-left:0;">Apache Pig 0.17</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _simple">
				
				
<h1>Getting Started</h1> <div id="front-matter"> <div id="minitoc-area"> <ul class="minitoc"> <li> <a href="#Pig+Setup">Pig Setup</a> <ul class="minitoc"> <li> <a href="#req">Requirements</a> </li> <li> <a href="#download">Download Pig</a> </li> <li> <a href="#build">Build Pig</a> </li> </ul> </li> <li> <a href="#run">Running Pig </a> <ul class="minitoc"> <li> <a href="#execution-modes">Execution Modes</a> </li> <li> <a href="#interactive-mode">Interactive Mode</a> </li> <li> <a href="#batch-mode">Batch Mode</a> </li> </ul> </li> <li> <a href="#kerberos">Running jobs on a Kerberos secured cluster</a> <ul class="minitoc"> <li> <a href="#kerberos-short">Short lived jobs</a> </li> <li> <a href="#kerberos-long">Long lived jobs</a> </li> </ul> </li> <li> <a href="#pl-statements">Pig Latin Statements</a> <ul class="minitoc"> <li> <a href="#data-load">Loading Data</a> </li> <li> <a href="#data-work-with">Working with Data</a> </li> <li> <a href="#data-store">Storing Intermediate Results</a> </li> <li> <a href="#data-results">Storing Final Results</a> </li> <li> <a href="#debug">Debugging Pig Latin</a> </li> </ul> </li> <li> <a href="#properties">Pig Properties</a> </li> <li> <a href="#tutorial">Pig Tutorial </a> <ul class="minitoc"> <li> <a href="#Running+the+Pig+Scripts+in+Local+Mode"> Running the Pig Scripts in Local Mode</a> </li> <li> <a href="#Running+the+Pig+Scripts+in+Mapreduce+Mode%2C+Tez+Mode+or+Spark+Mode"> Running the Pig Scripts in Mapreduce Mode, Tez Mode or Spark Mode</a> </li> <li> <a href="#pig-tutorial-files"> Pig Tutorial Files</a> </li> <li> <a href="#pig-script-1"> Pig Script 1: Query Phrase Popularity</a> </li> <li> <a href="#pig-script-2">Pig Script 2: Temporal Query Phrase Popularity</a> </li> </ul> </li> </ul> </div> </div>    <h2 id="Pig+Setup">Pig Setup</h2> <div class="section">  <h3 id="req">Requirements</h3> <p> <strong>Mandatory</strong> </p> <p>Unix and Windows users need the following:</p> <ul> <li> <strong>Hadoop 2.X</strong> - <a href="http://hadoop.apache.org/common/releases.html">http://hadoop.apache.org/common/releases.html</a> (You can run Pig with different versions of Hadoop by setting HADOOP_HOME to point to the directory where you have installed Hadoop. If you do not set HADOOP_HOME, by default Pig will run with the embedded version, currently Hadoop 2.7.3.)</li> <li> <strong>Java 1.7</strong> - <a href="http://java.sun.com/javase/downloads/index.jsp">http://java.sun.com/javase/downloads/index.jsp</a> (set JAVA_HOME to the root of your Java installation)</li> </ul>  <p> <strong>Optional</strong> </p> <ul> <li> <strong>Python 2.7</strong> - <a href="http://jython.org/downloads.html">https://www.python.org</a> (when using Streaming Python UDFs) </li> <li> <strong>Ant 1.8</strong> - <a href="http://ant.apache.org/">http://ant.apache.org/</a> (for builds) </li> </ul>  <h3 id="download">Download Pig</h3> <p>To get a Pig distribution, do the following:</p> <ol> <li>Download a recent stable release from one of the Apache Download Mirrors (see <a href="http://hadoop.apache.org/pig/releases.html"> Pig Releases</a>).</li> <li>Unpack the downloaded Pig distribution, and then note the following: <ul> <li>The Pig script file, pig, is located in the bin directory (/pig-n.n.n/bin/pig). The Pig environment variables are described in the Pig script file.</li> <li>The Pig properties file, pig.properties, is located in the conf directory (/pig-n.n.n/conf/pig.properties). You can specify an alternate location using the PIG_CONF_DIR environment variable.</li> </ul> </li> <li>Add /pig-n.n.n/bin to your path. Use export (bash,sh,ksh) or setenv (tcsh,csh). For example: <br> <span class="codefrag">$ export PATH=/&lt;my-path-to-pig&gt;/pig-n.n.n/bin:$PATH</span> </li> <li> Test the Pig installation with this simple command: <span class="codefrag">$ pig -help</span> </li> </ol>  <h3 id="build">Build Pig</h3> <p>To build pig, do the following:</p> <ol> <li> Check out the Pig code from SVN: <span class="codefrag">svn co http://svn.apache.org/repos/asf/pig/trunk</span> </li> <li> Build the code from the top directory: <span class="codefrag">ant</span> <br> If the build is successful, you should see the pig.jar file created in that directory. </li> <li> Validate the pig.jar by running a unit test: <span class="codefrag">ant test</span> </li> </ol> </div>    <h2 id="run">Running Pig </h2> <div class="section"> <p>You can run Pig (execute Pig Latin statements and Pig commands) using various modes.</p> <table class="ForrestTable"> <tr> <td colspan="1" rowspan="1"></td> <td colspan="1" rowspan="1"><strong>Local Mode</strong></td> <td colspan="1" rowspan="1"><strong>Tez Local Mode</strong></td> <td colspan="1" rowspan="1"><strong>Spark Local Mode</strong></td> <td colspan="1" rowspan="1"><strong>Mapreduce Mode</strong></td> <td colspan="1" rowspan="1"><strong>Tez Mode</strong></td> <td colspan="1" rowspan="1"><strong>Spark Mode</strong></td> </tr> <tr> <td colspan="1" rowspan="1"><strong>Interactive Mode </strong></td> <td colspan="1" rowspan="1">yes</td> <td colspan="1" rowspan="1">experimental</td> <td colspan="1" rowspan="1">yes</td> <td colspan="1" rowspan="1">yes</td> </tr> <tr> <td colspan="1" rowspan="1">
<strong>Batch Mode</strong> </td> <td colspan="1" rowspan="1">yes</td> <td colspan="1" rowspan="1">experimental</td> <td colspan="1" rowspan="1">yes</td> <td colspan="1" rowspan="1">yes</td> </tr> </table>  <h3 id="execution-modes">Execution Modes</h3> <p>Pig has six execution modes or exectypes: </p> <ul> <li> <strong>Local Mode</strong> - To run Pig in local mode, you need access to a single machine; all files are installed and run using your local host and file system. Specify local mode using the -x flag (pig -x local). </li> <li> <strong>Tez Local Mode</strong> - To run Pig in tez local mode. It is similar to local mode, except internally Pig will invoke tez runtime engine. Specify Tez local mode using the -x flag (pig -x tez_local). <p> <strong>Note:</strong> Tez local mode is experimental. There are some queries which just error out on bigger data in local mode.</p> </li> <li> <strong>Spark Local Mode</strong> - To run Pig in spark local mode. It is similar to local mode, except internally Pig will invoke spark runtime engine. Specify Spark local mode using the -x flag (pig -x spark_local). <p> <strong>Note:</strong> Spark local mode is experimental. There are some queries which just error out on bigger data in local mode.</p> </li> <li> <strong>Mapreduce Mode</strong> - To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation. Mapreduce mode is the default mode; you can, <em>but don't need to</em>, specify it using the -x flag (pig OR pig -x mapreduce). </li> <li> <strong>Tez Mode</strong> - To run Pig in Tez mode, you need access to a Hadoop cluster and HDFS installation. Specify Tez mode using the -x flag (-x tez). </li> <li> <strong>Spark Mode</strong> - To run Pig in Spark mode, you need access to a Spark, Yarn or Mesos cluster and HDFS installation. Specify Spark mode using the -x flag (-x spark). In Spark execution mode, it is necessary to set env::SPARK_MASTER to an appropriate value (local - local mode, yarn-client - yarn-client mode, mesos://host:port - spark on mesos or spark://host:port - spark cluster. For more information refer to spark documentation on Master URLs, <em>yarn-cluster mode is currently not supported</em>). Pig scripts run on Spark can take advantage of the <a href="http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">dynamic allocation</a> feature. The feature can be enabled by simply enabling <em>spark.dynamicAllocation.enabled</em>. Refer to spark <a href="http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation">configuration</a> for additional configuration details. In general all properties in the pig script prefixed with <em>spark.</em> are copied to the Spark Application Configuration. Please note that Yarn auxillary service need to be enabled on Spark for this to work. See Spark documentation for additional details. </li> </ul>  <p>You can run Pig in either mode using the "pig" command (the bin/pig Perl script) or the "java" command (java -cp pig.jar ...). </p>  <h4 id="Examples">Examples</h4> <p>This example shows how to run Pig in local and mapreduce mode using the pig command.</p> <pre class="code">
/* local mode */
$ pig -x local ...
 
/* Tez local mode */
$ pig -x tez_local ...
 
/* Spark local mode */
$ pig -x spark_local ...

/* mapreduce mode */
$ pig ...
or
$ pig -x mapreduce ...

/* Tez mode */
$ pig -x tez ...

/* Spark mode */
$ pig -x spark ...
</pre>  <h3 id="interactive-mode">Interactive Mode</h3> <p>You can run Pig in interactive mode using the Grunt shell. Invoke the Grunt shell using the "pig" command (as shown below) and then enter your Pig Latin statements and Pig commands interactively at the command line. </p>  <h4 id="Example">Example</h4> <p>These Pig Latin statements extract all user IDs from the /etc/passwd file. First, copy the /etc/passwd file to your local working directory. Next, invoke the Grunt shell by typing the "pig" command (in local or hadoop mode). Then, enter the Pig Latin statements interactively at the grunt prompt (be sure to include the semicolon after each statement). The DUMP operator will display the results to your terminal screen.</p> <pre class="code">
grunt&gt; A = load 'passwd' using PigStorage(':'); 
grunt&gt; B = foreach A generate $0 as id; 
grunt&gt; dump B; 
</pre> <p> <strong>Local Mode</strong> </p> <pre class="code">
$ pig -x local
... - Connecting to ...
grunt&gt;
</pre> <p> <strong>Tez Local Mode</strong> </p> <pre class="code">
$ pig -x tez_local
... - Connecting to ...
grunt&gt; 
</pre> <p> <strong>Spark Local Mode</strong> </p> <pre class="code">
$ pig -x spark_local
... - Connecting to ...
grunt&gt; 
</pre> <p> <strong>Mapreduce Mode</strong> </p> <pre class="code">
$ pig -x mapreduce
... - Connecting to ...
grunt&gt; 

or

$ pig 
... - Connecting to ...
grunt&gt; 
</pre> <p> <strong>Tez Mode</strong> </p> <pre class="code">
$ pig -x tez
... - Connecting to ...
grunt&gt; 
</pre> <p> <strong>Spark Mode</strong> </p> <pre class="code">
$ pig -x spark
... - Connecting to ...
grunt&gt;
</pre>  <h3 id="batch-mode">Batch Mode</h3> <p>You can run Pig in batch mode using <a href="#pig-scripts">Pig scripts</a> and the "pig" command (in local or hadoop mode).</p>  <h4 id="Example-N101DF">Example</h4> <p>The Pig Latin statements in the Pig script (id.pig) extract all user IDs from the /etc/passwd file. First, copy the /etc/passwd file to your local working directory. Next, run the Pig script from the command line (using local or mapreduce mode). The STORE operator will write the results to a file (id.out).</p> <pre class="code">
/* id.pig */

A = load 'passwd' using PigStorage(':');  -- load the passwd file 
B = foreach A generate $0 as id;  -- extract the user IDs 
store B into 'id.out';  -- write the results to a file name id.out
</pre> <p> <strong>Local Mode</strong> </p> <pre class="code">
$ pig -x local id.pig
</pre> <p> <strong>Tez Local Mode</strong> </p> <pre class="code">
$ pig -x tez_local id.pig
</pre> <p> <strong>Spark Local Mode</strong> </p> <pre class="code">
$ pig -x spark_local id.pig
</pre> <p> <strong>Mapreduce Mode</strong> </p> <pre class="code">
$ pig id.pig
or
$ pig -x mapreduce id.pig
</pre> <p> <strong>Tez Mode</strong> </p> <pre class="code">
$ pig -x tez id.pig
</pre> <p> <strong>Spark Mode</strong> </p> <pre class="code">
$ pig -x spark id.pig
</pre>  <h4 id="pig-scripts">Pig Scripts</h4> <p>Use Pig scripts to place Pig Latin statements and Pig commands in a single file. While not required, it is good practice to identify the file using the *.pig extension.</p> <p>You can run Pig scripts from the command line and from the Grunt shell (see the <a href="cmds#run">run</a> and <a href="cmds#exec">exec</a> commands). </p> <p>Pig scripts allow you to pass values to parameters using <a href="cont#Parameter-Sub">parameter substitution</a>. </p>  <p id="comments"> <strong>Comments in Scripts</strong> </p> <p>You can include comments in Pig scripts:</p> <ul> <li> <p>For multi-line comments use /* …. */</p> </li> <li> <p>For single-line comments use --</p> </li> </ul> <pre class="code">
/* myscript.pig
My script is simple.
It includes three Pig Latin statements.
*/

A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); -- loading data
B = FOREACH A GENERATE name;  -- transforming data
DUMP B;  -- retrieving results
</pre>  <p id="dfs"> <strong>Scripts and Distributed File Systems</strong> </p> <p>Pig supports running scripts (and Jar files) that are stored in HDFS, Amazon S3, and other distributed file systems. The script's full location URI is required (see <a href="basic#register">REGISTER</a> for information about Jar files). For example, to run a Pig script on HDFS, do the following:</p> <pre class="code">
$ pig hdfs://nn.mydomain.com:9020/myscripts/script.pig
</pre> </div>   <h2 id="kerberos">Running jobs on a Kerberos secured cluster</h2> <div class="section"> <p>Kerberos is a authentication system that uses tickets with a limited validity time.<br> As a consequence running a pig script on a kerberos secured hadoop cluster limits the running time to at most the remaining validity time of these kerberos tickets. When doing really complex analytics this may become a problem as the job may need to run for a longer time than these ticket times allow.</p>  <h3 id="kerberos-short">Short lived jobs</h3> <p>When running short jobs all you need to do is ensure that the user has been logged in into Kerberos via the normal kinit method.<br> The Hadoop job will automatically pickup these credentials and the job will run fine.</p>  <h3 id="kerberos-long">Long lived jobs</h3> <p>A kerberos keytab file is essentially a Kerberos specific form of the password of a user. <br> It is possible to enable a Hadoop job to request new tickets when they expire by creating a keytab file and make it part of the job that is running in the cluster. This will extend the maximum job duration beyond the maximum renew time of the kerberos tickets.</p> <p>Usage:</p> <ol> <li>Create a keytab file for the required principal.<br> Using the ktutil tool you can create a keytab using roughly these commands:<br> <pre class="code">addent -password -p niels@EXAMPLE.NL -k 1 -e rc4-hmac
addent -password -p niels@EXAMPLE.NL -k 1 -e aes256-cts
wkt niels.keytab</pre> </li> <li>Set the following properties (either via the .pigrc file or on the command line via -P file) <ul> <li> <em>java.security.krb5.conf</em> <br> The path to the local krb5.conf file.<br> Usually this is "/etc/krb5.conf"</li> <li> <em>hadoop.security.krb5.principal</em> <br> The pricipal you want to login with.<br> Usually this would look like this "niels@EXAMPLE.NL"</li> <li> <em>hadoop.security.krb5.keytab</em> <br> The path to the local keytab file that must be used to authenticate with.<br> Usually this would look like this "/home/niels/.krb/niels.keytab"</li> </ul> </li> </ol> <p> <strong>NOTE:</strong>All paths in these variables are local to the client system starting the actual pig script. This can be run without any special access to the cluster nodes.</p> <p>Overall you would create a file that looks like this (assume we call it niels.kerberos.properties):</p> <pre class="code">java.security.krb5.conf=/etc/krb5.conf
hadoop.security.krb5.principal=niels@EXAMPLE.NL
hadoop.security.krb5.keytab=/home/niels/.krb/niels.keytab</pre> <p>and start your script like this:</p> <pre class="code">pig -P niels.kerberos.properties script.pig</pre> </div>    <h2 id="pl-statements">Pig Latin Statements</h2> <div class="section"> <p>Pig Latin statements are the basic constructs you use to process data using Pig. A Pig Latin statement is an operator that takes a <a href="basic#relations">relation</a> as input and produces another relation as output. (This definition applies to all Pig Latin operators except LOAD and STORE which read data from and write data to the file system.) Pig Latin statements may include <a href="basic#expressions">expressions</a> and <a href="basic#schemas">schemas</a>. Pig Latin statements can span multiple lines and must end with a semi-colon ( ; ). By default, Pig Latin statements are processed using <a href="perf#multi-query-execution">multi-query execution</a>. </p> <p>Pig Latin statements are generally organized as follows:</p> <ul> <li> <p>A LOAD statement to read data from the file system. </p> </li> <li> <p>A series of "transformation" statements to process the data. </p> </li> <li> <p>A DUMP statement to view results or a STORE statement to save the results.</p> </li> </ul>  <p>Note that a DUMP or STORE statement is required to generate output.</p> <ul> <li> <p>In this example Pig will validate, but not execute, the LOAD and FOREACH statements.</p> <pre class="code">
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name;
</pre> </li> <li> <p>In this example, Pig will validate and then execute the LOAD, FOREACH, and DUMP statements.</p> <pre class="code">
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name;
DUMP B;
(John)
(Mary)
(Bill)
(Joe)
</pre> </li> </ul>  <h3 id="data-load">Loading Data</h3> <p>Use the <a href="basic#load">LOAD</a> operator and the <a href="udf#load-store-functions">load/store functions</a> to read data into Pig (PigStorage is the default load function).</p>  <h3 id="data-work-with">Working with Data</h3> <p>Pig allows you to transform data in many ways. As a starting point, become familiar with these operators:</p> <ul> <li> <p>Use the <a href="basic#filter">FILTER</a> operator to work with tuples or rows of data. Use the <a href="basic#foreach">FOREACH</a> operator to work with columns of data.</p> </li> <li> <p>Use the <a href="basic#group">GROUP</a> operator to group data in a single relation. Use the <a href="basic#cogroup">COGROUP</a>, <a href="basic#join-inner">inner JOIN</a>, and <a href="basic#join-outer">outer JOIN</a> operators to group or join data in two or more relations.</p> </li> <li> <p>Use the <a href="basic#union">UNION</a> operator to merge the contents of two or more relations. Use the <a href="basic#split">SPLIT</a> operator to partition the contents of a relation into multiple relations.</p> </li> </ul>  <h3 id="data-store">Storing Intermediate Results</h3> <p>Pig stores the intermediate data generated between MapReduce jobs in a temporary location on HDFS. This location must already exist on HDFS prior to use. This location can be configured using the pig.temp.dir property. The property's default value is "/tmp" which is the same as the hardcoded location in Pig 0.7.0 and earlier versions. </p>  <h3 id="data-results">Storing Final Results</h3> <p>Use the <a href="basic#store">STORE</a> operator and the <a href="udf#load-store-functions">load/store functions</a> to write results to the file system (PigStorage is the default store function). </p> <p> <strong>Note:</strong> During the testing/debugging phase of your implementation, you can use DUMP to display results to your terminal screen. However, in a production environment you always want to use the STORE operator to save your results (see <a href="perf#store-dump">Store vs. Dump</a>).</p>  <h3 id="debug">Debugging Pig Latin</h3> <p>Pig Latin provides operators that can help you debug your Pig Latin statements:</p> <ul> <li> <p>Use the <a href="test#dump">DUMP</a> operator to display results to your terminal screen. </p> </li> <li> <p>Use the <a href="test#describe">DESCRIBE</a> operator to review the schema of a relation.</p> </li> <li> <p>Use the <a href="test#explain">EXPLAIN</a> operator to view the logical, physical, or map reduce execution plans to compute a relation.</p> </li> <li> <p>Use the <a href="test#illustrate">ILLUSTRATE</a> operator to view the step-by-step execution of a series of statements.</p> </li> </ul> <p> <strong>Shortcuts for Debugging Operators</strong> </p> <p>Pig provides shortcuts for the frequently used debugging operators (DUMP, DESCRIBE, EXPLAIN, ILLUSTRATE). These shortcuts can be used in Grunt shell or within pig scripts. Following are the shortcuts supported by pig</p> <ul> <li> <p> \d alias - shourtcut for <a href="test#dump">DUMP</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \de alias - shourtcut for <a href="test#describe">DESCRIBE</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \e alias - shourtcut for <a href="test#explain">EXPLAIN</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \i alias - shourtcut for <a href="test#illustrate">ILLUSTRATE</a> operator. If alias is ignored last defined alias will be used.</p> </li> <li> <p> \q - To quit grunt shell </p> </li> </ul> </div>    <h2 id="properties">Pig Properties</h2> <div class="section"> <p>Pig supports a number of Java properties that you can use to customize Pig behavior. You can retrieve a list of the properties using the <a href="cmds#help">help properties</a> command. All of these properties are optional; none are required. </p>   <p id="pig-properties">To specify Pig properties use one of these mechanisms:</p> <ul> <li>The pig.properties file (add the directory that contains the pig.properties file to the classpath)</li> <li>The -D and a Pig property in PIG_OPTS environment variable (export PIG_OPTS=-Dpig.tmpfilecompression=true)</li> <li>The -P command line option and a properties file (pig -P mypig.properties)</li> <li>The <a href="cmds#set">set</a> command (set pig.exec.nocombiner true)</li> </ul> <p> <strong>Note:</strong> The properties file uses standard Java property file format.</p> <p>The following precedence order is supported: pig.properties &lt; -D Pig property &lt; -P properties file &lt; set command. This means that if the same property is provided using the –D command line option as well as the –P command line option (properties file), the value of the property in the properties file will take precedence.</p>  <p id="hadoop-properties">To specify Hadoop properties you can use the same mechanisms:</p> <ul> <li>Hadoop configuration files (include pig-cluster-hadoop-site.xml)</li> <li>The -D and a Hadoop property in PIG_OPTS environment variable (export PIG_OPTS=–Dmapreduce.task.profile=true) </li> <li>The -P command line option and a property file (pig -P property_file)</li> <li>The <a href="cmds#set">set</a> command (set mapred.map.tasks.speculative.execution false)</li> </ul>  <p>The same precedence holds: Hadoop configuration files &lt; -D Hadoop property &lt; -P properties_file &lt; set command.</p> <p>Hadoop properties are not interpreted by Pig but are passed directly to Hadoop. Any Hadoop property can be passed this way. </p> <p>All properties that Pig collects, including Hadoop properties, are available to any UDF via the UDFContext object. To get access to the properties, you can call the getJobConf method.</p> </div>    <h2 id="tutorial">Pig Tutorial </h2> <div class="section"> <p>The Pig tutorial shows you how to run Pig scripts using Pig's local mode, mapreduce mode, Tez mode and Spark mode (see <a href="#execution-modes">Execution Modes</a>).</p> <p>To get started, do the following preliminary tasks:</p> <ol> <li>Make sure the JAVA_HOME environment variable is set the root of your Java installation.</li> <li>Make sure your PATH includes bin/pig (this enables you to run the tutorials using the "pig" command). <pre class="code">
$ export PATH=/&lt;my-path-to-pig&gt;/pig-0.17.0/bin:$PATH 
</pre> </li> <li>Set the PIG_HOME environment variable: <pre class="code">
$ export PIG_HOME=/&lt;my-path-to-pig&gt;/pig-0.17.0 
</pre> </li> <li>Create the pigtutorial.tar.gz file: <ul> <li>Move to the Pig tutorial directory (.../pig-0.17.0/tutorial).</li> <li>Run the "ant" command from the tutorial directory. This will create the pigtutorial.tar.gz file. </li> </ul> </li> <li>Copy the pigtutorial.tar.gz file from the Pig tutorial directory to your local directory. </li> <li>Unzip the pigtutorial.tar.gz file. <pre class="code">
$ tar -xzf pigtutorial.tar.gz
</pre> </li> <li>A new directory named pigtmp is created. This directory contains the <a href="#pig-tutorial-files">Pig Tutorial Files</a>. These files work with Hadoop 0.20.2 and include everything you need to run <a href="#pig-script-1">Pig Script 1</a> and <a href="#pig-script-2">Pig Script 2</a>.</li> </ol>  <h3 id="Running+the+Pig+Scripts+in+Local+Mode"> Running the Pig Scripts in Local Mode</h3> <p>To run the Pig scripts in local mode, do the following: </p> <ol> <li>Move to the pigtmp directory.</li> <li>Execute the following command (using either script1-local.pig or script2-local.pig). <pre class="code">
$ pig -x local script1-local.pig
</pre> Or if you are using Tez local mode: <pre class="code">
$ pig -x tez_local script1-local.pig
</pre> Or if you are using Spark local mode: <pre class="code">
$ pig -x spark_local script1-local.pig
</pre> </li> <li>Review the result files, located in the script1-local-results.txt directory. <p>The output may contain a few Hadoop warnings which can be ignored:</p> <pre class="code">
2010-04-08 12:55:33,642 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics 
- Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
</pre> </li> </ol>  <h3 id="Running+the+Pig+Scripts+in+Mapreduce+Mode%2C+Tez+Mode+or+Spark+Mode"> Running the Pig Scripts in Mapreduce Mode, Tez Mode or Spark Mode</h3> <p>To run the Pig scripts in mapreduce mode, do the following: </p> <ol> <li>Move to the pigtmp directory.</li> <li>Copy the excite.log.bz2 file from the pigtmp directory to the HDFS directory. <pre class="code">
$ hadoop fs –copyFromLocal excite.log.bz2 .
</pre> </li> <li>Set the PIG_CLASSPATH environment variable to the location of the cluster configuration directory (the directory that contains the core-site.xml, hdfs-site.xml and mapred-site.xml files): <pre class="code">
export PIG_CLASSPATH=/mycluster/conf
</pre> <p>If you are using Tez, you will also need to put Tez configuration directory (the directory that contains the tez-site.xml):</p> <pre class="code">
export PIG_CLASSPATH=/mycluster/conf:/tez/conf
</pre> <p>If you are using Spark, you will also need to specify SPARK_HOME and specify SPARK_JAR which is the hdfs location where you uploaded $SPARK_HOME/lib/spark-assembly*.jar:</p> <pre class="code">export SPARK_HOME=/mysparkhome/; export SPARK_JAR=hdfs://example.com:8020/spark-assembly*.jar</pre> <p> <strong>Note:</strong> The PIG_CLASSPATH can also be used to add any other 3rd party dependencies or resource files a pig script may require. If there is also a need to make the added entries take the highest precedence in the Pig JVM's classpath order, one may also set the env-var PIG_USER_CLASSPATH_FIRST to any value, such as 'true' (and unset the env-var to disable).</p> </li> <li>Set the HADOOP_CONF_DIR environment variable to the location of the cluster configuration directory: <pre class="code">
export HADOOP_CONF_DIR=/mycluster/conf
</pre> </li> <li>Execute the following command (using either script1-hadoop.pig or script2-hadoop.pig): <pre class="code">
$ pig script1-hadoop.pig
</pre> Or if you are using Tez: <pre class="code">
$ pig -x tez script1-hadoop.pig
</pre> Or if you are using Spark: <pre class="code">
$ pig -x spark script1-hadoop.pig
</pre> </li> <li>Review the result files, located in the script1-hadoop-results or script2-hadoop-results HDFS directory: <pre class="code">
$ hadoop fs -ls script1-hadoop-results
$ hadoop fs -cat 'script1-hadoop-results/*' | less
</pre> </li> </ol>  <h3 id="pig-tutorial-files"> Pig Tutorial Files</h3> <p>The contents of the Pig tutorial file (pigtutorial.tar.gz) are described here. </p> <table class="ForrestTable"> <tr> <td colspan="1" rowspan="1"> <p> <strong>File</strong> </p> </td> <td colspan="1" rowspan="1"> <p> <strong>Description</strong> </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> pig.jar </p> </td> <td colspan="1" rowspan="1"> <p> Pig JAR file </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> tutorial.jar </p> </td> <td colspan="1" rowspan="1"> <p> User defined functions (UDFs) and Java classes </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> script1-local.pig </p> </td> <td colspan="1" rowspan="1"> <p> Pig Script 1, Query Phrase Popularity (local mode) </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> script1-hadoop.pig </p> </td> <td colspan="1" rowspan="1"> <p> Pig Script 1, Query Phrase Popularity (mapreduce mode) </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> script2-local.pig </p> </td> <td colspan="1" rowspan="1"> <p> Pig Script 2, Temporal Query Phrase Popularity (local mode)</p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> script2-hadoop.pig </p> </td> <td colspan="1" rowspan="1"> <p> Pig Script 2, Temporal Query Phrase Popularity (mapreduce mode) </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> excite-small.log </p> </td> <td colspan="1" rowspan="1"> <p> Log file, Excite search engine (local mode) </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> excite.log.bz2 </p> </td> <td colspan="1" rowspan="1"> <p> Log file, Excite search engine (mapreduce) </p> </td> </tr> </table> <p>The user defined functions (UDFs) are described here. </p> <table class="ForrestTable"> <tr> <td colspan="1" rowspan="1"> <p> <strong>UDF</strong> </p> </td> <td colspan="1" rowspan="1"> <p> <strong>Description</strong> </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> ExtractHour </p> </td> <td colspan="1" rowspan="1"> <p> Extracts the hour from the record.</p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> NGramGenerator </p> </td> <td colspan="1" rowspan="1"> <p> Composes n-grams from the set of words. </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> NonURLDetector </p> </td> <td colspan="1" rowspan="1"> <p> Removes the record if the query field is empty or a URL. </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> ScoreGenerator </p> </td> <td colspan="1" rowspan="1"> <p> Calculates a "popularity" score for the n-gram.</p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> ToLower </p> </td> <td colspan="1" rowspan="1"> <p> Changes the query field to lowercase. </p> </td> </tr> <tr> <td colspan="1" rowspan="1"> <p> TutorialUtil </p> </td> <td colspan="1" rowspan="1"> <p> Divides the query string into a set of words.</p> </td> </tr> </table>  <h3 id="pig-script-1"> Pig Script 1: Query Phrase Popularity</h3> <p>The Query Phrase Popularity script (script1-local.pig or script1-hadoop.pig) processes a search query log file from the Excite search engine and finds search phrases that occur with particular high frequency during certain times of the day. </p> <p>The script is shown here: </p> <ul> <li> <p> Register the tutorial JAR file so that the included UDFs can be called in the script. </p> </li> </ul> <pre class="code">
REGISTER ./tutorial.jar; 
</pre> <ul> <li> <p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>. </p> </li> </ul> <pre class="code">
raw = LOAD 'excite.log' USING PigStorage('\t') AS (user, time, query);
</pre> <ul> <li> <p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL. </p> </li> </ul> <pre class="code">
clean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);
</pre> <ul> <li> <p> Call the ToLower UDF to change the query field to lowercase. </p> </li> </ul> <pre class="code">
clean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;
</pre> <ul> <li> <p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour (HH) from the time field. </p> </li> </ul> <pre class="code">
houred = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;
</pre> <ul> <li> <p> Call the NGramGenerator UDF to compose the n-grams of the query. </p> </li> </ul> <pre class="code">
ngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;
</pre> <ul> <li> <p> Use the DISTINCT operator to get the unique n-grams for all records. </p> </li> </ul> <pre class="code">
ngramed2 = DISTINCT ngramed1;
</pre> <ul> <li> <p> Use the GROUP operator to group records by n-gram and hour. </p> </li> </ul> <pre class="code">
hour_frequency1 = GROUP ngramed2 BY (ngram, hour);
</pre> <ul> <li> <p> Use the COUNT function to get the count (occurrences) of each n-gram. </p> </li> </ul> <pre class="code">
hour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;
</pre> <ul> <li> <p> Use the GROUP operator to group records by n-gram only. Each group now corresponds to a distinct n-gram and has the count for each hour. </p> </li> </ul> <pre class="code">
uniq_frequency1 = GROUP hour_frequency2 BY group::ngram;
</pre> <ul> <li> <p> For each group, identify the hour in which this n-gram is used with a particularly high frequency. Call the ScoreGenerator UDF to calculate a "popularity" score for the n-gram. </p> </li> </ul> <pre class="code">
uniq_frequency2 = FOREACH uniq_frequency1 GENERATE flatten($0), flatten(org.apache.pig.tutorial.ScoreGenerator($1));
</pre> <ul> <li> <p> Use the FOREACH-GENERATE operator to assign names to the fields. </p> </li> </ul> <pre class="code">
uniq_frequency3 = FOREACH uniq_frequency2 GENERATE $1 as hour, $0 as ngram, $2 as score, $3 as count, $4 as mean;
</pre> <ul> <li> <p> Use the FILTER operator to remove all records with a score less than or equal to 2.0. </p> </li> </ul> <pre class="code">
filtered_uniq_frequency = FILTER uniq_frequency3 BY score &gt; 2.0;
</pre> <ul> <li> <p> Use the ORDER operator to sort the remaining records by hour and score. </p> </li> </ul> <pre class="code">
ordered_uniq_frequency = ORDER filtered_uniq_frequency BY hour, score;
</pre> <ul> <li> <p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>hour</strong>, <strong>ngram</strong>, <strong>score</strong>, <strong>count</strong>, <strong>mean</strong>. </p> </li> </ul> <pre class="code">
STORE ordered_uniq_frequency INTO '/tmp/tutorial-results' USING PigStorage(); 
</pre>  <h3 id="pig-script-2">Pig Script 2: Temporal Query Phrase Popularity</h3> <p>The Temporal Query Phrase Popularity script (script2-local.pig or script2-hadoop.pig) processes a search query log file from the Excite search engine and compares the occurrence of frequency of search phrases across two time periods separated by twelve hours. </p> <p>The script is shown here: </p> <ul> <li> <p> Register the tutorial JAR file so that the user defined functions (UDFs) can be called in the script. </p> </li> </ul> <pre class="code">
REGISTER ./tutorial.jar;
</pre> <ul> <li> <p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>. </p> </li> </ul> <pre class="code">
raw = LOAD 'excite.log' USING PigStorage('\t') AS (user, time, query);
</pre> <ul> <li> <p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL. </p> </li> </ul> <pre class="code">
clean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);
</pre> <ul> <li> <p> Call the ToLower UDF to change the query field to lowercase. </p> </li> </ul> <pre class="code">
clean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;
</pre> <ul> <li> <p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour from the time field. </p> </li> </ul> <pre class="code">
houred = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;
</pre> <ul> <li> <p> Call the NGramGenerator UDF to compose the n-grams of the query. </p> </li> </ul> <pre class="code">
ngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;
</pre> <ul> <li> <p> Use the DISTINCT operator to get the unique n-grams for all records. </p> </li> </ul> <pre class="code">
ngramed2 = DISTINCT ngramed1;
</pre> <ul> <li> <p> Use the GROUP operator to group the records by n-gram and hour. </p> </li> </ul> <pre class="code">
hour_frequency1 = GROUP ngramed2 BY (ngram, hour);
</pre> <ul> <li> <p> Use the COUNT function to get the count (occurrences) of each n-gram. </p> </li> </ul> <pre class="code">
hour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;
</pre> <ul> <li> <p> Use the FOREACH-GENERATE operator to assign names to the fields. </p> </li> </ul> <pre class="code">
hour_frequency3 = FOREACH hour_frequency2 GENERATE $0 as ngram, $1 as hour, $2 as count;
</pre> <ul> <li> <p> Use the FILTERoperator to get the n-grams for hour ‘00’ </p> </li> </ul> <pre class="code">
hour00 = FILTER hour_frequency2 BY hour eq '00';
</pre> <ul> <li> <p> Uses the FILTER operators to get the n-grams for hour ‘12’ </p> </li> </ul> <pre class="code">
hour12 = FILTER hour_frequency3 BY hour eq '12';
</pre> <ul> <li> <p> Use the JOIN operator to get the n-grams that appear in both hours. </p> </li> </ul> <pre class="code">
same = JOIN hour00 BY $0, hour12 BY $0;
</pre> <ul> <li> <p> Use the FOREACH-GENERATE operator to record their frequency. </p> </li> </ul> <pre class="code">
same1 = FOREACH same GENERATE hour_frequency2::hour00::group::ngram as ngram, $2 as count00, $5 as count12;
</pre> <ul> <li> <p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>ngram</strong>, <strong>count00</strong>, <strong>count12</strong>. </p> </li> </ul> <pre class="code">
STORE same1 INTO '/tmp/tutorial-join-results' USING PigStorage();
</pre> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2017 Apache Software Foundation<br>Licensed under the Apache Software License version 2.0.<br>
    <a href="https://pig.apache.org/docs/r0.17.0/start.html" class="_attribution-link">https://pig.apache.org/docs/r0.17.0/start.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
