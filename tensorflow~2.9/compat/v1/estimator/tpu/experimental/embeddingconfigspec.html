
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.compat.v1.estimator.tpu.experimental.EmbeddingConfigSpec - TensorFlow 2.9 - W3cubDocs</title>
  
  <meta name="description" content=" Class to keep track of the specification for TPU embeddings. ">
  <meta name="keywords" content="tf, compat, estimator, tpu, experimental, embeddingconfigspec, tensorflow, tensorflow~2.9">
  <meta name="HandheldFriendly" content="True">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.9/compat/v1/estimator/tpu/experimental/embeddingconfigspec.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-f16eecbe420d8b2925d31ffbb21d05646497ecbd9515f08ffe69e9bba7332f5657accc7003c7f6c72cb4a132171acf171b359ae3bae4ae5660ddfb1718f88c67.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~2.9.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.9/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.9</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.compat.v1.estimator.tpu.experimental.EmbeddingConfigSpec</h1> <devsite-bookmark></devsite-bookmark>       <p>Class to keep track of the specification for TPU embeddings.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.estimator.tpu.experimental.EmbeddingConfigSpec(
    feature_columns=None,
    optimization_parameters=None,
    clipping_limit=None,
    pipeline_execution_with_tensor_core=False,
    experimental_gradient_multiplier_fn=None,
    feature_to_config_dict=None,
    table_to_config_dict=None,
    partition_strategy='div',
    profile_data_directory=None
)
</pre> <p><section><devsite-expandable expanded> <h2 class="showalways" id="migrate-to-tf2" data-text="Migrate to TF2">Migrate to TF2</h2></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p>TPU Estimator manages its own TensorFlow graph and session, so it is not compatible with TF2 behaviors. We recommend that you migrate to the newer <a href="../../../../../distribute/tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>. See the <a href="https://www.tensorflow.org/guide/tpu">TPU guide</a> for details.</p>  <h2 id="description" data-text="Description">Description</h2>  <p>Pass this class to <code translate="no" dir="ltr">tf.estimator.tpu.TPUEstimator</code> via the <code translate="no" dir="ltr">embedding_config_spec</code> parameter. At minimum you need to specify <code translate="no" dir="ltr">feature_columns</code> and <code translate="no" dir="ltr">optimization_parameters</code>. The feature columns passed should be created with some combination of <code translate="no" dir="ltr">tf.tpu.experimental.embedding_column</code> and <code translate="no" dir="ltr">tf.tpu.experimental.shared_embedding_columns</code>.</p> <p>TPU embeddings do not support arbitrary Tensorflow optimizers and the main optimizer you use for your model will be ignored for the embedding table variables. Instead TPU embeddigns support a fixed set of predefined optimizers that you can select from and set the parameters of. These include adagrad, adam and stochastic gradient descent. Each supported optimizer has a <code translate="no" dir="ltr">Parameters</code> class in the <a href="../../../../../tpu/experimental"><code translate="no" dir="ltr">tf.tpu.experimental</code></a> namespace.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">column_a = tf.feature_column.categorical_column_with_identity(...)
column_b = tf.feature_column.categorical_column_with_identity(...)
column_c = tf.feature_column.categorical_column_with_identity(...)
tpu_shared_columns = tf.tpu.experimental.shared_embedding_columns(
    [column_a, column_b], 10)
tpu_non_shared_column = tf.tpu.experimental.embedding_column(
    column_c, 10)
tpu_columns = [tpu_non_shared_column] + tpu_shared_columns
...
def model_fn(features):
  dense_features = tf.keras.layers.DenseFeature(tpu_columns)
  embedded_feature = dense_features(features)
  ...

estimator = tf.estimator.tpu.TPUEstimator(
    model_fn=model_fn,
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        column=tpu_columns,
        optimization_parameters=(
            tf.estimator.tpu.experimental.AdagradParameters(0.1))))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">feature_columns</code> </td> <td> All embedding <code translate="no" dir="ltr">FeatureColumn</code>s used by model. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">optimization_parameters</code> </td> <td> An instance of <code translate="no" dir="ltr">AdagradParameters</code>, <code translate="no" dir="ltr">AdamParameters</code> or <code translate="no" dir="ltr">StochasticGradientDescentParameters</code>. This optimizer will be applied to all embedding variables specified by <code translate="no" dir="ltr">feature_columns</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clipping_limit</code> </td> <td> (Optional) Clipping limit (absolute value). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">pipeline_execution_with_tensor_core</code> </td> <td> setting this to <code translate="no" dir="ltr">True</code> makes training faster, but trained model will be different if step N and step N+1 involve the same set of embedding IDs. Please see <code translate="no" dir="ltr">tpu_embedding_configuration.proto</code> for details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_gradient_multiplier_fn</code> </td> <td> (Optional) A Fn taking global step as input returning the current multiplier for all embedding gradients. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">feature_to_config_dict</code> </td> <td> A dictionary mapping feature names to instances of the class <code translate="no" dir="ltr">FeatureConfig</code>. Either features_columns or the pair of <code translate="no" dir="ltr">feature_to_config_dict</code> and <code translate="no" dir="ltr">table_to_config_dict</code> must be specified. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">table_to_config_dict</code> </td> <td> A dictionary mapping feature names to instances of the class <code translate="no" dir="ltr">TableConfig</code>. Either features_columns or the pair of <code translate="no" dir="ltr">feature_to_config_dict</code> and <code translate="no" dir="ltr">table_to_config_dict</code> must be specified. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">partition_strategy</code> </td> <td> A string, determining how tensors are sharded to the tpu hosts. See <a href="../../../../../nn/safe_embedding_lookup_sparse"><code translate="no" dir="ltr">tf.nn.safe_embedding_lookup_sparse</code></a> for more details. Allowed value are <code translate="no" dir="ltr">"div"</code> and <code translate="no" dir="ltr">"mod"'. If</code>"mod"<code translate="no" dir="ltr">is used, evaluation and exporting the model to CPU will not work as expected. &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt; &lt;td&gt;</code>profile_data_directory` </td> <td> Directory where embedding lookup statistics are stored. These statistics summarize information about the inputs to the embedding lookup operation, in particular, the average number of embedding IDs per example and how well the embedding IDs are load balanced across the system. The lookup statistics are used during TPU initialization for embedding table partitioning. Collection of lookup statistics is done at runtime by profiling the embedding inputs, only a small fraction of input samples are profiled to minimize host CPU overhead. Once a suitable number of samples are profiled, the lookup statistics are saved to table-specific files in the profile data directory generally at the end of a TPU training loop. The filename corresponding to each table is obtained by hashing table specific parameters (e.g., table name and number of features) and global configuration parameters (e.g., sharding strategy and task count). The same profile data directory can be shared among several models to reuse embedding lookup statistics. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the feature_columns are not specified. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If the feature columns are not of ths correct type (one of _SUPPORTED_FEATURE_COLUMNS, _TPU_EMBEDDING_COLUMN_CLASSES OR _EMBEDDING_COLUMN_CLASSES). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If <code translate="no" dir="ltr">optimization_parameters</code> is not one of the required types. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">feature_columns</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 0 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">tensor_core_feature_columns</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 1 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">optimization_parameters</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 2 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clipping_limit</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 3 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">pipeline_execution_with_tensor_core</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 4 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_gradient_multiplier_fn</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 5 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">feature_to_config_dict</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 6 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">table_to_config_dict</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 7 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">partition_strategy</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 8 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">profile_data_directory</code> </td> <td> A <code translate="no" dir="ltr">namedtuple</code> alias for field number 9 </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/estimator/tpu/experimental/EmbeddingConfigSpec" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/estimator/tpu/experimental/EmbeddingConfigSpec</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
