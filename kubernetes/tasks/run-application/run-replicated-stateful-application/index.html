
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Run a Replicated Stateful Application - Kubernetes - W3cubDocs</title>
  
  <meta name="description" content="This page shows how to run a replicated stateful application using a StatefulSet controller. This application is a replicated MySQL database. The &hellip;">
  <meta name="keywords" content="run, replicated, stateful, application, kubernetes">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/kubernetes/tasks/run-application/run-replicated-stateful-application/">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/kubernetes.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/kubernetes/" class="_nav-link" title="" style="margin-left:0;">Kubernetes</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _kubernetes">
				
				
<h1>Run a Replicated Stateful Application</h1>  <p>This page shows how to run a replicated stateful application using a <a href="../../../concepts/workloads/controllers/statefulset/index">StatefulSet</a> controller. This application is a replicated MySQL database. The example topology has a single primary server and multiple replicas, using asynchronous row-based replication.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> <strong>This is not a production configuration</strong>. MySQL settings remain on insecure defaults to keep the focus on general patterns for running stateful applications in Kubernetes. </div> <h2 id="before-you-begin">Before you begin</h2> <ul> <li>
<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a> or you can use one of these Kubernetes playgrounds:</p> <ul> <li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li> <li><a href="http://labs.play-with-k8s.com/">Play with Kubernetes</a></li> </ul> To check the version, enter <code>kubectl version</code>. </li> <li>
<p>You need to either have a dynamic PersistentVolume provisioner with a default <a href="../../../concepts/storage/storage-classes/index">StorageClass</a>, or <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioning">statically provision PersistentVolumes</a> yourself to satisfy the <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> used here.</p> </li> <li>This tutorial assumes you are familiar with <a href="../../../concepts/storage/persistent-volumes/index">PersistentVolumes</a> and <a href="../../../concepts/workloads/controllers/statefulset/index">StatefulSets</a>, as well as other core concepts like <a href="../../../concepts/workloads/pods/index">Pods</a>, <a href="../../../concepts/services-networking/service/index">Services</a>, and <a href="../../configure-pod-container/configure-pod-configmap/index">ConfigMaps</a>.</li> <li>Some familiarity with MySQL helps, but this tutorial aims to present general patterns that should be useful for other systems.</li> <li>You are using the default namespace or another namespace that does not contain any conflicting objects.</li> </ul> <h2 id="objectives">Objectives</h2> <ul> <li>Deploy a replicated MySQL topology with a StatefulSet controller.</li> <li>Send MySQL client traffic.</li> <li>Observe resistance to downtime.</li> <li>Scale the StatefulSet up and down.</li> </ul>  <h2 id="deploy-mysql">Deploy MySQL</h2> <p>The example MySQL deployment consists of a ConfigMap, two Services, and a StatefulSet.</p> <h3 id="configmap">ConfigMap</h3> <p>Create the ConfigMap from the following YAML configuration file:</p> <div class="highlight">  <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-configmap.yaml" download="application/mysql/mysql-configmap.yaml"><code>application/mysql/mysql-configmap.yaml</code> </a>   <div class="includecode" id="application-mysql-mysql-configmap-yaml"> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:
  primary.cnf: |
    # Apply this config only on the primary.
    [mysqld]
    log-bin    
  replica.cnf: |
    # Apply this config only on replicas.
    [mysqld]
    super-read-only    

</pre></div> </div> </div> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml
</pre></div>
<p>This ConfigMap provides <code>my.cnf</code> overrides that let you independently control configuration on the primary MySQL server and replicas. In this case, you want the primary server to be able to serve replication logs to replicas and you want replicas to reject any writes that don't come via replication.</p> <p>There's nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it's initializing, based on information provided by the StatefulSet controller.</p> <h3 id="services">Services</h3> <p>Create the Services from the following YAML configuration file:</p> <div class="highlight">  <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-services.yaml" download="application/mysql/mysql-services.yaml"><code>application/mysql/mysql-services.yaml</code> </a>   <div class="includecode" id="application-mysql-mysql-services-yaml"> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml"># Headless service for stable DNS entries of StatefulSet members.
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
# Client service for connecting to any MySQL instance for reads.
# For writes, you must instead connect to the primary: mysql-0.mysql.
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql

</pre></div> </div> </div> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml
</pre></div>
<p>The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that's part of the set. Because the Headless Service is named <code>mysql</code>, the Pods are accessible by resolving <code>&lt;pod-name&gt;.mysql</code> from within any other Pod in the same Kubernetes cluster and namespace.</p> <p>The Client Service, called <code>mysql-read</code>, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the primary MySQL server and all replicas.</p> <p>Note that only read queries can use the load-balanced Client Service. Because there is only one primary MySQL server, clients should connect directly to the primary MySQL Pod (through its DNS entry within the Headless Service) to execute writes.</p> <h3 id="statefulset">StatefulSet</h3> <p>Finally, create the StatefulSet from the following YAML configuration file:</p> <div class="highlight">  <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-statefulset.yaml" download="application/mysql/mysql-statefulset.yaml"><code>application/mysql/mysql-statefulset.yaml</code> </a>   <div class="includecode" id="application-mysql-mysql-statefulset-yaml"> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] &gt; /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/primary.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/replica.cnf /mnt/conf.d/
          fi          
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0
          # Skip the clone on primary (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql          
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # Check we can execute queries over TCP (skip-networking is off).
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql

          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info &amp;&amp; "x$(&lt;xtrabackup_slave_info)" != "x" ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing replica. (Need to remove the tailing semicolon!)
            cat xtrabackup_slave_info | sed -E 's/;$//g' &gt; change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_slave_info xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from primary. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm -f xtrabackup_binlog_info xtrabackup_slave_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" &gt; change_master_to.sql.in
          fi

          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

            echo "Initializing replication from clone position"
            mysql -h 127.0.0.1 \
                  -e "$(&lt;change_master_to.sql.in), \
                          MASTER_HOST='mysql-0.mysql', \
                          MASTER_USER='root', \
                          MASTER_PASSWORD='', \
                          MASTER_CONNECT_RETRY=10; \
                        START SLAVE;" || exit 1
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
          fi

          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"          
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
</pre></div> </div> </div> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml
</pre></div>
<p>You can watch the startup progress by running:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl get pods -l app=mysql --watch
</pre></div>
<p>After a while, you should see all 3 Pods become Running:</p> <pre><code>NAME      READY     STATUS    RESTARTS   AGE
mysql-0   2/2       Running   0          2m
mysql-1   2/2       Running   0          1m
mysql-2   2/2       Running   0          1m
</code></pre>
<p>Press <strong>Ctrl+C</strong> to cancel the watch. If you don't see any progress, make sure you have a dynamic PersistentVolume provisioner enabled as mentioned in the <a href="#before-you-begin">prerequisites</a>.</p> <p>This manifest uses a variety of techniques for managing stateful Pods as part of a StatefulSet. The next section highlights some of these techniques to explain what happens as the StatefulSet creates Pods.</p> <h2 id="understanding-stateful-pod-initialization">Understanding stateful Pod initialization</h2> <p>The StatefulSet controller starts Pods one at a time, in order by their ordinal index. It waits until each Pod reports being Ready before starting the next one.</p> <p>In addition, the controller assigns each Pod a unique, stable name of the form <code>&lt;statefulset-name&gt;-&lt;ordinal-index&gt;</code>, which results in Pods named <code>mysql-0</code>, <code>mysql-1</code>, and <code>mysql-2</code>.</p> <p>The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication.</p> <h3 id="generating-configuration">Generating configuration</h3> <p>Before starting any of the containers in the Pod spec, the Pod first runs any <a href="../../../concepts/workloads/pods/init-containers/index">Init Containers</a> in the order defined.</p> <p>The first Init Container, named <code>init-mysql</code>, generates special MySQL config files based on the ordinal index.</p> <p>The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the <code>hostname</code> command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called <code>server-id.cnf</code> in the MySQL <code>conf.d</code> directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties.</p> <p>The script in the <code>init-mysql</code> container also applies either <code>primary.cnf</code> or <code>replica.cnf</code> from the ConfigMap by copying the contents into <code>conf.d</code>. Because the example topology consists of a single primary MySQL server and any number of replicas, the script assigns ordinal <code>0</code> to be the primary server, and everyone else to be replicas. Combined with the StatefulSet controller's <a href="../../../concepts/workloads/controllers/statefulset/index#deployment-and-scaling-guarantees">deployment order guarantee</a>, this ensures the primary MySQL server is Ready before creating replicas, so they can begin replicating.</p> <h3 id="cloning-existing-data">Cloning existing data</h3> <p>In general, when a new Pod joins the set as a replica, it must assume the primary MySQL server might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size.</p> <p>The second Init Container, named <code>clone-mysql</code>, performs a clone operation on a replica Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the primary server.</p> <p>MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the primary MySQL server, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod <code>N</code> is Ready before starting Pod <code>N+1</code>.</p> <h3 id="starting-replication">Starting replication</h3> <p>After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a <code>mysql</code> container that runs the actual <code>mysqld</code> server, and an <code>xtrabackup</code> container that acts as a <a href="https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns">sidecar</a>.</p> <p>The <code>xtrabackup</code> sidecar looks at the cloned data files and determines if it's necessary to initialize MySQL replication on the replica. If so, it waits for <code>mysqld</code> to be ready and then executes the <code>CHANGE MASTER TO</code> and <code>START SLAVE</code> commands with replication parameters extracted from the XtraBackup clone files.</p> <p>Once a replica begins replication, it remembers its primary MySQL server and reconnects automatically if the server restarts or the connection dies. Also, because replicas look for the primary server at its stable DNS name (<code>mysql-0.mysql</code>), they automatically find the primary server even if it gets a new Pod IP due to being rescheduled.</p> <p>Lastly, after starting replication, the <code>xtrabackup</code> container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.</p> <h2 id="sending-client-traffic">Sending client traffic</h2> <p>You can send test queries to the primary MySQL server (hostname <code>mysql-0.mysql</code>) by running a temporary container with the <code>mysql:5.7</code> image and running the <code>mysql</code> client binary.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\
  mysql -h mysql-0.mysql &lt;&lt;EOF
CREATE DATABASE test;
CREATE TABLE test.messages (message VARCHAR(250));
INSERT INTO test.messages VALUES ('hello');
EOF
</pre></div>
<p>Use the hostname <code>mysql-read</code> to send test queries to any server that reports being Ready:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  mysql -h mysql-read -e "SELECT * FROM test.messages"
</pre></div>
<p>You should get output like this:</p> <pre><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
</code></pre>
<p>To demonstrate that the <code>mysql-read</code> Service distributes connections across servers, you can run <code>SELECT @@server_id</code> in a loop:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\
  bash -ic "while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done"
</pre></div>
<p>You should see the reported <code>@@server_id</code> change randomly, because a different endpoint might be selected upon each connection attempt:</p> <pre><code>+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2006-01-02 15:04:05 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2006-01-02 15:04:06 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2006-01-02 15:04:07 |
+-------------+---------------------+
</code></pre>
<p>You can press <strong>Ctrl+C</strong> when you want to stop the loop, but it's useful to keep it running in another window so you can see the effects of the following steps.</p> <h2 id="simulating-pod-and-node-downtime">Simulating Pod and Node downtime</h2> <p>To demonstrate the increased availability of reading from the pool of replicas instead of a single server, keep the <code>SELECT @@server_id</code> loop from above running while you force a Pod out of the Ready state.</p> <h3 id="break-the-readiness-probe">Break the Readiness Probe</h3> <p>The <a href="../../configure-pod-container/configure-liveness-readiness-startup-probes/index#define-readiness-probes">readiness probe</a> for the <code>mysql</code> container runs the command <code>mysql -h 127.0.0.1 -e 'SELECT 1'</code> to make sure the server is up and able to execute queries.</p> <p>One way to force this readiness probe to fail is to break that command:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off
</pre></div>
<p>This reaches into the actual container's filesystem for Pod <code>mysql-2</code> and renames the <code>mysql</code> command so the readiness probe can't find it. After a few seconds, the Pod should report one of its containers as not Ready, which you can check by running:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl get pod mysql-2
</pre></div>
<p>Look for <code>1/2</code> in the <code>READY</code> column:</p> <pre><code>NAME      READY     STATUS    RESTARTS   AGE
mysql-2   1/2       Running   0          3m
</code></pre>
<p>At this point, you should see your <code>SELECT @@server_id</code> loop continue to run, although it never reports <code>102</code> anymore. Recall that the <code>init-mysql</code> script defined <code>server-id</code> as <code>100 + $ordinal</code>, so server ID <code>102</code> corresponds to Pod <code>mysql-2</code>.</p> <p>Now repair the Pod and it should reappear in the loop output after a few seconds:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql
</pre></div>
<h3 id="delete-pods">Delete Pods</h3> <p>The StatefulSet also recreates Pods if they're deleted, similar to what a ReplicaSet does for stateless Pods.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl delete pod mysql-2
</pre></div>
<p>The StatefulSet controller notices that no <code>mysql-2</code> Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. You should see server ID <code>102</code> disappear from the loop output for a while and then return on its own.</p> <h3 id="drain-a-node">Drain a Node</h3> <p>If your Kubernetes cluster has multiple Nodes, you can simulate Node downtime (such as when Nodes are upgraded) by issuing a <a href="../../../reference/generated/kubectl/kubectl-commands/index#drain">drain</a>.</p> <p>First determine which Node one of the MySQL Pods is on:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl get pod mysql-2 -o wide
</pre></div>
<p>The Node name should show up in the last column:</p> <pre><code>NAME      READY     STATUS    RESTARTS   AGE       IP            NODE
mysql-2   2/2       Running   0          15m       10.244.5.27   kubernetes-node-9l2t
</code></pre>
<p>Then drain the Node by running the following command, which cordons it so no new Pods may schedule there, and then evicts any existing Pods. Replace <code>&lt;node-name&gt;</code> with the name of the Node you found in the last step.</p> <p>This might impact other applications on the Node, so it's best to <strong>only do this in a test cluster</strong>.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl drain &lt;node-name&gt; --force --delete-emptydir-data --ignore-daemonsets
</pre></div>
<p>Now you can watch as the Pod reschedules on a different Node:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl get pod mysql-2 -o wide --watch
</pre></div>
<p>It should look something like this:</p> <pre><code>NAME      READY   STATUS          RESTARTS   AGE       IP            NODE
mysql-2   2/2     Terminating     0          15m       10.244.1.56   kubernetes-node-9l2t
[...]
mysql-2   0/2     Pending         0          0s        &lt;none&gt;        kubernetes-node-fjlm
mysql-2   0/2     Init:0/2        0          0s        &lt;none&gt;        kubernetes-node-fjlm
mysql-2   0/2     Init:1/2        0          20s       10.244.5.32   kubernetes-node-fjlm
mysql-2   0/2     PodInitializing 0          21s       10.244.5.32   kubernetes-node-fjlm
mysql-2   1/2     Running         0          22s       10.244.5.32   kubernetes-node-fjlm
mysql-2   2/2     Running         0          30s       10.244.5.32   kubernetes-node-fjlm
</code></pre>
<p>And again, you should see server ID <code>102</code> disappear from the <code>SELECT @@server_id</code> loop output for a while and then return.</p> <p>Now uncordon the Node to return it to a normal state:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl uncordon &lt;node-name&gt;
</pre></div>
<h2 id="scaling-the-number-of-replicas">Scaling the number of replicas</h2> <p>With MySQL replication, you can scale your read query capacity by adding replicas. With StatefulSet, you can do this with a single command:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl scale statefulset mysql  --replicas=5
</pre></div>
<p>Watch the new Pods come up by running:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl get pods -l app=mysql --watch
</pre></div>
<p>Once they're up, you should see server IDs <code>103</code> and <code>104</code> start appearing in the <code>SELECT @@server_id</code> loop output.</p> <p>You can also verify that these new servers have the data you added before they existed:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  mysql -h mysql-3.mysql -e "SELECT * FROM test.messages"
</pre></div>
<pre><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
</code></pre>
<p>Scaling back down is also seamless:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl scale statefulset mysql --replicas=3
</pre></div>
<p>Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them.</p> <p>You can see this by running:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl get pvc -l app=mysql
</pre></div>
<p>Which shows that all 5 PVCs still exist, despite having scaled the StatefulSet down to 3:</p> <pre><code>NAME           STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
data-mysql-0   Bound     pvc-8acbf5dc-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-1   Bound     pvc-8ad39820-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-2   Bound     pvc-8ad69a6d-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-3   Bound     pvc-50043c45-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
data-mysql-4   Bound     pvc-500a9957-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
</code></pre>
<p>If you don't intend to reuse the extra PVCs, you can delete them:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl delete pvc data-mysql-3
kubectl delete pvc data-mysql-4
</pre></div>
<h2 id="cleaning-up">Cleaning up</h2> <ol> <li> <p>Cancel the <code>SELECT @@server_id</code> loop by pressing <strong>Ctrl+C</strong> in its terminal, or running the following from another terminal:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl delete pod mysql-client-loop --now
</pre></div>
</li> <li> <p>Delete the StatefulSet. This also begins terminating the Pods.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl delete statefulset mysql
</pre></div>
</li> <li> <p>Verify that the Pods disappear. They might take some time to finish terminating.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl get pods -l app=mysql
</pre></div>
<p>You'll know the Pods have terminated when the above returns:</p> <pre><code>No resources found.
</code></pre>
</li> <li> <p>Delete the ConfigMap, Services, and PersistentVolumeClaims.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="shell">kubectl delete configmap,service,pvc -l app=mysql
</pre></div>
</li> <li> <p>If you manually provisioned PersistentVolumes, you also need to manually delete them, as well as release the underlying resources. If you used a dynamic provisioner, it automatically deletes the PersistentVolumes when it sees that you deleted the PersistentVolumeClaims. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resources upon deleting the PersistentVolumes.</p> </li> </ol> <h2 id="what-s-next">What's next</h2> <ul> <li>Learn more about <a href="../scale-stateful-set/index">scaling a StatefulSet</a>.</li> <li>Learn more about <a href="../../debug-application-cluster/debug-stateful-set/index">debugging a StatefulSet</a>.</li> <li>Learn more about <a href="../delete-stateful-set/index">deleting a StatefulSet</a>.</li> <li>Learn more about <a href="../force-delete-stateful-set-pod/index">force deleting StatefulSet Pods</a>.</li> <li>Look in the <a href="https://artifacthub.io/">Helm Charts repository</a> for other stateful application examples.</li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2022 The Kubernetes Authors<br>Documentation Distributed under CC BY 4.0.<br>
    <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/" class="_attribution-link">https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
