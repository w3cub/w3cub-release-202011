
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Pod Topology Spread Constraints - Kubernetes - W3cubDocs</title>
  
  <meta name="description" content="You can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, and &hellip;">
  <meta name="keywords" content="pod, topology, spread, constraints, kubernetes">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/kubernetes/concepts/workloads/pods/pod-topology-spread-constraints/">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/kubernetes.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/kubernetes/" class="_nav-link" title="" style="margin-left:0;">Kubernetes</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _kubernetes">
				
				
<h1>Pod Topology Spread Constraints</h1> <div style="margin-top: 10px; margin-bottom: 10px;"> <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code> </div>   <p>You can use <em>topology spread constraints</em> to control how <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="../index" target="_blank" aria-label="Pods">Pods</a> are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> In versions of Kubernetes before v1.18, you must enable the <code>EvenPodsSpread</code> <a href="../../../../reference/command-line-tools-reference/feature-gates/index">feature gate</a> on the <a href="../../../overview/components/index#kube-apiserver">API server</a> and the <a href="../../../../reference/command-line-tools-reference/kube-scheduler/index">scheduler</a> in order to use Pod topology spread constraints. </div>  <h2 id="prerequisites">Prerequisites</h2> <h3 id="node-labels">Node Labels</h3> <p>Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. For example, a Node might have labels: <code>node=node1,zone=us-east-1a,region=us-east-1</code></p> <p>Suppose you have a 4-node cluster with the following labels:</p> <pre><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre>
<p>Then the cluster is logically viewed as below:</p> <div class="mermaid"> graph TB subgraph "zoneB" n3(Node3) n4(Node4) end subgraph "zoneA" n1(Node1) n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4 k8s; class zoneA,zoneB cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <p>Instead of manually applying labels, you can also reuse the <a href="../../../../reference/labels-annotations-taints/index">well-known labels</a> that are created and populated automatically on most clusters.</p> <h2 id="spread-constraints-for-pods">Spread Constraints for Pods</h2> <h3 id="api">API</h3> <p>The API field <code>pod.spec.topologySpreadConstraints</code> is defined as below:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt;
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
</pre></div>
<p>You can define one or multiple <code>topologySpreadConstraint</code> to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. The fields are:</p> <ul> <li>
<strong>maxSkew</strong> describes the degree to which Pods may be unevenly distributed. It must be greater than zero. Its semantics differs according to the value of <code>whenUnsatisfiable</code>: <ul> <li>when <code>whenUnsatisfiable</code> equals to "DoNotSchedule", <code>maxSkew</code> is the maximum permitted difference between the number of matching pods in the target topology and the global minimum (the minimum number of pods that match the label selector in a topology domain. For example, if you have 3 zones with 0, 2 and 3 matching pods respectively, The global minimum is 0).</li> <li>when <code>whenUnsatisfiable</code> equals to "ScheduleAnyway", scheduler gives higher precedence to topologies that would help reduce the skew.</li> </ul> </li> <li>
<strong>topologyKey</strong> is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.</li> <li>
<strong>whenUnsatisfiable</strong> indicates how to deal with a Pod if it doesn't satisfy the spread constraint: <ul> <li>
<code>DoNotSchedule</code> (default) tells the scheduler not to schedule it.</li> <li>
<code>ScheduleAnyway</code> tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.</li> </ul> </li> <li>
<strong>labelSelector</strong> is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See <a href="../../../overview/working-with-objects/labels/index#label-selectors">Label Selectors</a> for more details.</li> </ul> <p>When a Pod defines more than one <code>topologySpreadConstraint</code>, those constraints are ANDed: The kube-scheduler looks for a node for the incoming Pod that satisfies all the constraints.</p> <p>You can read more about this field by running <code>kubectl explain Pod.spec.topologySpreadConstraints</code>.</p> <h3 id="example-one-topologyspreadconstraint">Example: One TopologySpreadConstraint</h3> <p>Suppose you have a 4-node cluster where 3 Pods labeled <code>foo:bar</code> are located in node1, node2 and node3 respectively:</p> <div class="mermaid"> graph BT subgraph "zoneB" p3(Pod) --&gt; n3(Node3) n4(Node4) end subgraph "zoneA" p1(Pod) --&gt; n1(Node1) p2(Pod) --&gt; n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class zoneA,zoneB cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <p>If we want an incoming Pod to be evenly spread with existing Pods across zones, the spec can be given as:</p> <div class="highlight">  <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint.yaml" download="pods/topology-spread-constraints/one-constraint.yaml"><code>pods/topology-spread-constraints/one-constraint.yaml</code> </a>   <div class="includecode" id="pods-topology-spread-constraints-one-constraint-yaml"> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</pre></div> </div> </div> <p><code>topologyKey: zone</code> implies the even distribution will only be applied to the nodes which have label pair "zone:&lt;any value&gt;" present. <code>whenUnsatisfiable: DoNotSchedule</code> tells the scheduler to let it stay pending if the incoming Pod can't satisfy the constraint.</p> <p>If the scheduler placed this incoming Pod into "zoneA", the Pods distribution would become [3, 1], hence the actual skew is 2 (3 - 1) - which violates <code>maxSkew: 1</code>. In this example, the incoming Pod can only be placed onto "zoneB":</p> <div class="mermaid"> graph BT subgraph "zoneB" p3(Pod) --&gt; n3(Node3) p4(mypod) --&gt; n4(Node4) end subgraph "zoneA" p1(Pod) --&gt; n1(Node1) p2(Pod) --&gt; n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <p>OR</p> <div class="mermaid"> graph BT subgraph "zoneB" p3(Pod) --&gt; n3(Node3) p4(mypod) --&gt; n3 n4(Node4) end subgraph "zoneA" p1(Pod) --&gt; n1(Node1) p2(Pod) --&gt; n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <p>You can tweak the Pod spec to meet various kinds of requirements:</p> <ul> <li>Change <code>maxSkew</code> to a bigger value like "2" so that the incoming Pod can be placed onto "zoneA" as well.</li> <li>Change <code>topologyKey</code> to "node" so as to distribute the Pods evenly across nodes instead of zones. In the above example, if <code>maxSkew</code> remains "1", the incoming Pod can only be placed onto "node4".</li> <li>Change <code>whenUnsatisfiable: DoNotSchedule</code> to <code>whenUnsatisfiable: ScheduleAnyway</code> to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it's preferred to be placed onto the topology domain which has fewer matching Pods. (Be aware that this preferability is jointly normalized with other internal scheduling priorities like resource usage ratio, etc.)</li> </ul> <h3 id="example-multiple-topologyspreadconstraints">Example: Multiple TopologySpreadConstraints</h3> <p>This builds upon the previous example. Suppose you have a 4-node cluster where 3 Pods labeled <code>foo:bar</code> are located in node1, node2 and node3 respectively:</p> <div class="mermaid"> graph BT subgraph "zoneB" p3(Pod) --&gt; n3(Node3) n4(Node4) end subgraph "zoneA" p1(Pod) --&gt; n1(Node1) p2(Pod) --&gt; n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <p>You can use 2 TopologySpreadConstraints to control the Pods spreading on both zone and node:</p> <div class="highlight">  <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml" download="pods/topology-spread-constraints/two-constraints.yaml"><code>pods/topology-spread-constraints/two-constraints.yaml</code> </a>   <div class="includecode" id="pods-topology-spread-constraints-two-constraints-yaml"> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</pre></div> </div> </div> <p>In this case, to match the first constraint, the incoming Pod can only be placed onto "zoneB"; while in terms of the second constraint, the incoming Pod can only be placed onto "node4". Then the results of 2 constraints are ANDed, so the only viable option is to place on "node4".</p> <p>Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:</p> <div class="mermaid"> graph BT subgraph "zoneB" p4(Pod) --&gt; n3(Node3) p5(Pod) --&gt; n3 end subgraph "zoneA" p1(Pod) --&gt; n1(Node1) p2(Pod) --&gt; n1 p3(Pod) --&gt; n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s; class zoneA,zoneB cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <p>If you apply "two-constraints.yaml" to this cluster, you will notice "mypod" stays in <code>Pending</code> state. This is because: to satisfy the first constraint, "mypod" can only be put to "zoneB"; while in terms of the second constraint, "mypod" can only put to "node2". Then a joint result of "zoneB" and "node2" returns nothing.</p> <p>To overcome this situation, you can either increase the <code>maxSkew</code> or modify one of the constraints to use <code>whenUnsatisfiable: ScheduleAnyway</code>.</p> <h3 id="interaction-with-node-affinity-and-node-selectors">Interaction With Node Affinity and Node Selectors</h3> <p>The scheduler will skip the non-matching nodes from the skew calculations if the incoming Pod has <code>spec.nodeSelector</code> or <code>spec.affinity.nodeAffinity</code> defined.</p> <h3 id="example-topologyspreadconstraints-with-nodeaffinity">Example: TopologySpreadConstraints with NodeAffinity</h3> <p>Suppose you have a 5-node cluster ranging from zoneA to zoneC:</p> <div class="mermaid"> graph BT subgraph "zoneB" p3(Pod) --&gt; n3(Node3) n4(Node4) end subgraph "zoneA" p1(Pod) --&gt; n1(Node1) p2(Pod) --&gt; n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <div class="mermaid"> graph BT subgraph "zoneC" n5(Node5) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n5 k8s; class zoneC cluster; </div> <noscript>  <h4>[JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view content]</h4> </noscript> <p>and you know that "zoneC" must be excluded. In this case, you can compose the yaml as below, so that "mypod" will be placed onto "zoneB" instead of "zoneC". Similarly <code>spec.nodeSelector</code> is also respected.</p> <div class="highlight">  <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml" download="pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml"><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code> </a>   <div class="includecode" id="pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml"> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: NotIn
            values:
            - zoneC
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</pre></div> </div> </div> <p>The scheduler doesn't have prior knowledge of all the zones or other topology domains that a cluster has. They are determined from the existing nodes in the cluster. This could lead to a problem in autoscaled clusters, when a node pool (or node group) is scaled to zero nodes and the user is expecting them to scale up, because, in this case, those topology domains won't be considered until there is at least one node in them.</p> <h3 id="other-noticeable-semantics">Other Noticeable Semantics</h3> <p>There are some implicit conventions worth noting here:</p> <ul> <li> <p>Only the Pods holding the same namespace as the incoming Pod can be matching candidates.</p> </li> <li> <p>The scheduler will bypass the nodes without <code>topologySpreadConstraints[*].topologyKey</code> present. This implies that:</p> <ol> <li>the Pods located on those nodes do not impact <code>maxSkew</code> calculation - in the above example, suppose "node1" does not have label "zone", then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into "zoneA".</li> <li>the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a "node5" carrying label <code>{zone-typo: zoneC}</code> joins the cluster, it will be bypassed due to the absence of label key "zone".</li> </ol> </li> <li> <p>Be aware of what will happen if the incomingPod's <code>topologySpreadConstraints[*].labelSelector</code> doesn't match its own labels. In the above example, if we remove the incoming Pod's labels, it can still be placed onto "zoneB" since the constraints are still satisfied. However, after the placement, the degree of imbalance of the cluster remains unchanged - it's still zoneA having 2 Pods which hold label {foo:bar}, and zoneB having 1 Pod which holds label {foo:bar}. So if this is not what you expect, we recommend the workload's <code>topologySpreadConstraints[*].labelSelector</code> to match its own labels.</p> </li> </ul> <h3 id="cluster-level-default-constraints">Cluster-level default constraints</h3> <p>It is possible to set default topology spread constraints for a cluster. Default topology spread constraints are applied to a Pod if, and only if:</p> <ul> <li>It doesn't define any constraints in its <code>.spec.topologySpreadConstraints</code>.</li> <li>It belongs to a service, replication controller, replica set or stateful set.</li> </ul> <p>Default constraints can be set as part of the <code>PodTopologySpread</code> plugin args in a <a href="../../../../reference/scheduling/config/index#profiles">scheduling profile</a>. The constraints are specified with the same <a href="#api">API above</a>, except that <code>labelSelector</code> must be empty. The selectors are calculated from the services, replication controllers, replica sets or stateful sets that the Pod belongs to.</p> <p>An example configuration might look like follows:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration

profiles:
  - pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
</pre></div>
<div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> The score produced by default scheduling constraints might conflict with the score produced by the <a href="../../../../reference/scheduling/config/index#scheduling-plugins"><code>SelectorSpread</code> plugin</a>. It is recommended that you disable this plugin in the scheduling profile when using default constraints for <code>PodTopologySpread</code>. </div> <h4 id="internal-default-constraints">Internal default constraints</h4> <div style="margin-top: 10px; margin-bottom: 10px;"> <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code> </div> <p>With the <code>DefaultPodTopologySpread</code> feature gate, enabled by default, the legacy <code>SelectorSpread</code> plugin is disabled. kube-scheduler uses the following default topology constraints for the <code>PodTopologySpread</code> plugin configuration:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">defaultConstraints:
  - maxSkew: 3
    topologyKey: "kubernetes.io/hostname"
    whenUnsatisfiable: ScheduleAnyway
  - maxSkew: 5
    topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: ScheduleAnyway
</pre></div>
<p>Also, the legacy <code>SelectorSpread</code> plugin, which provides an equivalent behavior, is disabled.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> <p>The <code>PodTopologySpread</code> plugin does not score the nodes that don't have the topology keys specified in the spreading constraints. This might result in a different default behavior compared to the legacy <code>SelectorSpread</code> plugin when using the default topology constraints.</p> <p>If your nodes are not expected to have <strong>both</strong> <code>kubernetes.io/hostname</code> and <code>topology.kubernetes.io/zone</code> labels set, define your own constraints instead of using the Kubernetes defaults.</p> </div> <p>If you don't want to use the default Pod spreading constraints for your cluster, you can disable those defaults by setting <code>defaultingType</code> to <code>List</code> and leaving empty <code>defaultConstraints</code> in the <code>PodTopologySpread</code> plugin configuration:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4" data-language="yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration

profiles:
  - pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints: []
          defaultingType: List
</pre></div>
<h2 id="comparison-with-podaffinity-podantiaffinity">Comparison with PodAffinity/PodAntiAffinity</h2> <p>In Kubernetes, directives related to "Affinity" control how Pods are scheduled - more packed or more scattered.</p> <ul> <li>For <code>PodAffinity</code>, you can try to pack any number of Pods into qualifying topology domain(s)</li> <li>For <code>PodAntiAffinity</code>, only one Pod can be scheduled into a single topology domain.</li> </ul> <p>For finer control, you can specify topology spread constraints to distribute Pods across different topology domains - to achieve either high availability or cost-saving. This can also help on rolling update workloads and scaling out replicas smoothly. See <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation">Motivation</a> for more details.</p> <h2 id="known-limitations">Known Limitations</h2> <ul> <li>There's no guarantee that the constraints remain satisfied when Pods are removed. For example, scaling down a Deployment may result in imbalanced Pods distribution. You can use <a href="https://github.com/kubernetes-sigs/descheduler">Descheduler</a> to rebalance the Pods distribution.</li> <li>Pods matched on tainted nodes are respected. See <a href="https://github.com/kubernetes/kubernetes/issues/80921">Issue 80921</a>
</li> </ul> <h2 id="what-s-next">What's next</h2> <ul> <li>
<a href="https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/">Blog: Introducing PodTopologySpread</a> explains <code>maxSkew</code> in details, as well as bringing up some advanced usage examples.</li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2022 The Kubernetes Authors<br>Documentation Distributed under CC BY 4.0.<br>
    <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/" class="_attribution-link">https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
