
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>torch.sparse - PyTorch - W3cubDocs</title>
  
  <meta name="description" content=" Warning ">
  <meta name="keywords" content="torch, sparse, pytorch">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/pytorch/sparse.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/pytorch.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/pytorch/" class="_nav-link" title="" style="margin-left:0;">PyTorch</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _sphinx">
				
				
<h1 id="sparse-docs">torch.sparse</h1> <div class="admonition warning" id="torch-sparse"> <p class="admonition-title">Warning</p> <p>This API is in beta and may change in the near future.</p> </div> <p>Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors for which the majority of elements are zeros.</p> <p>A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor (which cannot be inferred from these tensors!) Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). We would then write:</p> <pre data-language="python">&gt;&gt;&gt; i = torch.LongTensor([[0, 1, 1],
                          [2, 0, 2]])
&gt;&gt;&gt; v = torch.FloatTensor([3, 4, 5])
&gt;&gt;&gt; torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()
 0  0  3
 4  0  5
[torch.FloatTensor of size 2x3]
</pre> <p>Note that the input to LongTensor is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:</p> <pre data-language="python">&gt;&gt;&gt; i = torch.LongTensor([[0, 2], [1, 0], [1, 2]])
&gt;&gt;&gt; v = torch.FloatTensor([3,      4,      5    ])
&gt;&gt;&gt; torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3])).to_dense()
 0  0  3
 4  0  5
[torch.FloatTensor of size 2x3]
</pre> <p>You can also construct hybrid sparse tensors, where only the first n dimensions are sparse, and the rest of the dimensions are dense.</p> <pre data-language="python">&gt;&gt;&gt; i = torch.LongTensor([[2, 4]])
&gt;&gt;&gt; v = torch.FloatTensor([[1, 3], [5, 7]])
&gt;&gt;&gt; torch.sparse.FloatTensor(i, v).to_dense()
 0  0
 0  0
 1  3
 0  0
 5  7
[torch.FloatTensor of size 5x2]
</pre> <p>An empty sparse tensor can be constructed by specifying its size:</p> <pre data-language="python">&gt;&gt;&gt; torch.sparse.FloatTensor(2, 3)
SparseFloatTensor of size 2x3 with indices:
[torch.LongTensor with no dimension]
and values:
[torch.FloatTensor with no dimension]
</pre> <dl class="simple"> <dt>SparseTensor has the following invariants:</dt>
<dd>
<ol class="arabic simple"> <li>sparse_dim + dense_dim = len(SparseTensor.shape)</li> <li>SparseTensor._indices().shape = (sparse_dim, nnz)</li> <li>SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</li> </ol> </dd> </dl> <p>Since SparseTensor._indices() is always a 2D tensor, the smallest sparse_dim = 1. Therefore, representation of a SparseTensor of sparse_dim = 0 is simply a dense tensor.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Our sparse tensor format permits <em>uncoalesced</em> sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. Uncoalesced tensors permit us to implement certain operators more efficiently.</p> <p>For the most part, you shouldn’t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a coalesced or uncoalesced sparse tensor. However, there are two cases in which you may need to care.</p> <p>First, if you repeatedly perform an operation that can produce duplicate entries (e.g., <a class="reference internal" href="#torch.sparse.FloatTensor.add" title="torch.sparse.FloatTensor.add"><code>torch.sparse.FloatTensor.add()</code></a>), you should occasionally coalesce your sparse tensors to prevent them from growing too large.</p> <p>Second, some operators will produce different values depending on whether or not they are coalesced or not (e.g., <a class="reference internal" href="#torch.sparse.FloatTensor._values" title="torch.sparse.FloatTensor._values"><code>torch.sparse.FloatTensor._values()</code></a> and <a class="reference internal" href="#torch.sparse.FloatTensor._indices" title="torch.sparse.FloatTensor._indices"><code>torch.sparse.FloatTensor._indices()</code></a>, as well as <a class="reference internal" href="tensors#torch.Tensor.sparse_mask" title="torch.Tensor.sparse_mask"><code>torch.Tensor.sparse_mask()</code></a>). These operators are prefixed by an underscore to indicate that they reveal internal implementation details and should be used with care, since code that works with coalesced sparse tensors may not work with uncoalesced sparse tensors; generally speaking, it is safest to explicitly coalesce before working with these operators.</p> <p>For example, suppose that we wanted to implement an operator by operating directly on <a class="reference internal" href="#torch.sparse.FloatTensor._values" title="torch.sparse.FloatTensor._values"><code>torch.sparse.FloatTensor._values()</code></a>. Multiplication by a scalar can be implemented in the obvious way, as multiplication distributes over addition; however, square root cannot be implemented directly, since <code>sqrt(a + b) != sqrt(a) +
sqrt(b)</code> (which is what would be computed if you were given an uncoalesced tensor.)</p> </div> <dl class="class"> <dt id="torch.sparse.FloatTensor">
<code>class torch.sparse.FloatTensor</code> </dt> <dd>
<dl class="method"> <dt id="torch.sparse.FloatTensor.add">
<code>add()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.add_">
<code>add_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.clone">
<code>clone()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.dim">
<code>dim()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.div">
<code>div()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.div_">
<code>div_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.get_device">
<code>get_device()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.hspmm">
<code>hspmm()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.mm">
<code>mm()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.mul">
<code>mul()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.mul_">
<code>mul_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.narrow_copy">
<code>narrow_copy()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.resizeAs_">
<code>resizeAs_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.size">
<code>size()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.spadd">
<code>spadd()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.spmm">
<code>spmm()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.sspaddmm">
<code>sspaddmm()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.sspmm">
<code>sspmm()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.sub">
<code>sub()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.sub_">
<code>sub_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.t_">
<code>t_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.to_dense">
<code>to_dense()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.transpose">
<code>transpose()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.transpose_">
<code>transpose_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.zero_">
<code>zero_()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.coalesce">
<code>coalesce()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor.is_coalesced">
<code>is_coalesced()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor._indices">
<code>_indices()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor._values">
<code>_values()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.sparse.FloatTensor._nnz">
<code>_nnz()</code> </dt> 
</dl> </dd>
</dl>  <h2 id="functions">Functions</h2> <dl class="function"> <dt id="torch.sparse.addmm">
<code>torch.sparse.addmm(mat: torch.Tensor, mat1: torch.Tensor, mat2: torch.Tensor, beta: float = 1.0, alpha: float = 1.0) → torch.Tensor</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#addmm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This function does exact same thing as <a class="reference internal" href="generated/torch.addmm#torch.addmm" title="torch.addmm"><code>torch.addmm()</code></a> in the forward, except that it supports backward for sparse matrix <code>mat1</code>. <code>mat1</code> need to have <code>sparse_dim = 2</code>. Note that the gradients of <code>mat1</code> is a coalesced sparse tensor.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>mat</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – a dense matrix to be added</li> <li>
<strong>mat1</strong> (<em>SparseTensor</em>) – a sparse matrix to be multiplied</li> <li>
<strong>mat2</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – a dense matrix be multiplied</li> <li>
<strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <code>mat</code> (<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span></span> </span>)</li> <li>
<strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>t</mi><mn>1</mn><mi mathvariant="normal">@</mi><mi>m</mi><mi>a</mi><mi>t</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">mat1 @ mat2</annotation></semantics></math></span></span> </span> (<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span></span> </span>)</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.sparse.mm">
<code>torch.sparse.mm(mat1: torch.Tensor, mat2: torch.Tensor) → torch.Tensor</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#mm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Performs a matrix multiplication of the sparse matrix <code>mat1</code> and dense matrix <code>mat2</code>. Similar to <a class="reference internal" href="generated/torch.mm#torch.mm" title="torch.mm"><code>torch.mm()</code></a>, If <code>mat1</code> is a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>×</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n \times m)</annotation></semantics></math></span></span> </span> tensor, <code>mat2</code> is a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>×</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(m \times p)</annotation></semantics></math></span></span> </span> tensor, out will be a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>×</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n \times p)</annotation></semantics></math></span></span> </span> dense tensor. <code>mat1</code> need to have <code>sparse_dim = 2</code>. This function also supports backward for both matrices. Note that the gradients of <code>mat1</code> is a coalesced sparse tensor.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>mat1</strong> (<em>SparseTensor</em>) – the first sparse matrix to be multiplied</li> <li>
<strong>mat2</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the second dense matrix to be multiplied</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(2, 3).to_sparse().requires_grad_(True)
&gt;&gt;&gt; a
tensor(indices=tensor([[0, 0, 0, 1, 1, 1],
                       [0, 1, 2, 0, 1, 2]]),
       values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),
       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)

&gt;&gt;&gt; b = torch.randn(3, 2, requires_grad=True)
&gt;&gt;&gt; b
tensor([[-0.6479,  0.7874],
        [-1.2056,  0.5641],
        [-1.1716, -0.9923]], requires_grad=True)

&gt;&gt;&gt; y = torch.sparse.mm(a, b)
&gt;&gt;&gt; y
tensor([[-0.3323,  1.8723],
        [-1.8951,  0.7904]], grad_fn=&lt;SparseAddmmBackward&gt;)
&gt;&gt;&gt; y.sum().backward()
&gt;&gt;&gt; a.grad
tensor(indices=tensor([[0, 0, 0, 1, 1, 1],
                       [0, 1, 2, 0, 1, 2]]),
       values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),
       size=(2, 3), nnz=6, layout=torch.sparse_coo)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.sparse.sum">
<code>torch.sparse.sum(input: torch.Tensor, dim: Optional[Tuple[int]] = None, dtype: Optional[int] = None) → torch.Tensor</code> <a class="reference internal" href="https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#sum"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the sum of each row of SparseTensor <code>input</code> in the given dimensions <code>dim</code>. If <code>dim</code> is a list of dimensions, reduce over all of them. When sum over all <code>sparse_dim</code>, this method returns a Tensor instead of SparseTensor.</p> <p>All summed <code>dim</code> are squeezed (see <a class="reference internal" href="generated/torch.squeeze#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting an output tensor having <code>dim</code> fewer dimensions than <code>input</code>.</p> <p>During backward, only gradients at <code>nnz</code> locations of <code>input</code> will propagate back. Note that the gradients of <code>input</code> is coalesced.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input SparseTensor</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><em>tuple of python:ints</em>) – a dimension or a list of dimensions to reduce. Default: reduce over all dims.</li> <li>
<strong>dtype</strong> (<code>torch.dtype</code>, optional) – the desired data type of returned Tensor. Default: dtype of <code>input</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; nnz = 3
&gt;&gt;&gt; dims = [5, 5, 2, 3]
&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),
                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)
&gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3])
&gt;&gt;&gt; size = torch.Size(dims)
&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size)
&gt;&gt;&gt; S
tensor(indices=tensor([[2, 0, 3],
                       [2, 4, 1]]),
       values=tensor([[[-0.6438, -1.6467,  1.4004],
                       [ 0.3411,  0.0918, -0.2312]],

                      [[ 0.5348,  0.0634, -2.0494],
                       [-0.7125, -1.0646,  2.1844]],

                      [[ 0.1276,  0.1874, -0.6334],
                       [-1.9682, -0.5340,  0.7483]]]),
       size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)

# when sum over only part of sparse_dims, return a SparseTensor
&gt;&gt;&gt; torch.sparse.sum(S, [1, 3])
tensor(indices=tensor([[0, 2, 3]]),
       values=tensor([[-1.4512,  0.4073],
                      [-0.8901,  0.2017],
                      [-0.3183, -1.7539]]),
       size=(5, 2), nnz=3, layout=torch.sparse_coo)

# when sum over all sparse dim, return a dense Tensor
# with summed dims squeezed
&gt;&gt;&gt; torch.sparse.sum(S, [0, 1, 3])
tensor([-2.6596, -1.1450])
</pre> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.7.0/sparse.html" class="_attribution-link">https://pytorch.org/docs/1.7.0/sparse.html</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
