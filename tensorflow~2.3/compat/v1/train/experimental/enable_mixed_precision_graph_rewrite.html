
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite - TensorFlow 2.3 - W3cubDocs</title>
  
  <meta name="description" content=" Enable mixed precision via a graph rewrite. ">
  <meta name="keywords" content="tf, compat, train, experimental, enable, mixed, precision, graph, rewrite, tensorflow, tensorflow~2.3">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.3/compat/v1/train/experimental/enable_mixed_precision_graph_rewrite.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~2.3.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.3/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.3</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite</h1>       <p>Enable mixed precision via a graph rewrite.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite(
    opt, loss_scale='dynamic'
)
</pre>  <p>Mixed precision is the use of both float32 and float16 data types when training a model to improve performance. This is achieved via a graph rewrite operation and a loss-scale optimizer.</p> <p>Performing arithmetic operations in float16 takes advantage of specialized processing units, such as NVIDIA Tensor Cores, for much higher arithmetic throughput. However, due to the smaller representable range, performing the entire training with float16 can result in gradient underflow, that is, small gradient values becoming zeroes. Instead, performing only select arithmetic operations in float16 results in higher throughput and decreased training time when using compatible hardware accelerators while also reducing memory usage, typically without sacrificing model accuracy.</p> <blockquote class="note">
<strong>Note:</strong><span> While the mixed precision rewrite changes the datatype of various layers throughout the model, the same accuracy reached in float32 is expected. If a <code translate="no" dir="ltr">NaN</code> gradient occurs with dynamic loss scaling, the model update for that batch is skipped. In this case, the global step count is not incremented, and the <code translate="no" dir="ltr">LossScaleOptimizer</code> attempts to decrease the loss scaling value to avoid <code translate="no" dir="ltr">NaN</code> values in subsequent iterations. This approach has been shown to achieve the same accuracy as float32 and, in most cases, better training throughput.</span>
</blockquote> <h4 id="example" data-text="Example:" tabindex="0">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='softmax'),
])

opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)
model.compile(loss="mse", optimizer=opt)

x_train = np.random.random((1024, 64))
y_train = np.random.random((1024, 64))
model.fit(x_train, y_train)
</pre> <p>Calling <code translate="no" dir="ltr">enable_mixed_precision_graph_rewrite(opt)</code> enables the graph rewrite operation before computing gradients. The function additionally returns an <code translate="no" dir="ltr">Optimizer</code> (<code translate="no" dir="ltr">opt</code>) wrapped with a <code translate="no" dir="ltr">LossScaleOptimizer</code>. This prevents underflow in the float16 tensors during the backward pass. An optimizer of type <code translate="no" dir="ltr">tf.train.Optimizer</code> or <a href="../../../../keras/optimizers/optimizer"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> must be passed to this function, which will then be wrapped to use loss scaling.</p> <p>The graph rewrite operation changes the <code translate="no" dir="ltr">dtype</code> of certain operations in the graph from float32 to float16. There are several categories of operations that are either included or excluded by this rewrite operation. The following categories of Ops are defined inside corresponding functions under the class <code translate="no" dir="ltr">AutoMixedPrecisionLists</code> in <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/auto_mixed_precision_lists.h"> auto_mixed_precision_lists.h</a>:</p> <ul> <li>
<code translate="no" dir="ltr">ClearList</code>: Ops that do not have numerically significant adverse effects. E.g. <code translate="no" dir="ltr">ArgMax</code> and <code translate="no" dir="ltr">Floor</code>.</li> <li>
<code translate="no" dir="ltr">WhiteList</code>: Ops that are considered numerically safe for execution in float16, and thus are always converted. E.g. <code translate="no" dir="ltr">Conv2D</code>.</li> <li>
<code translate="no" dir="ltr">BlackList</code>: Ops that are numerically unsafe to execute in float16 and can negatively affect downstream nodes. E.g. <code translate="no" dir="ltr">Softmax</code>.</li> <li>
<code translate="no" dir="ltr">GrayList</code>: Ops that are considered numerically safe for execution in float16 unless downstream from a BlackList Op. E.g. <code translate="no" dir="ltr">Add</code> and <code translate="no" dir="ltr">AvgPool</code>.</li> </ul> <p>When this function is used, gradients should only be computed and applied with the returned optimizer, either by calling <code translate="no" dir="ltr">opt.minimize()</code> or <code translate="no" dir="ltr">opt.compute_gradients()</code> followed by <code translate="no" dir="ltr">opt.apply_gradients()</code>. Gradients should not be computed with <a href="../../../../gradients"><code translate="no" dir="ltr">tf.gradients</code></a> or <a href="../../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. This is because the returned optimizer will apply loss scaling, and <a href="../../../../gradients"><code translate="no" dir="ltr">tf.gradients</code></a> or <a href="../../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> will not. If you do directly use <a href="../../../../gradients"><code translate="no" dir="ltr">tf.gradients</code></a> or <a href="../../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>, your model may not converge due to float16 underflow problems.</p> <p>When eager execution is enabled, the mixed precision graph rewrite is only enabled within <a href="../../../../function"><code translate="no" dir="ltr">tf.function</code></a>s, as outside <a href="../../../../function"><code translate="no" dir="ltr">tf.function</code></a>s, there is no graph.</p> <p>For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions (such as batch size, input size, output size, and channel counts) should be powers of two if under 256, or otherwise divisible by 8 if above 256. For more information, check out the <a href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html">NVIDIA Deep Learning Performance Guide</a>.</p> <p>Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The parts of the graph on CPUs and TPUs are untouched by the graph rewrite.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">ValueError</code>, if the <a href="../../../../keras/mixed_precision"><code translate="no" dir="ltr">tf.keras.mixed_precision</code></a> API is also used by calling <a href="../../../../keras/mixed_precision/experimental/set_policy"><code translate="no" dir="ltr">tf.keras.mixed_precision.experimental.set_policy</code></a>. Only one mixed precision API can be used. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">opt</code> </td> <td> An instance of a <a href="../../../../keras/optimizers/optimizer"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> or a <code translate="no" dir="ltr">tf.train.Optimizer</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_scale</code> </td> <td> Either an int/float, the string <code translate="no" dir="ltr">"dynamic"</code>, or an instance of a <a href="../../../../mixed_precision/experimental/lossscale"><code translate="no" dir="ltr">tf.mixed_precision.experimental.LossScale</code></a>. The loss scale to use. It is recommended to keep this as its default value of <code translate="no" dir="ltr">"dynamic"</code>, which will adjust the scaling automatically to prevent <code translate="no" dir="ltr">Inf</code> or <code translate="no" dir="ltr">NaN</code> values. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A version of <code translate="no" dir="ltr">opt</code> that will use loss scaling to prevent underflow. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/compat/v1/train/experimental/enable_mixed_precision_graph_rewrite" class="_attribution-link">https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/compat/v1/train/experimental/enable_mixed_precision_graph_rewrite</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
