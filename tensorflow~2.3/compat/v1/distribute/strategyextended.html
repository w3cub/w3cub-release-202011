
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.compat.v1.distribute.StrategyExtended - TensorFlow 2.3 - W3cubDocs</title>
  
  <meta name="description" content=" Additional APIs for algorithms that need to be distribution-aware. ">
  <meta name="keywords" content="tf, compat, distribute, strategyextended, tensorflow, tensorflow~2.3">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.3/compat/v1/distribute/strategyextended.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~2.3.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.3/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.3</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.compat.v1.distribute.StrategyExtended</h1>       <p>Additional APIs for algorithms that need to be distribution-aware.</p> <p>Inherits From: <a href="../../../distribute/strategyextended"><code translate="no" dir="ltr">StrategyExtended</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.distribute.StrategyExtended(
    container_strategy
)
</pre>  <blockquote class="note">
<strong>Note:</strong><span> For most usage of <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>, there should be no need to call these methods, since TensorFlow libraries (such as optimizers) already call these methods when needed on your behalf.</span>
</blockquote> <p>Some common use cases of functions on this page:</p> <ul> <li><em>Locality</em></li> </ul> <p><a href="../../../distribute/distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> can have the same <em>locality</em> as a <em>distributed variable</em>, which leads to a mirrored value residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to <a href="../../../distribute/strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update</code></a> to update the value of a variable. You may use <a href="../../../distribute/strategyextended#colocate_vars_with"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.colocate_vars_with</code></a> to give a variable the same locality as another variable. You may convert a "PerReplica" value to a variable's locality by using <a href="../../../distribute/strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a> or <a href="../../../distribute/strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>.</p> <ul> <li><em>How to update a distributed variable</em></li> </ul> <p>A distributed variable is variables created on multiple devices. As discussed in the <a href="https://www.tensorflow.org/api_docs/python/tf/distribute">glossary</a>, mirrored variable and SyncOnRead variable are two examples. The standard pattern for updating distributed variables is to:</p> <ol> <li>In your function passed to <a href="../../../distribute/strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>, compute a list of (update, variable) pairs. For example, the update might be a gradient of the loss with respect to the variable.</li> <li>Switch to cross-replica mode by calling <code translate="no" dir="ltr">tf.distribute.get_replica_context().merge_call()</code> with the updates and variables as arguments.</li> <li>Call <a href="../../../distribute/strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to(VariableAggregation.SUM, t, v)</code></a> (for one variable) or <a href="../../../distribute/strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a> (for a list of variables) to sum the updates.</li> <li>Call <a href="../../../distribute/strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update(v)</code></a> for each variable to update its value.</li> </ol> <p>Steps 2 through 4 are done automatically by class <a href="../../../keras/optimizers/optimizer"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> if you call its <a href="../../../keras/optimizers/optimizer#apply_gradients"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer.apply_gradients</code></a> method in a replica context.</p> <p>In fact, a higher-level solution to update a distributed variable is by calling <code translate="no" dir="ltr">assign</code> on the variable as you would do to a regular <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a>. You can call the method in both <em>replica context</em> and <em>cross-replica context</em>. For a <em>mirrored variable</em>, calling <code translate="no" dir="ltr">assign</code> in <em>replica context</em> requires you to specify the <code translate="no" dir="ltr">aggregation</code> type in the variable constructor. In that case, the context switching and sync described in steps 2 through 4 are handled for you. If you call <code translate="no" dir="ltr">assign</code> on <em>mirrored variable</em> in <em>cross-replica context</em>, you can only assign a single value or assign values from another mirrored variable or a mirrored <a href="../../../distribute/distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. For a <em>SyncOnRead variable</em>, in <em>replica context</em>, you can simply call <code translate="no" dir="ltr">assign</code> on it and no aggregation happens under the hood. In <em>cross-replica context</em>, you can only assign a single value to a SyncOnRead variable. One example case is restoring from a checkpoint: if the <code translate="no" dir="ltr">aggregation</code> type of the variable is <a href="../../../variableaggregation#SUM"><code translate="no" dir="ltr">tf.VariableAggregation.SUM</code></a>, it is assumed that replica values were added before checkpointing, so at the time of restoring, the value is divided by the number of replicas and then assigned to each replica; if the <code translate="no" dir="ltr">aggregation</code> type is <a href="../../../variableaggregation#MEAN"><code translate="no" dir="ltr">tf.VariableAggregation.MEAN</code></a>, the value is assigned to each replica directly.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">experimental_between_graph</code> </td> <td> Whether the strategy uses between-graph replication or not. <p>This is expected to return a constant value that will not be changed throughout its life cycle. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_require_static_shapes</code> </td> <td> Returns <code translate="no" dir="ltr">True</code> if static shape is required; <code translate="no" dir="ltr">False</code> otherwise. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_should_init</code> </td> <td> Whether initialization is needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parameter_devices</code> </td> <td> Returns the tuple of all devices used to place variables. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">should_checkpoint</code> </td> <td> Whether checkpointing is needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">should_save_summary</code> </td> <td> Whether saving summaries is needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">worker_devices</code> </td> <td> Returns the tuple of all devices used to for compute replica execution. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="batch_reduce_to" data-text="batch_reduce_to" tabindex="0"><code translate="no" dir="ltr">batch_reduce_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2199-L2223">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
batch_reduce_to(
    reduce_op, value_destination_pairs, experimental_hints=None
)
</pre> <p>Combine multiple <code translate="no" dir="ltr">reduce_to</code> calls into one for faster execution.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> Reduction type, an instance of <a href="../../../distribute/reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> enum. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value_destination_pairs</code> </td> <td> A sequence of (value, destinations) pairs. See <code translate="no" dir="ltr">reduce_to()</code> for a description. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_hints</code> </td> <td> A <code translate="no" dir="ltr">tf.distrbute.experimental.CollectiveHints</code>. Hints to perform collective operations. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of mirrored values, one per pair in <code translate="no" dir="ltr">value_destination_pairs</code>. </td> </tr> 
</table> <h3 id="broadcast_to" data-text="broadcast_to" tabindex="0"><code translate="no" dir="ltr">broadcast_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2466-L2481">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
broadcast_to(
    tensor, destinations
)
</pre> <p>Mirror a tensor on one device to all worker devices.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor</code> </td> <td> A Tensor value to broadcast. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">destinations</code> </td> <td> A mirrored variable or device string specifying the destination devices to copy <code translate="no" dir="ltr">tensor</code> to. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A value mirrored to <code translate="no" dir="ltr">destinations</code> devices. </td> </tr> 
</table> <h3 id="call_for_each_replica" data-text="call_for_each_replica" tabindex="0"><code translate="no" dir="ltr">call_for_each_replica</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2537-L2585">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
call_for_each_replica(
    fn, args=(), kwargs=None
)
</pre> <p>Run <code translate="no" dir="ltr">fn</code> once per replica.</p> <p><code translate="no" dir="ltr">fn</code> may call <code translate="no" dir="ltr">tf.get_replica_context()</code> to access methods such as <code translate="no" dir="ltr">replica_id_in_sync_group</code> and <code translate="no" dir="ltr">merge_call()</code>.</p> <p><code translate="no" dir="ltr">merge_call()</code> is used to communicate between the replicas and re-enter the cross-replica context. All replicas pause their execution having encountered a <code translate="no" dir="ltr">merge_call()</code> call. After that the <code translate="no" dir="ltr">merge_fn</code>-function is executed. Its results are then unwrapped and given back to each replica call. After that execution resumes until <code translate="no" dir="ltr">fn</code> is complete or encounters another <code translate="no" dir="ltr">merge_call()</code>. Example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Called once in "cross-replica" context.
def merge_fn(distribution, three_plus_replica_id):
  # sum the values across replicas
  return sum(distribution.experimental_local_results(three_plus_replica_id))

# Called once per replica in `distribution`, in a "replica" context.
def fn(three):
  replica_ctx = tf.get_replica_context()
  v = three + replica_ctx.replica_id_in_sync_group
  # Computes the sum of the `v` values across all replicas.
  s = replica_ctx.merge_call(merge_fn, args=(v,))
  return s + v

with distribution.scope():
  # in "cross-replica" context
  ...
  merged_results = distribution.run(fn, args=[3])
  # merged_results has the values from every replica execution of `fn`.
  # This statement prints a list:
  print(distribution.experimental_local_results(merged_results))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> function to run (will be run once per replica). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> Tuple or list with positional arguments for <code translate="no" dir="ltr">fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword arguments for <code translate="no" dir="ltr">fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Merged return value of <code translate="no" dir="ltr">fn</code> across all replicas. </td> </tr> 
</table> <h3 id="colocate_vars_with" data-text="colocate_vars_with" tabindex="0"><code translate="no" dir="ltr">colocate_vars_with</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2083-L2128">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
colocate_vars_with(
    colocate_with_variable
)
</pre> <p>Scope that controls which devices variables will be created on.</p> <p>No operations should be added to the graph inside this scope, it should only be used when creating variables (some implementations work by changing variable creation, others work by using a tf.compat.v1.colocate_with() scope).</p> <p>This may only be used inside <code translate="no" dir="ltr">self.scope()</code>.</p> <h4 id="example_usage" data-text="Example usage:" tabindex="0">Example usage:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">with strategy.scope():
  var1 = tf.Variable(...)
  with strategy.extended.colocate_vars_with(var1):
    # var2 and var3 will be created on the same device(s) as var1
    var2 = tf.Variable(...)
    var3 = tf.Variable(...)

  def fn(v1, v2, v3):
    # operates on v1 from var1, v2 from var2, and v3 from var3

  # `fn` runs on every device `var1` is on, `var2` and `var3` will be there
  # too.
  strategy.extended.update(var1, fn, args=(var2, var3))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">colocate_with_variable</code> </td> <td> A variable created in this strategy's <code translate="no" dir="ltr">scope()</code>. Variables created while in the returned context manager will be on the same set of devices as <code translate="no" dir="ltr">colocate_with_variable</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A context manager. </td> </tr> 
</table> <h3 id="experimental_make_numpy_dataset" data-text="experimental_make_numpy_dataset" tabindex="0"><code translate="no" dir="ltr">experimental_make_numpy_dataset</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2443-L2461">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
experimental_make_numpy_dataset(
    numpy_input, session=None
)
</pre> <p>Makes a dataset for input provided via a numpy array.</p> <p>This avoids adding <code translate="no" dir="ltr">numpy_input</code> as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">numpy_input</code> </td> <td> A nest of NumPy input arrays that will be distributed evenly across all replicas. Note that lists of Numpy arrays are stacked, as that is normal <a href="../../../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> behavior. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">session</code> </td> <td> (TensorFlow v1.x graph execution only) A session used for initialization. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="../../../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> representing <code translate="no" dir="ltr">numpy_input</code>. </td> </tr> 
</table> <h3 id="experimental_run_steps_on_iterator" data-text="experimental_run_steps_on_iterator" tabindex="0"><code translate="no" dir="ltr">experimental_run_steps_on_iterator</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2486-L2531">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
experimental_run_steps_on_iterator(
    fn, iterator, iterations=1, initial_loop_values=None
)
</pre> <p>DEPRECATED: please use <code translate="no" dir="ltr">run</code> instead.</p> <p>Run <code translate="no" dir="ltr">fn</code> with input from <code translate="no" dir="ltr">iterator</code> for <code translate="no" dir="ltr">iterations</code> times.</p> <p>This method can be used to run a step function for training a number of times using input from a dataset.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> function to run using this distribution strategy. The function must have the following signature: <code translate="no" dir="ltr">def fn(context, inputs)</code>. <code translate="no" dir="ltr">context</code> is an instance of <code translate="no" dir="ltr">MultiStepContext</code> that will be passed when <code translate="no" dir="ltr">fn</code> is run. <code translate="no" dir="ltr">context</code> can be used to specify the outputs to be returned from <code translate="no" dir="ltr">fn</code> by calling <code translate="no" dir="ltr">context.set_last_step_output</code>. It can also be used to capture non tensor outputs by <code translate="no" dir="ltr">context.set_non_tensor_output</code>. See <code translate="no" dir="ltr">MultiStepContext</code> documentation for more information. <code translate="no" dir="ltr">inputs</code> will have same type/structure as <code translate="no" dir="ltr">iterator.get_next()</code>. Typically, <code translate="no" dir="ltr">fn</code> will use <code translate="no" dir="ltr">call_for_each_replica</code> method of the strategy to distribute the computation over multiple replicas. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">iterator</code> </td> <td> Iterator of a dataset that represents the input for <code translate="no" dir="ltr">fn</code>. The caller is responsible for initializing the iterator as needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">iterations</code> </td> <td> (Optional) Number of iterations that <code translate="no" dir="ltr">fn</code> should be run. Defaults to 1. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initial_loop_values</code> </td> <td> (Optional) Initial values to be passed into the loop that runs <code translate="no" dir="ltr">fn</code>. Defaults to <code translate="no" dir="ltr">None</code>. initial_loop_values argument when we have a mechanism to infer the outputs of <code translate="no" dir="ltr">fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Returns the <code translate="no" dir="ltr">MultiStepContext</code> object which has the following properties, among other things: <ul> <li>run_op: An op that runs <code translate="no" dir="ltr">fn</code> <code translate="no" dir="ltr">iterations</code> times.</li> <li>last_step_outputs: A dictionary containing tensors set using <code translate="no" dir="ltr">context.set_last_step_output</code>. Evaluating this returns the value of the tensors after the last iteration.</li> <li>non_tensor_outputs: A dictionary containing anything that was set by <code translate="no" dir="ltr">fn</code> by calling <code translate="no" dir="ltr">context.set_non_tensor_output</code>. </li>
</ul>
</td> </tr> 
</table> <h3 id="read_var" data-text="read_var" tabindex="0"><code translate="no" dir="ltr">read_var</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2590-L2603">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
read_var(
    v
)
</pre> <p>Reads the value of a variable.</p> <p>Returns the aggregate value of a replica-local variable, or the (read-only) value of any other variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">v</code> </td> <td> A variable allocated within the scope of this <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor representing the value of <code translate="no" dir="ltr">v</code>, aggregated across replicas if necessary. </td> </tr> 
</table> <h3 id="reduce_to" data-text="reduce_to" tabindex="0"><code translate="no" dir="ltr">reduce_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2168-L2194">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reduce_to(
    reduce_op, value, destinations, experimental_hints=None
)
</pre> <p>Combine (via e.g. sum or mean) values across replicas.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> Reduction type, an instance of <a href="../../../distribute/reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> enum. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> A per-replica value with one value per replica. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">destinations</code> </td> <td> A mirrored variable, a per-replica tensor, or a device string. The return value will be copied to all destination devices (or all the devices where the <code translate="no" dir="ltr">destinations</code> value resides). To perform an all-reduction, pass <code translate="no" dir="ltr">value</code> to <code translate="no" dir="ltr">destinations</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_hints</code> </td> <td> A <code translate="no" dir="ltr">tf.distrbute.experimental.CollectiveHints</code>. Hints to perform collective operations. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor or value mirrored to <code translate="no" dir="ltr">destinations</code>. </td> </tr> 
</table> <h3 id="update" data-text="update" tabindex="0"><code translate="no" dir="ltr">update</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2233-L2300">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
update(
    var, fn, args=(), kwargs=None, group=True
)
</pre> <p>Run <code translate="no" dir="ltr">fn</code> to update <code translate="no" dir="ltr">var</code> using inputs mirrored to the same devices.</p> <p><a href="../../../distribute/strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update</code></a> takes a distributed variable <code translate="no" dir="ltr">var</code> to be updated, an update function <code translate="no" dir="ltr">fn</code>, and <code translate="no" dir="ltr">args</code> and <code translate="no" dir="ltr">kwargs</code> for <code translate="no" dir="ltr">fn</code>. It applies <code translate="no" dir="ltr">fn</code> to each component variable of <code translate="no" dir="ltr">var</code> and passes corresponding values from <code translate="no" dir="ltr">args</code> and <code translate="no" dir="ltr">kwargs</code>. Neither <code translate="no" dir="ltr">args</code> nor <code translate="no" dir="ltr">kwargs</code> may contain per-replica values. If they contain mirrored values, they will be unwrapped before calling <code translate="no" dir="ltr">fn</code>. For example, <code translate="no" dir="ltr">fn</code> can be <code translate="no" dir="ltr">assign_add</code> and <code translate="no" dir="ltr">args</code> can be a mirrored DistributedValues where each component contains the value to be added to this mirrored variable <code translate="no" dir="ltr">var</code>. Calling <code translate="no" dir="ltr">update</code> will call <code translate="no" dir="ltr">assign_add</code> on each component variable of <code translate="no" dir="ltr">var</code> with the corresponding tensor value on that device.</p> <h4 id="example_usage_2" data-text="Example usage:" tabindex="0">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1']) # With 2 devices
with strategy.scope():
  v = tf.Variable(5.0, aggregation=tf.VariableAggregation.SUM)
def update_fn(v):
  return v.assign(1.0)
result = strategy.extended.update(v, update_fn)
# result is
# Mirrored:{
#  0: tf.Tensor(1.0, shape=(), dtype=float32),
#  1: tf.Tensor(1.0, shape=(), dtype=float32)
# }
</pre> <p>If <code translate="no" dir="ltr">var</code> is mirrored across multiple devices, then this method implements logic as following:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">results = {}
for device, v in var:
  with tf.device(device):
    # args and kwargs will be unwrapped if they are mirrored.
    results[device] = fn(v, *args, **kwargs)
return merged(results)
</pre> <p>Otherwise, this method returns <code translate="no" dir="ltr">fn(var, *args, **kwargs)</code> colocated with <code translate="no" dir="ltr">var</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var</code> </td> <td> Variable, possibly mirrored to multiple devices, to operate on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> Function to call. Should take the variable as the first argument. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> Tuple or list. Additional positional arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">group</code> </td> <td> Boolean. Defaults to True. If False, the return value will be unwrapped. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> By default, the merged return value of <code translate="no" dir="ltr">fn</code> across all replicas. The merged result has dependencies to make sure that if it is evaluated at all, the side effects (updates) will happen on every replica. If instead "group=False" is specified, this function will return a nest of lists where each list has an element per replica, and the caller is responsible for ensuring all elements are executed. </td> </tr> 
</table> <h3 id="value_container" data-text="value_container" tabindex="0"><code translate="no" dir="ltr">value_container</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2337-L2350">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
value_container(
    value
)
</pre> <p>Returns the container that this per-replica <code translate="no" dir="ltr">value</code> belongs to.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> A value returned by <code translate="no" dir="ltr">run()</code> or a variable created in <code translate="no" dir="ltr">scope()</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A container that <code translate="no" dir="ltr">value</code> belongs to. If value does not belong to any container (including the case of container having been destroyed), returns the value itself. <code translate="no" dir="ltr">value in experimental_local_results(value_container(value))</code> will always be true. </td> </tr> 
</table> <h3 id="variable_created_in_scope" data-text="variable_created_in_scope" tabindex="0"><code translate="no" dir="ltr">variable_created_in_scope</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/distribute/distribute_lib.py#L2057-L2081">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variable_created_in_scope(
    v
)
</pre> <p>Tests whether <code translate="no" dir="ltr">v</code> was created while this strategy scope was active.</p> <p>Variables created inside the strategy scope are "owned" by it:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
  v = tf.Variable(1.)
strategy.extended.variable_created_in_scope(v)
True
</pre> <p>Variables created outside the strategy are not owned by it:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy()
v = tf.Variable(1.)
strategy.extended.variable_created_in_scope(v)
False
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">v</code> </td> <td> A <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a> instance. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> True if <code translate="no" dir="ltr">v</code> was created inside the scope, False if not. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/compat/v1/distribute/StrategyExtended" class="_attribution-link">https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/compat/v1/distribute/StrategyExtended</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
