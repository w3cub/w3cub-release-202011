
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.keras.optimizers.Adamax - TensorFlow 2.3 - W3cubDocs</title>
  
  <meta name="description" content=" Optimizer that implements the Adamax algorithm. ">
  <meta name="keywords" content="tf, keras, optimizers, adamax, tensorflow, tensorflow~2.3">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~2.3/keras/optimizers/adamax.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~2.3.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~2.3/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 2.3</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.keras.optimizers.Adamax</h1>      <table class="tfo-notebook-buttons tfo-api" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/adamax.py#L33-L188">  View source on GitHub </a> </td> </table> <p>Optimizer that implements the Adamax algorithm.</p> <p>Inherits From: <a href="optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="0">View aliases</h4> <p> <b>Main aliases</b> </p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax"><code translate="no" dir="ltr">tf.optimizers.Adamax</code></a></p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax"><code translate="no" dir="ltr">tf.compat.v1.keras.optimizers.Adamax</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.optimizers.Adamax(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Adamax',
    **kwargs
)
</pre>  <p>It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper. Adamax is sometimes superior to adam, specially in models with embeddings.</p> <h4 id="initialization" data-text="Initialization:" tabindex="0">Initialization:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">m = 0  # Initialize initial 1st moment vector
v = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
</pre> <p>The update rule for parameter <code translate="no" dir="ltr">w</code> with gradient <code translate="no" dir="ltr">g</code> is described at the end of section 7.1 of the paper:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">t += 1
m = beta1 * m + (1 - beta) * g
v = max(beta2 * v, abs(g))
current_lr = learning_rate / (1 - beta1 ** t)
w = w - current_lr * m / (v + epsilon)
</pre> <p>Similarly to <code translate="no" dir="ltr">Adam</code>, the epsilon is added for numerical stability (especially to get rid of division by zero when <code translate="no" dir="ltr">v_t == 0</code>).</p> <p>In contrast to <code translate="no" dir="ltr">Adam</code>, the sparse implementation of this algorithm (used when the gradient is an IndexedSlices object, typically because of <a href="../../gather"><code translate="no" dir="ltr">tf.gather</code></a> or an embedding lookup in the forward pass) only updates variable slices and corresponding <code translate="no" dir="ltr">m_t</code>, <code translate="no" dir="ltr">v_t</code> terms when that part of the variable was used in the forward pass. This means that the sparse behavior is contrast to the dense behavior (similar to some momentum implementations which ignore momentum unless a variable slice was actually used).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code>, floating point value, or a schedule that is a <a href="schedules/learningrateschedule"><code translate="no" dir="ltr">tf.keras.optimizers.schedules.LearningRateSchedule</code></a>. The learning rate. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_1</code> </td> <td> A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_2</code> </td> <td> A float value or a constant float tensor. The exponential decay rate for the exponentially weighted infinity norm. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">epsilon</code> </td> <td> A small constant for numerical stability. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the operations created when applying gradients. Defaults to <code translate="no" dir="ltr">"Adamax"</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Keyword arguments. Allowed to be one of <code translate="no" dir="ltr">"clipnorm"</code> or <code translate="no" dir="ltr">"clipvalue"</code>. <code translate="no" dir="ltr">"clipnorm"</code> (float) clips gradients by norm; <code translate="no" dir="ltr">"clipvalue"</code> (float) clips gradients by value. </td> </tr> </table> <h4 id="reference" data-text="Reference:" tabindex="0">Reference:</h4> <ul> <li><a href="http://arxiv.org/abs/1412.6980">Kingma et al., 2014</a></li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A non-empty string. The name to use for accumulators created for the optimizer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> keyword arguments. Allowed to be {<code translate="no" dir="ltr">clipnorm</code>, <code translate="no" dir="ltr">clipvalue</code>, <code translate="no" dir="ltr">lr</code>, <code translate="no" dir="ltr">decay</code>}. <code translate="no" dir="ltr">clipnorm</code> is clip gradients by norm; <code translate="no" dir="ltr">clipvalue</code> is clip gradients by value, <code translate="no" dir="ltr">decay</code> is included for backward compatibility to allow time inverse decay of learning rate. <code translate="no" dir="ltr">lr</code> is included for backward compatibility, recommended to use <code translate="no" dir="ltr">learning_rate</code> instead. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If name is malformed. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">iterations</code> </td> <td> Variable. The number of training steps this Optimizer has run. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> Returns variables of this Optimizer based on the order created. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="add_slot" data-text="add_slot" tabindex="0"><code translate="no" dir="ltr">add_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L735-L771">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_slot(
    var, slot_name, initializer='zeros'
)
</pre> <p>Add a new slot variable for <code translate="no" dir="ltr">var</code>.</p> <h3 id="add_weight" data-text="add_weight" tabindex="0"><code translate="no" dir="ltr">add_weight</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1003-L1043">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_weight(
    name, shape, dtype=None, initializer='zeros', trainable=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
</pre> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="0"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L473-L550">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
apply_gradients(
    grads_and_vars, name=None, experimental_aggregate_gradients=True
)
</pre> <p>Apply gradients to variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p> <p>The method sums gradients from all replicas in the presence of <a href="../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> by default. You can aggregate gradients yourself by passing <code translate="no" dir="ltr">experimental_aggregate_gradients=False</code>.</p> <h4 id="example" data-text="Example:" tabindex="0">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">grads = tape.gradient(loss, vars)
grads = tf.distribute.get_replica_context().all_reduce('sum', grads)
# Processing aggregated gradients.
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)

</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads_and_vars</code> </td> <td> List of (gradient, variable) pairs. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_aggregate_gradients</code> </td> <td> Whether to sum gradients from different replicas in the presense of <a href="../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. If False, it's user responsibility to aggregate the gradients. Default to True. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">grads_and_vars</code> is malformed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If none of the variables have gradients. </td> </tr> </table> <h3 id="from_config" data-text="from_config" tabindex="0"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L879-L902">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@classmethod
from_config(
    config, custom_objects=None
)
</pre> <p>Creates an optimizer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same optimizer from the config dictionary.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">custom_objects</code> </td> <td> A Python dictionary mapping names to additional Python objects used to create this optimizer, such as a function used for a hyperparameter. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An optimizer instance. </td> </tr> 
</table> <h3 id="get_config" data-text="get_config" tabindex="0"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/adamax.py#L179-L188">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_config()
</pre> <p>Returns the config of the optimizer.</p> <p>An optimizer config is a Python dictionary (serializable) containing the configuration of an optimizer. The same optimizer can be reinstantiated later (without any saved state) from this configuration.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Python dictionary. </td> </tr> 
</table> <h3 id="get_gradients" data-text="get_gradients" tabindex="0"><code translate="no" dir="ltr">get_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L445-L471">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_gradients(
    loss, params
)
</pre> <p>Returns gradients of <code translate="no" dir="ltr">loss</code> with respect to <code translate="no" dir="ltr">params</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> Loss tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">params</code> </td> <td> List of variables. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> List of gradient tensors. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> In case any gradient cannot be computed (e.g. if gradient function not implemented). </td> </tr> </table> <h3 id="get_slot" data-text="get_slot" tabindex="0"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L773-L776">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot(
    var, slot_name
)
</pre> <h3 id="get_slot_names" data-text="get_slot_names" tabindex="0"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L731-L733">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot_names()
</pre> <p>A list of names for this optimizer's slots.</p> <h3 id="get_updates" data-text="get_updates" tabindex="0"><code translate="no" dir="ltr">get_updates</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L647-L654">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_updates(
    loss, params
)
</pre> <h3 id="get_weights" data-text="get_weights" tabindex="0"><code translate="no" dir="ltr">get_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L924-L952">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_weights()
</pre> <p>Returns the current weights of the optimizer.</p> <p>The weights of an optimizer are its state (ie, variables). This function returns the weight values associated with this optimizer as a list of Numpy arrays. The first value is always the iterations count of the optimizer, followed by the optimizer's state variables in the order they were created. The returned list can in turn be used to load state into similarly parameterized optimizers.</p> <p>For example, the RMSprop optimizer for this simple model returns a list of three values-- the iteration count, followed by the root-mean-square value of the kernel and bias of the single Dense layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
Training ...
len(opt.get_weights())
3
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Weights values as a list of numpy arrays. </td> </tr> 
</table> <h3 id="minimize" data-text="minimize" tabindex="0"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L348-L377">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
minimize(
    loss, var_list, grad_loss=None, name=None
)
</pre> <p>Minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply computes gradient using <a href="../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and calls <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying then call <a href="../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A callable taking no arguments which returns the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>, or a callable returning the list or tuple of <code translate="no" dir="ltr">Variable</code> objects. Use callable when the variable list would otherwise be incomplete before <code translate="no" dir="ltr">minimize</code> since the variables are created at the first time <code translate="no" dir="ltr">loss</code> is called. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_loss</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that updates the variables in <code translate="no" dir="ltr">var_list</code>. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects. </td> </tr> </table> <h3 id="set_weights" data-text="set_weights" tabindex="0"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L955-L1001">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_weights(
    weights
)
</pre> <p>Set the weights of the optimizer.</p> <p>The weights of an optimizer are its state (ie, variables). This function takes the weight values associated with this optimizer as a list of Numpy arrays. The first value is always the iterations count of the optimizer, followed by the optimizer's state variables in the order they are created. The passed values are used to set the new state of the optimizer.</p> <p>For example, the RMSprop optimizer for this simple model takes a list of three values-- the iteration count, followed by the root-mean-square value of the kernel and bias of the single Dense layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
Training ...
new_weights = [np.array(10), np.ones([20, 10]), np.zeros([10])]
opt.set_weights(new_weights)
opt.iterations
&lt;tf.Variable 'RMSprop/iter:0' shape=() dtype=int64, numpy=10&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> weight values as a list of numpy arrays. </td> </tr> </table> <h3 id="variables" data-text="variables" tabindex="0"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L915-L917">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variables()
</pre> <p>Returns variables of this Optimizer based on the order created.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/optimizers/Adamax" class="_attribution-link">https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/optimizers/Adamax</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
