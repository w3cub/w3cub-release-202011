
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>TensorFlow 1.15 Documentation - W3cubDocs</title>
  
  <meta name="description" content=" TensorFlow 1.15 documentation ">
  <meta name="keywords" content="module, tf, tensorflow, documentation, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">Module: tf</h1>   <p><devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax> </p>    <p>TensorFlow root package</p> <h2 id="modules" data-text="Modules" tabindex="0">Modules</h2> <p><a href="app"><code translate="no" dir="ltr">app</code></a> module: Generic entry point script.</p> <p><a href="audio"><code translate="no" dir="ltr">audio</code></a> module: Public API for tf.audio namespace.</p> <p><a href="autograph"><code translate="no" dir="ltr">autograph</code></a> module: Conversion of plain Python into TensorFlow graph code.</p> <p><a href="bitwise"><code translate="no" dir="ltr">bitwise</code></a> module: Operations for manipulating the binary representations of integers.</p> <p><a href="compat"><code translate="no" dir="ltr">compat</code></a> module: Functions for Python 2 vs. 3 compatibility.</p> <p><a href="config"><code translate="no" dir="ltr">config</code></a> module: Public API for tf.config namespace.</p> <p><a href="contrib"><code translate="no" dir="ltr">contrib</code></a> module: Contrib module containing volatile or experimental code.</p> <p><a href="data"><code translate="no" dir="ltr">data</code></a> module: <a href="data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> API for input pipelines.</p> <p><a href="debugging"><code translate="no" dir="ltr">debugging</code></a> module: Public API for tf.debugging namespace.</p> <p><a href="distribute"><code translate="no" dir="ltr">distribute</code></a> module: Library for running a computation across multiple devices.</p> <p><a href="distributions"><code translate="no" dir="ltr">distributions</code></a> module: Core module for TensorFlow distribution objects and helpers.</p> <p><a href="dtypes"><code translate="no" dir="ltr">dtypes</code></a> module: Public API for tf.dtypes namespace.</p> <p><a href="errors"><code translate="no" dir="ltr">errors</code></a> module: Exception types for TensorFlow errors.</p> <p><a href="estimator"><code translate="no" dir="ltr">estimator</code></a> module: Estimator: High level tools for working with models.</p> <p><a href="experimental"><code translate="no" dir="ltr">experimental</code></a> module: Public API for tf.experimental namespace.</p> <p><a href="feature_column"><code translate="no" dir="ltr">feature_column</code></a> module: Public API for tf.feature_column namespace.</p> <p><a href="gfile"><code translate="no" dir="ltr">gfile</code></a> module: Import router for file_io.</p> <p><a href="graph_util"><code translate="no" dir="ltr">graph_util</code></a> module: Helpers to manipulate a tensor graph in python.</p> <p><a href="image"><code translate="no" dir="ltr">image</code></a> module: Image processing and decoding ops.</p> <p><a href="initializers"><code translate="no" dir="ltr">initializers</code></a> module: Public API for tf.initializers namespace.</p> <p><a href="io"><code translate="no" dir="ltr">io</code></a> module: Public API for tf.io namespace.</p> <p><a href="keras"><code translate="no" dir="ltr">keras</code></a> module: Implementation of the Keras API meant to be a high-level API for TensorFlow.</p> <p><a href="layers"><code translate="no" dir="ltr">layers</code></a> module: Public API for tf.layers namespace.</p> <p><a href="linalg"><code translate="no" dir="ltr">linalg</code></a> module: Operations for linear algebra.</p> <p><a href="lite"><code translate="no" dir="ltr">lite</code></a> module: Public API for tf.lite namespace.</p> <p><a href="logging"><code translate="no" dir="ltr">logging</code></a> module: Logging and Summary Operations.</p> <p><a href="lookup"><code translate="no" dir="ltr">lookup</code></a> module: Public API for tf.lookup namespace.</p> <p><a href="losses"><code translate="no" dir="ltr">losses</code></a> module: Loss operations for use in neural networks.</p> <p><a href="manip"><code translate="no" dir="ltr">manip</code></a> module: Operators for manipulating tensors.</p> <p><a href="math"><code translate="no" dir="ltr">math</code></a> module: Math Operations.</p> <p><a href="metrics"><code translate="no" dir="ltr">metrics</code></a> module: Evaluation-related metrics.</p> <p><a href="nest"><code translate="no" dir="ltr">nest</code></a> module: Public API for tf.nest namespace.</p> <p><a href="nn"><code translate="no" dir="ltr">nn</code></a> module: Wrappers for primitive Neural Net (NN) Operations.</p> <p><a href="profiler"><code translate="no" dir="ltr">profiler</code></a> module: Public API for tf.profiler namespace.</p> <p><a href="python_io"><code translate="no" dir="ltr">python_io</code></a> module: Python functions for directly manipulating TFRecord-formatted files.</p> <p><a href="quantization"><code translate="no" dir="ltr">quantization</code></a> module: Public API for tf.quantization namespace.</p> <p><a href="queue"><code translate="no" dir="ltr">queue</code></a> module: Public API for tf.queue namespace.</p> <p><a href="ragged"><code translate="no" dir="ltr">ragged</code></a> module: Ragged Tensors.</p> <p><a href="random"><code translate="no" dir="ltr">random</code></a> module: Public API for tf.random namespace.</p> <p><a href="raw_ops"><code translate="no" dir="ltr">raw_ops</code></a> module: Note: <a href="raw_ops"><code translate="no" dir="ltr">tf.raw_ops</code></a> provides direct/low level access to all TensorFlow ops. See <a href="https://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md">the RFC</a></p> <p><a href="resource_loader"><code translate="no" dir="ltr">resource_loader</code></a> module: Resource management library.</p> <p><a href="saved_model"><code translate="no" dir="ltr">saved_model</code></a> module: Public API for tf.saved_model namespace.</p> <p><a href="sets"><code translate="no" dir="ltr">sets</code></a> module: Tensorflow set operations.</p> <p><a href="signal"><code translate="no" dir="ltr">signal</code></a> module: Signal processing operations.</p> <p><a href="sparse"><code translate="no" dir="ltr">sparse</code></a> module: Sparse Tensor Representation.</p> <p><a href="spectral"><code translate="no" dir="ltr">spectral</code></a> module: Public API for tf.spectral namespace.</p> <p><a href="strings"><code translate="no" dir="ltr">strings</code></a> module: Operations for working with string Tensors.</p> <p><a href="summary"><code translate="no" dir="ltr">summary</code></a> module: Operations for writing summary data, for use in analysis and visualization.</p> <p><a href="sysconfig"><code translate="no" dir="ltr">sysconfig</code></a> module: System configuration library.</p> <p><a href="test"><code translate="no" dir="ltr">test</code></a> module: Testing.</p> <p><a href="tpu"><code translate="no" dir="ltr">tpu</code></a> module: Ops related to Tensor Processing Units.</p> <p><a href="train"><code translate="no" dir="ltr">train</code></a> module: Support for training models.</p> <p><a href="user_ops"><code translate="no" dir="ltr">user_ops</code></a> module: Public API for tf.user_ops namespace.</p> <p><a href="version"><code translate="no" dir="ltr">version</code></a> module: Public API for tf.version namespace.</p> <p><a href="xla"><code translate="no" dir="ltr">xla</code></a> module: Public API for tf.xla namespace.</p> <h2 id="classes" data-text="Classes" tabindex="0">Classes</h2> <p><a href="aggregationmethod"><code translate="no" dir="ltr">class AggregationMethod</code></a>: A class listing aggregation methods used to combine gradients.</p> <p><a href="attrvalue"><code translate="no" dir="ltr">class AttrValue</code></a>: A ProtocolMessage</p> <p><a href="conditionalaccumulator"><code translate="no" dir="ltr">class ConditionalAccumulator</code></a>: A conditional accumulator for aggregating gradients.</p> <p><a href="conditionalaccumulatorbase"><code translate="no" dir="ltr">class ConditionalAccumulatorBase</code></a>: A conditional accumulator for aggregating gradients.</p> <p><a href="configproto"><code translate="no" dir="ltr">class ConfigProto</code></a>: A ProtocolMessage</p> <p><a href="criticalsection"><code translate="no" dir="ltr">class CriticalSection</code></a>: Critical section.</p> <p><a href="dtypes/dtype"><code translate="no" dir="ltr">class DType</code></a>: Represents the type of the elements in a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="devicespec"><code translate="no" dir="ltr">class DeviceSpec</code></a>: Represents a (possibly partial) specification for a TensorFlow device.</p> <p><a href="dimension"><code translate="no" dir="ltr">class Dimension</code></a>: Represents the value of one dimension in a TensorShape.</p> <p><a href="event"><code translate="no" dir="ltr">class Event</code></a>: A ProtocolMessage</p> <p><a href="queue/fifoqueue"><code translate="no" dir="ltr">class FIFOQueue</code></a>: A queue implementation that dequeues elements in first-in first-out order.</p> <p><a href="io/fixedlenfeature"><code translate="no" dir="ltr">class FixedLenFeature</code></a>: Configuration for parsing a fixed-length input feature.</p> <p><a href="io/fixedlensequencefeature"><code translate="no" dir="ltr">class FixedLenSequenceFeature</code></a>: Configuration for parsing a variable-length input feature into a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="fixedlengthrecordreader"><code translate="no" dir="ltr">class FixedLengthRecordReader</code></a>: A Reader that outputs fixed-length records from a file.</p> <p><a href="gpuoptions"><code translate="no" dir="ltr">class GPUOptions</code></a>: A ProtocolMessage</p> <p><a href="gradienttape"><code translate="no" dir="ltr">class GradientTape</code></a>: Record operations for automatic differentiation.</p> <p><a href="graph"><code translate="no" dir="ltr">class Graph</code></a>: A TensorFlow computation, represented as a dataflow graph.</p> <p><a href="graphdef"><code translate="no" dir="ltr">class GraphDef</code></a>: A ProtocolMessage</p> <p><a href="graphkeys"><code translate="no" dir="ltr">class GraphKeys</code></a>: Standard names to use for graph collections.</p> <p><a href="graphoptions"><code translate="no" dir="ltr">class GraphOptions</code></a>: A ProtocolMessage</p> <p><a href="histogramproto"><code translate="no" dir="ltr">class HistogramProto</code></a>: A ProtocolMessage</p> <p><a href="identityreader"><code translate="no" dir="ltr">class IdentityReader</code></a>: A Reader that outputs the queued work as both the key and value.</p> <p><a href="indexedslices"><code translate="no" dir="ltr">class IndexedSlices</code></a>: A sparse representation of a set of tensor slices at given indices.</p> <p><a href="indexedslicesspec"><code translate="no" dir="ltr">class IndexedSlicesSpec</code></a>: Type specification for a <a href="indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>.</p> <p><a href="interactivesession"><code translate="no" dir="ltr">class InteractiveSession</code></a>: A TensorFlow <code translate="no" dir="ltr">Session</code> for use in interactive contexts, such as a shell.</p> <p><a href="lmdbreader"><code translate="no" dir="ltr">class LMDBReader</code></a>: A Reader that outputs the records from a LMDB file.</p> <p><a href="logmessage"><code translate="no" dir="ltr">class LogMessage</code></a>: A ProtocolMessage</p> <p><a href="metagraphdef"><code translate="no" dir="ltr">class MetaGraphDef</code></a>: A ProtocolMessage</p> <p><a href="module"><code translate="no" dir="ltr">class Module</code></a>: Base neural network module class.</p> <p><a href="nameattrlist"><code translate="no" dir="ltr">class NameAttrList</code></a>: A ProtocolMessage</p> <p><a href="nodedef"><code translate="no" dir="ltr">class NodeDef</code></a>: A ProtocolMessage</p> <p><a href="errors/operror"><code translate="no" dir="ltr">class OpError</code></a>: A generic error that is raised when TensorFlow execution fails.</p> <p><a href="operation"><code translate="no" dir="ltr">class Operation</code></a>: Represents a graph node that performs computation on tensors.</p> <p><a href="optimizeroptions"><code translate="no" dir="ltr">class OptimizerOptions</code></a>: A ProtocolMessage</p> <p><a href="optionalspec"><code translate="no" dir="ltr">class OptionalSpec</code></a>: Represents an optional potentially containing a structured value.</p> <p><a href="queue/paddingfifoqueue"><code translate="no" dir="ltr">class PaddingFIFOQueue</code></a>: A FIFOQueue that supports batching variable-sized tensors by padding.</p> <p><a href="queue/priorityqueue"><code translate="no" dir="ltr">class PriorityQueue</code></a>: A queue implementation that dequeues elements in prioritized order.</p> <p><a href="queue/queuebase"><code translate="no" dir="ltr">class QueueBase</code></a>: Base class for queue implementations.</p> <p><a href="raggedtensor"><code translate="no" dir="ltr">class RaggedTensor</code></a>: Represents a ragged tensor.</p> <p><a href="raggedtensorspec"><code translate="no" dir="ltr">class RaggedTensorSpec</code></a>: Type specification for a <a href="raggedtensor"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>.</p> <p><a href="queue/randomshufflequeue"><code translate="no" dir="ltr">class RandomShuffleQueue</code></a>: A queue implementation that dequeues elements in a random order.</p> <p><a href="readerbase"><code translate="no" dir="ltr">class ReaderBase</code></a>: Base class for different Reader types, that produce a record every step.</p> <p><a href="registergradient"><code translate="no" dir="ltr">class RegisterGradient</code></a>: A decorator for registering the gradient function for an op type.</p> <p><a href="runmetadata"><code translate="no" dir="ltr">class RunMetadata</code></a>: A ProtocolMessage</p> <p><a href="runoptions"><code translate="no" dir="ltr">class RunOptions</code></a>: A ProtocolMessage</p> <p><a href="session"><code translate="no" dir="ltr">class Session</code></a>: A class for running TensorFlow operations.</p> <p><a href="sessionlog"><code translate="no" dir="ltr">class SessionLog</code></a>: A ProtocolMessage</p> <p><a href="sparse/sparseconditionalaccumulator"><code translate="no" dir="ltr">class SparseConditionalAccumulator</code></a>: A conditional accumulator for aggregating sparse gradients.</p> <p><a href="io/sparsefeature"><code translate="no" dir="ltr">class SparseFeature</code></a>: Configuration for parsing a sparse input feature from an <code translate="no" dir="ltr">Example</code>.</p> <p><a href="sparse/sparsetensor"><code translate="no" dir="ltr">class SparseTensor</code></a>: Represents a sparse tensor.</p> <p><a href="sparsetensorspec"><code translate="no" dir="ltr">class SparseTensorSpec</code></a>: Type specification for a <a href="sparse/sparsetensor"><code translate="no" dir="ltr">tf.SparseTensor</code></a>.</p> <p><a href="sparsetensorvalue"><code translate="no" dir="ltr">class SparseTensorValue</code></a>: SparseTensorValue(indices, values, dense_shape)</p> <p><a href="summary"><code translate="no" dir="ltr">class Summary</code></a>: A ProtocolMessage</p> <p><a href="summarymetadata"><code translate="no" dir="ltr">class SummaryMetadata</code></a>: A ProtocolMessage</p> <p><a href="tfrecordreader"><code translate="no" dir="ltr">class TFRecordReader</code></a>: A Reader that outputs the records from a TFRecords file.</p> <p><a href="tensor"><code translate="no" dir="ltr">class Tensor</code></a>: Represents one of the outputs of an <code translate="no" dir="ltr">Operation</code>.</p> <p><a href="tensorarray"><code translate="no" dir="ltr">class TensorArray</code></a>: Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <p><a href="tensorarrayspec"><code translate="no" dir="ltr">class TensorArraySpec</code></a>: Type specification for a <a href="tensorarray"><code translate="no" dir="ltr">tf.TensorArray</code></a>.</p> <p><a href="tensorinfo"><code translate="no" dir="ltr">class TensorInfo</code></a>: A ProtocolMessage</p> <p><a href="tensorshape"><code translate="no" dir="ltr">class TensorShape</code></a>: Represents the shape of a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="tensorspec"><code translate="no" dir="ltr">class TensorSpec</code></a>: Describes a tf.Tensor.</p> <p><a href="textlinereader"><code translate="no" dir="ltr">class TextLineReader</code></a>: A Reader that outputs the lines of a file delimited by newlines.</p> <p><a href="typespec"><code translate="no" dir="ltr">class TypeSpec</code></a>: Specifies a TensorFlow value type.</p> <p><a href="unconnectedgradients"><code translate="no" dir="ltr">class UnconnectedGradients</code></a>: Controls how gradient computation behaves when y does not depend on x.</p> <p><a href="io/varlenfeature"><code translate="no" dir="ltr">class VarLenFeature</code></a>: Configuration for parsing a variable-length input feature.</p> <p><a href="variable"><code translate="no" dir="ltr">class Variable</code></a>: See the <a href="https://tensorflow.org/guide/variables">Variables Guide</a>.</p> <p><a href="variableaggregation"><code translate="no" dir="ltr">class VariableAggregation</code></a>: Indicates how a distributed variable will be aggregated.</p> <p><a href="variablescope"><code translate="no" dir="ltr">class VariableScope</code></a>: Variable scope object to carry defaults to provide to <code translate="no" dir="ltr">get_variable</code>.</p> <p><a href="variablesynchronization"><code translate="no" dir="ltr">class VariableSynchronization</code></a>: Indicates when a distributed variable will be synced.</p> <p><a href="wholefilereader"><code translate="no" dir="ltr">class WholeFileReader</code></a>: A Reader that outputs the entire contents of a file as a value.</p> <p><a href="initializers/constant"><code translate="no" dir="ltr">class constant_initializer</code></a>: Initializer that generates tensors with constant values.</p> <p><a href="glorot_normal_initializer"><code translate="no" dir="ltr">class glorot_normal_initializer</code></a>: The Glorot normal initializer, also called Xavier normal initializer.</p> <p><a href="glorot_uniform_initializer"><code translate="no" dir="ltr">class glorot_uniform_initializer</code></a>: The Glorot uniform initializer, also called Xavier uniform initializer.</p> <p><a href="name_scope"><code translate="no" dir="ltr">class name_scope</code></a>: A context manager for use when defining a Python op.</p> <p><a href="initializers/ones"><code translate="no" dir="ltr">class ones_initializer</code></a>: Initializer that generates tensors initialized to 1.</p> <p><a href="initializers/orthogonal"><code translate="no" dir="ltr">class orthogonal_initializer</code></a>: Initializer that generates an orthogonal matrix.</p> <p><a href="random_normal_initializer"><code translate="no" dir="ltr">class random_normal_initializer</code></a>: Initializer that generates tensors with a normal distribution.</p> <p><a href="random_uniform_initializer"><code translate="no" dir="ltr">class random_uniform_initializer</code></a>: Initializer that generates tensors with a uniform distribution.</p> <p><a href="initializers/truncated_normal"><code translate="no" dir="ltr">class truncated_normal_initializer</code></a>: Initializer that generates a truncated normal distribution.</p> <p><a href="initializers/uniform_unit_scaling"><code translate="no" dir="ltr">class uniform_unit_scaling_initializer</code></a>: Initializer that generates tensors without scaling variance.</p> <p><a href="variable_scope"><code translate="no" dir="ltr">class variable_scope</code></a>: A context manager for defining ops that creates variables (layers).</p> <p><a href="initializers/variance_scaling"><code translate="no" dir="ltr">class variance_scaling_initializer</code></a>: Initializer capable of adapting its scale to the shape of weights tensors.</p> <p><a href="zeros_initializer"><code translate="no" dir="ltr">class zeros_initializer</code></a>: Initializer that generates tensors initialized to 0.</p> <h2 id="functions" data-text="Functions" tabindex="0">Functions</h2> <p><a href="debugging/assert"><code translate="no" dir="ltr">Assert(...)</code></a>: Asserts that the given condition is true.</p> <p><a href="no_gradient"><code translate="no" dir="ltr">NoGradient(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="no_gradient"><code translate="no" dir="ltr">NotDifferentiable(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="print"><code translate="no" dir="ltr">Print(...)</code></a>: Prints a list of tensors. (deprecated)</p> <p><a href="math/abs"><code translate="no" dir="ltr">abs(...)</code></a>: Computes the absolute value of a tensor.</p> <p><a href="math/accumulate_n"><code translate="no" dir="ltr">accumulate_n(...)</code></a>: Returns the element-wise sum of a list of tensors.</p> <p><a href="math/acos"><code translate="no" dir="ltr">acos(...)</code></a>: Computes acos of x element-wise.</p> <p><a href="math/acosh"><code translate="no" dir="ltr">acosh(...)</code></a>: Computes inverse hyperbolic cosine of x element-wise.</p> <p><a href="math/add"><code translate="no" dir="ltr">add(...)</code></a>: Returns x + y element-wise.</p> <p><a href="add_check_numerics_ops"><code translate="no" dir="ltr">add_check_numerics_ops(...)</code></a>: Connect a <a href="debugging/check_numerics"><code translate="no" dir="ltr">tf.debugging.check_numerics</code></a> to every floating point tensor.</p> <p><a href="math/add_n"><code translate="no" dir="ltr">add_n(...)</code></a>: Adds all input tensors element-wise.</p> <p><a href="add_to_collection"><code translate="no" dir="ltr">add_to_collection(...)</code></a>: Wrapper for <a href="graph#add_to_collection"><code translate="no" dir="ltr">Graph.add_to_collection()</code></a> using the default graph.</p> <p><a href="add_to_collections"><code translate="no" dir="ltr">add_to_collections(...)</code></a>: Wrapper for <a href="graph#add_to_collections"><code translate="no" dir="ltr">Graph.add_to_collections()</code></a> using the default graph.</p> <p><a href="all_variables"><code translate="no" dir="ltr">all_variables(...)</code></a>: Use <a href="global_variables"><code translate="no" dir="ltr">tf.compat.v1.global_variables</code></a> instead. (deprecated)</p> <p><a href="math/angle"><code translate="no" dir="ltr">angle(...)</code></a>: Returns the element-wise argument of a complex (or real) tensor.</p> <p><a href="arg_max"><code translate="no" dir="ltr">arg_max(...)</code></a>: Returns the index with the largest value across dimensions of a tensor.</p> <p><a href="arg_min"><code translate="no" dir="ltr">arg_min(...)</code></a>: Returns the index with the smallest value across dimensions of a tensor.</p> <p><a href="math/argmax"><code translate="no" dir="ltr">argmax(...)</code></a>: Returns the index with the largest value across axes of a tensor. (deprecated arguments)</p> <p><a href="math/argmin"><code translate="no" dir="ltr">argmin(...)</code></a>: Returns the index with the smallest value across axes of a tensor. (deprecated arguments)</p> <p><a href="argsort"><code translate="no" dir="ltr">argsort(...)</code></a>: Returns the indices of a tensor that give its sorted order along an axis.</p> <p><a href="dtypes/as_dtype"><code translate="no" dir="ltr">as_dtype(...)</code></a>: Converts the given <code translate="no" dir="ltr">type_value</code> to a <code translate="no" dir="ltr">DType</code>.</p> <p><a href="strings/as_string"><code translate="no" dir="ltr">as_string(...)</code></a>: Converts each entry in the given tensor to strings.</p> <p><a href="math/asin"><code translate="no" dir="ltr">asin(...)</code></a>: Computes the trignometric inverse sine of x element-wise.</p> <p><a href="math/asinh"><code translate="no" dir="ltr">asinh(...)</code></a>: Computes inverse hyperbolic sine of x element-wise.</p> <p><a href="debugging/assert_equal"><code translate="no" dir="ltr">assert_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x == y</code> holds element-wise.</p> <p><a href="debugging/assert_greater"><code translate="no" dir="ltr">assert_greater(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt; y</code> holds element-wise.</p> <p><a href="debugging/assert_greater_equal"><code translate="no" dir="ltr">assert_greater_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt;= y</code> holds element-wise.</p> <p><a href="debugging/assert_integer"><code translate="no" dir="ltr">assert_integer(...)</code></a>: Assert that <code translate="no" dir="ltr">x</code> is of integer dtype.</p> <p><a href="debugging/assert_less"><code translate="no" dir="ltr">assert_less(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt; y</code> holds element-wise.</p> <p><a href="debugging/assert_less_equal"><code translate="no" dir="ltr">assert_less_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt;= y</code> holds element-wise.</p> <p><a href="debugging/assert_near"><code translate="no" dir="ltr">assert_near(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> are close element-wise.</p> <p><a href="debugging/assert_negative"><code translate="no" dir="ltr">assert_negative(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt; 0</code> holds element-wise.</p> <p><a href="debugging/assert_non_negative"><code translate="no" dir="ltr">assert_non_negative(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt;= 0</code> holds element-wise.</p> <p><a href="debugging/assert_non_positive"><code translate="no" dir="ltr">assert_non_positive(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt;= 0</code> holds element-wise.</p> <p><a href="debugging/assert_none_equal"><code translate="no" dir="ltr">assert_none_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x != y</code> holds element-wise.</p> <p><a href="debugging/assert_positive"><code translate="no" dir="ltr">assert_positive(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt; 0</code> holds element-wise.</p> <p><a href="debugging/assert_proper_iterable"><code translate="no" dir="ltr">assert_proper_iterable(...)</code></a>: Static assert that values is a "proper" iterable.</p> <p><a href="debugging/assert_rank"><code translate="no" dir="ltr">assert_rank(...)</code></a>: Assert <code translate="no" dir="ltr">x</code> has rank equal to <code translate="no" dir="ltr">rank</code>.</p> <p><a href="debugging/assert_rank_at_least"><code translate="no" dir="ltr">assert_rank_at_least(...)</code></a>: Assert <code translate="no" dir="ltr">x</code> has rank equal to <code translate="no" dir="ltr">rank</code> or higher.</p> <p><a href="debugging/assert_rank_in"><code translate="no" dir="ltr">assert_rank_in(...)</code></a>: Assert <code translate="no" dir="ltr">x</code> has rank in <code translate="no" dir="ltr">ranks</code>.</p> <p><a href="debugging/assert_same_float_dtype"><code translate="no" dir="ltr">assert_same_float_dtype(...)</code></a>: Validate and return float type based on <code translate="no" dir="ltr">tensors</code> and <code translate="no" dir="ltr">dtype</code>.</p> <p><a href="debugging/assert_scalar"><code translate="no" dir="ltr">assert_scalar(...)</code></a>: Asserts that the given <code translate="no" dir="ltr">tensor</code> is a scalar (i.e. zero-dimensional).</p> <p><a href="debugging/assert_type"><code translate="no" dir="ltr">assert_type(...)</code></a>: Statically asserts that the given <code translate="no" dir="ltr">Tensor</code> is of the specified type.</p> <p><a href="assert_variables_initialized"><code translate="no" dir="ltr">assert_variables_initialized(...)</code></a>: Returns an Op to check if variables are initialized.</p> <p><a href="assign"><code translate="no" dir="ltr">assign(...)</code></a>: Update <code translate="no" dir="ltr">ref</code> by assigning <code translate="no" dir="ltr">value</code> to it.</p> <p><a href="assign_add"><code translate="no" dir="ltr">assign_add(...)</code></a>: Update <code translate="no" dir="ltr">ref</code> by adding <code translate="no" dir="ltr">value</code> to it.</p> <p><a href="assign_sub"><code translate="no" dir="ltr">assign_sub(...)</code></a>: Update <code translate="no" dir="ltr">ref</code> by subtracting <code translate="no" dir="ltr">value</code> from it.</p> <p><a href="math/atan"><code translate="no" dir="ltr">atan(...)</code></a>: Computes the trignometric inverse tangent of x element-wise.</p> <p><a href="math/atan2"><code translate="no" dir="ltr">atan2(...)</code></a>: Computes arctangent of <code translate="no" dir="ltr">y/x</code> element-wise, respecting signs of the arguments.</p> <p><a href="math/atanh"><code translate="no" dir="ltr">atanh(...)</code></a>: Computes inverse hyperbolic tangent of x element-wise.</p> <p><a href="batch_gather"><code translate="no" dir="ltr">batch_gather(...)</code></a>: Gather slices from params according to indices with leading batch dims. (deprecated)</p> <p><a href="batch_scatter_update"><code translate="no" dir="ltr">batch_scatter_update(...)</code></a>: Generalization of <a href="scatter_update"><code translate="no" dir="ltr">tf.compat.v1.scatter_update</code></a> to axis different than 0. (deprecated)</p> <p><a href="batch_to_space"><code translate="no" dir="ltr">batch_to_space(...)</code></a>: BatchToSpace for 4-D tensors of type T.</p> <p><a href="batch_to_space_nd"><code translate="no" dir="ltr">batch_to_space_nd(...)</code></a>: BatchToSpace for N-D tensors of type T.</p> <p><a href="math/betainc"><code translate="no" dir="ltr">betainc(...)</code></a>: Compute the regularized incomplete beta integral \(I_x(a, b)\).</p> <p><a href="math/bincount"><code translate="no" dir="ltr">bincount(...)</code></a>: Counts the number of occurrences of each value in an integer array.</p> <p><a href="bitcast"><code translate="no" dir="ltr">bitcast(...)</code></a>: Bitcasts a tensor from one type to another without copying data.</p> <p><a href="boolean_mask"><code translate="no" dir="ltr">boolean_mask(...)</code></a>: Apply boolean mask to tensor.</p> <p><a href="broadcast_dynamic_shape"><code translate="no" dir="ltr">broadcast_dynamic_shape(...)</code></a>: Computes the shape of a broadcast given symbolic shapes.</p> <p><a href="broadcast_static_shape"><code translate="no" dir="ltr">broadcast_static_shape(...)</code></a>: Computes the shape of a broadcast given known shapes.</p> <p><a href="broadcast_to"><code translate="no" dir="ltr">broadcast_to(...)</code></a>: Broadcast an array for a compatible shape.</p> <p><a href="case"><code translate="no" dir="ltr">case(...)</code></a>: Create a case operation.</p> <p><a href="cast"><code translate="no" dir="ltr">cast(...)</code></a>: Casts a tensor to a new type.</p> <p><a href="math/ceil"><code translate="no" dir="ltr">ceil(...)</code></a>: Returns element-wise smallest integer not less than x.</p> <p><a href="debugging/check_numerics"><code translate="no" dir="ltr">check_numerics(...)</code></a>: Checks a tensor for NaN and Inf values.</p> <p><a href="linalg/cholesky"><code translate="no" dir="ltr">cholesky(...)</code></a>: Computes the Cholesky decomposition of one or more square matrices.</p> <p><a href="linalg/cholesky_solve"><code translate="no" dir="ltr">cholesky_solve(...)</code></a>: Solves systems of linear eqns <code translate="no" dir="ltr">A X = RHS</code>, given Cholesky factorizations.</p> <p><a href="clip_by_average_norm"><code translate="no" dir="ltr">clip_by_average_norm(...)</code></a>: Clips tensor values to a maximum average L2-norm. (deprecated)</p> <p><a href="clip_by_global_norm"><code translate="no" dir="ltr">clip_by_global_norm(...)</code></a>: Clips values of multiple tensors by the ratio of the sum of their norms.</p> <p><a href="clip_by_norm"><code translate="no" dir="ltr">clip_by_norm(...)</code></a>: Clips tensor values to a maximum L2-norm.</p> <p><a href="clip_by_value"><code translate="no" dir="ltr">clip_by_value(...)</code></a>: Clips tensor values to a specified min and max.</p> <p><a href="colocate_with"><code translate="no" dir="ltr">colocate_with(...)</code></a>: DEPRECATED FUNCTION</p> <p><a href="dtypes/complex"><code translate="no" dir="ltr">complex(...)</code></a>: Converts two real numbers to a complex number.</p> <p><a href="concat"><code translate="no" dir="ltr">concat(...)</code></a>: Concatenates tensors along one dimension.</p> <p><a href="cond"><code translate="no" dir="ltr">cond(...)</code></a>: Return <code translate="no" dir="ltr">true_fn()</code> if the predicate <code translate="no" dir="ltr">pred</code> is true else <code translate="no" dir="ltr">false_fn()</code>. (deprecated arguments)</p> <p><a href="math/confusion_matrix"><code translate="no" dir="ltr">confusion_matrix(...)</code></a>: Computes the confusion matrix from predictions and labels.</p> <p><a href="math/conj"><code translate="no" dir="ltr">conj(...)</code></a>: Returns the complex conjugate of a complex number.</p> <p><a href="constant"><code translate="no" dir="ltr">constant(...)</code></a>: Creates a constant tensor.</p> <p><a href="container"><code translate="no" dir="ltr">container(...)</code></a>: Wrapper for <a href="graph#container"><code translate="no" dir="ltr">Graph.container()</code></a> using the default graph.</p> <p><a href="control_dependencies"><code translate="no" dir="ltr">control_dependencies(...)</code></a>: Wrapper for <a href="graph#control_dependencies"><code translate="no" dir="ltr">Graph.control_dependencies()</code></a> using the default graph.</p> <p><a href="control_flow_v2_enabled"><code translate="no" dir="ltr">control_flow_v2_enabled(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if v2 control flow is enabled.</p> <p><a href="convert_to_tensor"><code translate="no" dir="ltr">convert_to_tensor(...)</code></a>: Converts the given <code translate="no" dir="ltr">value</code> to a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="convert_to_tensor_or_indexed_slices"><code translate="no" dir="ltr">convert_to_tensor_or_indexed_slices(...)</code></a>: Converts the given object to a <code translate="no" dir="ltr">Tensor</code> or an <code translate="no" dir="ltr">IndexedSlices</code>.</p> <p><a href="convert_to_tensor_or_sparse_tensor"><code translate="no" dir="ltr">convert_to_tensor_or_sparse_tensor(...)</code></a>: Converts value to a <code translate="no" dir="ltr">SparseTensor</code> or <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="math/cos"><code translate="no" dir="ltr">cos(...)</code></a>: Computes cos of x element-wise.</p> <p><a href="math/cosh"><code translate="no" dir="ltr">cosh(...)</code></a>: Computes hyperbolic cosine of x element-wise.</p> <p><a href="math/count_nonzero"><code translate="no" dir="ltr">count_nonzero(...)</code></a>: Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments)</p> <p><a href="count_up_to"><code translate="no" dir="ltr">count_up_to(...)</code></a>: Increments 'ref' until it reaches 'limit'. (deprecated)</p> <p><a href="create_partitioned_variables"><code translate="no" dir="ltr">create_partitioned_variables(...)</code></a>: Create a list of partitioned variables according to the given <code translate="no" dir="ltr">slicing</code>. (deprecated)</p> <p><a href="linalg/cross"><code translate="no" dir="ltr">cross(...)</code></a>: Compute the pairwise cross product.</p> <p><a href="math/cumprod"><code translate="no" dir="ltr">cumprod(...)</code></a>: Compute the cumulative product of the tensor <code translate="no" dir="ltr">x</code> along <code translate="no" dir="ltr">axis</code>.</p> <p><a href="math/cumsum"><code translate="no" dir="ltr">cumsum(...)</code></a>: Compute the cumulative sum of the tensor <code translate="no" dir="ltr">x</code> along <code translate="no" dir="ltr">axis</code>.</p> <p><a href="custom_gradient"><code translate="no" dir="ltr">custom_gradient(...)</code></a>: Decorator to define a function with a custom gradient.</p> <p><a href="io/decode_base64"><code translate="no" dir="ltr">decode_base64(...)</code></a>: Decode web-safe base64-encoded strings.</p> <p><a href="io/decode_compressed"><code translate="no" dir="ltr">decode_compressed(...)</code></a>: Decompress strings.</p> <p><a href="io/decode_csv"><code translate="no" dir="ltr">decode_csv(...)</code></a>: Convert CSV records to tensors. Each column maps to one tensor.</p> <p><a href="io/decode_json_example"><code translate="no" dir="ltr">decode_json_example(...)</code></a>: Convert JSON-encoded Example records to binary protocol buffer strings.</p> <p><a href="decode_raw"><code translate="no" dir="ltr">decode_raw(...)</code></a>: Convert raw byte strings into tensors. (deprecated arguments)</p> <p><a href="delete_session_tensor"><code translate="no" dir="ltr">delete_session_tensor(...)</code></a>: Delete the tensor for the given tensor handle.</p> <p><a href="nn/depth_to_space"><code translate="no" dir="ltr">depth_to_space(...)</code></a>: DepthToSpace for tensors of type T.</p> <p><a href="quantization/dequantize"><code translate="no" dir="ltr">dequantize(...)</code></a>: Dequantize the 'input' tensor into a float Tensor.</p> <p><a href="io/deserialize_many_sparse"><code translate="no" dir="ltr">deserialize_many_sparse(...)</code></a>: Deserialize and concatenate <code translate="no" dir="ltr">SparseTensors</code> from a serialized minibatch.</p> <p><a href="device"><code translate="no" dir="ltr">device(...)</code></a>: Wrapper for <a href="graph#device"><code translate="no" dir="ltr">Graph.device()</code></a> using the default graph.</p> <p><a href="linalg/tensor_diag"><code translate="no" dir="ltr">diag(...)</code></a>: Returns a diagonal tensor with a given diagonal values.</p> <p><a href="linalg/tensor_diag_part"><code translate="no" dir="ltr">diag_part(...)</code></a>: Returns the diagonal part of the tensor.</p> <p><a href="math/digamma"><code translate="no" dir="ltr">digamma(...)</code></a>: Computes Psi, the derivative of Lgamma (the log of the absolute value of</p> <p><a href="compat/dimension_at_index"><code translate="no" dir="ltr">dimension_at_index(...)</code></a>: Compatibility utility required to allow for both V1 and V2 behavior in TF.</p> <p><a href="compat/dimension_value"><code translate="no" dir="ltr">dimension_value(...)</code></a>: Compatibility utility required to allow for both V1 and V2 behavior in TF.</p> <p><a href="disable_control_flow_v2"><code translate="no" dir="ltr">disable_control_flow_v2(...)</code></a>: Opts out of control flow v2.</p> <p><a href="disable_eager_execution"><code translate="no" dir="ltr">disable_eager_execution(...)</code></a>: Disables eager execution.</p> <p><a href="disable_resource_variables"><code translate="no" dir="ltr">disable_resource_variables(...)</code></a>: Opts out of resource variables. (deprecated)</p> <p><a href="disable_tensor_equality"><code translate="no" dir="ltr">disable_tensor_equality(...)</code></a>: Compare Tensors by their id and be hashable.</p> <p><a href="disable_v2_behavior"><code translate="no" dir="ltr">disable_v2_behavior(...)</code></a>: Disables TensorFlow 2.x behaviors.</p> <p><a href="disable_v2_tensorshape"><code translate="no" dir="ltr">disable_v2_tensorshape(...)</code></a>: Disables the V2 TensorShape behavior and reverts to V1 behavior.</p> <p><a href="div"><code translate="no" dir="ltr">div(...)</code></a>: Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)</p> <p><a href="math/divide_no_nan"><code translate="no" dir="ltr">div_no_nan(...)</code></a>: Computes an unsafe divide which returns 0 if the y is zero.</p> <p><a href="math/divide"><code translate="no" dir="ltr">divide(...)</code></a>: Computes Python style division of <code translate="no" dir="ltr">x</code> by <code translate="no" dir="ltr">y</code>.</p> <p><a href="dynamic_partition"><code translate="no" dir="ltr">dynamic_partition(...)</code></a>: Partitions <code translate="no" dir="ltr">data</code> into <code translate="no" dir="ltr">num_partitions</code> tensors using indices from <code translate="no" dir="ltr">partitions</code>.</p> <p><a href="dynamic_stitch"><code translate="no" dir="ltr">dynamic_stitch(...)</code></a>: Interleave the values from the <code translate="no" dir="ltr">data</code> tensors into a single tensor.</p> <p><a href="edit_distance"><code translate="no" dir="ltr">edit_distance(...)</code></a>: Computes the Levenshtein distance between sequences.</p> <p><a href="einsum"><code translate="no" dir="ltr">einsum(...)</code></a>: Tensor contraction over specified indices and outer product.</p> <p><a href="enable_control_flow_v2"><code translate="no" dir="ltr">enable_control_flow_v2(...)</code></a>: Use control flow v2.</p> <p><a href="enable_eager_execution"><code translate="no" dir="ltr">enable_eager_execution(...)</code></a>: Enables eager execution for the lifetime of this program.</p> <p><a href="enable_resource_variables"><code translate="no" dir="ltr">enable_resource_variables(...)</code></a>: Creates resource variables by default.</p> <p><a href="enable_tensor_equality"><code translate="no" dir="ltr">enable_tensor_equality(...)</code></a>: Compare Tensors with element-wise comparison and thus be unhashable.</p> <p><a href="enable_v2_behavior"><code translate="no" dir="ltr">enable_v2_behavior(...)</code></a>: Enables TensorFlow 2.x behaviors.</p> <p><a href="enable_v2_tensorshape"><code translate="no" dir="ltr">enable_v2_tensorshape(...)</code></a>: In TensorFlow 2.0, iterating over a TensorShape instance returns values.</p> <p><a href="io/encode_base64"><code translate="no" dir="ltr">encode_base64(...)</code></a>: Encode strings into web-safe base64 format.</p> <p><a href="ensure_shape"><code translate="no" dir="ltr">ensure_shape(...)</code></a>: Updates the shape of a tensor and checks at runtime that the shape holds.</p> <p><a href="math/equal"><code translate="no" dir="ltr">equal(...)</code></a>: Returns the truth value of (x == y) element-wise.</p> <p><a href="math/erf"><code translate="no" dir="ltr">erf(...)</code></a>: Computes the Gauss error function of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="math/erfc"><code translate="no" dir="ltr">erfc(...)</code></a>: Computes the complementary error function of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="executing_eagerly"><code translate="no" dir="ltr">executing_eagerly(...)</code></a>: Returns True if the current thread has eager execution enabled.</p> <p><a href="math/exp"><code translate="no" dir="ltr">exp(...)</code></a>: Computes exponential of x element-wise. \(y = e^x\).</p> <p><a href="expand_dims"><code translate="no" dir="ltr">expand_dims(...)</code></a>: Inserts a dimension of 1 into a tensor's shape. (deprecated arguments)</p> <p><a href="math/expm1"><code translate="no" dir="ltr">expm1(...)</code></a>: Computes <code translate="no" dir="ltr">exp(x) - 1</code> element-wise.</p> <p><a href="image/extract_image_patches"><code translate="no" dir="ltr">extract_image_patches(...)</code></a>: Extract <code translate="no" dir="ltr">patches</code> from <code translate="no" dir="ltr">images</code> and put them in the "depth" output dimension.</p> <p><a href="extract_volume_patches"><code translate="no" dir="ltr">extract_volume_patches(...)</code></a>: Extract <code translate="no" dir="ltr">patches</code> from <code translate="no" dir="ltr">input</code> and put them in the "depth" output dimension. 3D extension of <code translate="no" dir="ltr">extract_image_patches</code>.</p> <p><a href="eye"><code translate="no" dir="ltr">eye(...)</code></a>: Construct an identity matrix, or a batch of matrices.</p> <p><a href="quantization/fake_quant_with_min_max_args"><code translate="no" dir="ltr">fake_quant_with_min_max_args(...)</code></a>: Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type.</p> <p><a href="quantization/fake_quant_with_min_max_args_gradient"><code translate="no" dir="ltr">fake_quant_with_min_max_args_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxArgs operation.</p> <p><a href="quantization/fake_quant_with_min_max_vars"><code translate="no" dir="ltr">fake_quant_with_min_max_vars(...)</code></a>: Fake-quantize the 'inputs' tensor of type float via global float scalars <code translate="no" dir="ltr">min</code></p> <p><a href="quantization/fake_quant_with_min_max_vars_gradient"><code translate="no" dir="ltr">fake_quant_with_min_max_vars_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxVars operation.</p> <p><a href="quantization/fake_quant_with_min_max_vars_per_channel"><code translate="no" dir="ltr">fake_quant_with_min_max_vars_per_channel(...)</code></a>: Fake-quantize the 'inputs' tensor of type float and one of the shapes: <code translate="no" dir="ltr">[d]</code>,</p> <p><a href="quantization/fake_quant_with_min_max_vars_per_channel_gradient"><code translate="no" dir="ltr">fake_quant_with_min_max_vars_per_channel_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.</p> <p><a href="signal/fft"><code translate="no" dir="ltr">fft(...)</code></a>: Fast Fourier transform.</p> <p><a href="signal/fft2d"><code translate="no" dir="ltr">fft2d(...)</code></a>: 2D fast Fourier transform.</p> <p><a href="signal/fft3d"><code translate="no" dir="ltr">fft3d(...)</code></a>: 3D fast Fourier transform.</p> <p><a href="fill"><code translate="no" dir="ltr">fill(...)</code></a>: Creates a tensor filled with a scalar value.</p> <p><a href="fingerprint"><code translate="no" dir="ltr">fingerprint(...)</code></a>: Generates fingerprint values.</p> <p><a href="fixed_size_partitioner"><code translate="no" dir="ltr">fixed_size_partitioner(...)</code></a>: Partitioner to specify a fixed number of shards along given axis.</p> <p><a href="math/floor"><code translate="no" dir="ltr">floor(...)</code></a>: Returns element-wise largest integer not greater than x.</p> <p><a href="floor_div"><code translate="no" dir="ltr">floor_div(...)</code></a>: Returns x // y element-wise.</p> <p><a href="math/floordiv"><code translate="no" dir="ltr">floordiv(...)</code></a>: Divides <code translate="no" dir="ltr">x / y</code> elementwise, rounding toward the most negative integer.</p> <p><a href="math/floormod"><code translate="no" dir="ltr">floormod(...)</code></a>: Returns element-wise remainder of division. When <code translate="no" dir="ltr">x &lt; 0</code> xor <code translate="no" dir="ltr">y &lt; 0</code> is</p> <p><a href="foldl"><code translate="no" dir="ltr">foldl(...)</code></a>: foldl on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="foldr"><code translate="no" dir="ltr">foldr(...)</code></a>: foldr on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="function"><code translate="no" dir="ltr">function(...)</code></a>: Creates a callable TensorFlow graph from a Python function.</p> <p><a href="gather"><code translate="no" dir="ltr">gather(...)</code></a>: Gather slices from params axis axis according to indices.</p> <p><a href="gather_nd"><code translate="no" dir="ltr">gather_nd(...)</code></a>: Gather slices from <code translate="no" dir="ltr">params</code> into a Tensor with shape specified by <code translate="no" dir="ltr">indices</code>.</p> <p><a href="get_collection"><code translate="no" dir="ltr">get_collection(...)</code></a>: Wrapper for <a href="graph#get_collection"><code translate="no" dir="ltr">Graph.get_collection()</code></a> using the default graph.</p> <p><a href="get_collection_ref"><code translate="no" dir="ltr">get_collection_ref(...)</code></a>: Wrapper for <a href="graph#get_collection_ref"><code translate="no" dir="ltr">Graph.get_collection_ref()</code></a> using the default graph.</p> <p><a href="get_default_graph"><code translate="no" dir="ltr">get_default_graph(...)</code></a>: Returns the default graph for the current thread.</p> <p><a href="get_default_session"><code translate="no" dir="ltr">get_default_session(...)</code></a>: Returns the default session for the current thread.</p> <p><a href="get_local_variable"><code translate="no" dir="ltr">get_local_variable(...)</code></a>: Gets an existing <em>local</em> variable or creates a new one.</p> <p><a href="get_logger"><code translate="no" dir="ltr">get_logger(...)</code></a>: Return TF logger instance.</p> <p><a href="random/get_seed"><code translate="no" dir="ltr">get_seed(...)</code></a>: Returns the local seeds an operation should use given an op-specific seed.</p> <p><a href="get_session_handle"><code translate="no" dir="ltr">get_session_handle(...)</code></a>: Return the handle of <code translate="no" dir="ltr">data</code>.</p> <p><a href="get_session_tensor"><code translate="no" dir="ltr">get_session_tensor(...)</code></a>: Get the tensor of type <code translate="no" dir="ltr">dtype</code> by feeding a tensor handle.</p> <p><a href="get_static_value"><code translate="no" dir="ltr">get_static_value(...)</code></a>: Returns the constant value of the given tensor, if efficiently calculable.</p> <p><a href="get_variable"><code translate="no" dir="ltr">get_variable(...)</code></a>: Gets an existing variable with these parameters or create a new one.</p> <p><a href="get_variable_scope"><code translate="no" dir="ltr">get_variable_scope(...)</code></a>: Returns the current variable scope.</p> <p><a href="linalg/global_norm"><code translate="no" dir="ltr">global_norm(...)</code></a>: Computes the global norm of multiple tensors.</p> <p><a href="global_variables"><code translate="no" dir="ltr">global_variables(...)</code></a>: Returns global variables.</p> <p><a href="initializers/global_variables"><code translate="no" dir="ltr">global_variables_initializer(...)</code></a>: Returns an Op that initializes global variables.</p> <p><a href="grad_pass_through"><code translate="no" dir="ltr">grad_pass_through(...)</code></a>: Creates a grad-pass-through op with the forward behavior provided in f.</p> <p><a href="gradients"><code translate="no" dir="ltr">gradients(...)</code></a>: Constructs symbolic derivatives of sum of <code translate="no" dir="ltr">ys</code> w.r.t. x in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="math/greater"><code translate="no" dir="ltr">greater(...)</code></a>: Returns the truth value of (x &gt; y) element-wise.</p> <p><a href="math/greater_equal"><code translate="no" dir="ltr">greater_equal(...)</code></a>: Returns the truth value of (x &gt;= y) element-wise.</p> <p><a href="group"><code translate="no" dir="ltr">group(...)</code></a>: Create an op that groups multiple operations.</p> <p><a href="guarantee_const"><code translate="no" dir="ltr">guarantee_const(...)</code></a>: Gives a guarantee to the TF runtime that the input tensor is a constant.</p> <p><a href="hessians"><code translate="no" dir="ltr">hessians(...)</code></a>: Constructs the Hessian of sum of <code translate="no" dir="ltr">ys</code> with respect to <code translate="no" dir="ltr">x</code> in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="histogram_fixed_width"><code translate="no" dir="ltr">histogram_fixed_width(...)</code></a>: Return histogram of values.</p> <p><a href="histogram_fixed_width_bins"><code translate="no" dir="ltr">histogram_fixed_width_bins(...)</code></a>: Bins the given values for use in a histogram.</p> <p><a href="identity"><code translate="no" dir="ltr">identity(...)</code></a>: Return a tensor with the same shape and contents as input.</p> <p><a href="identity_n"><code translate="no" dir="ltr">identity_n(...)</code></a>: Returns a list of tensors with the same shapes and contents as the input</p> <p><a href="signal/ifft"><code translate="no" dir="ltr">ifft(...)</code></a>: Inverse fast Fourier transform.</p> <p><a href="signal/ifft2d"><code translate="no" dir="ltr">ifft2d(...)</code></a>: Inverse 2D fast Fourier transform.</p> <p><a href="signal/ifft3d"><code translate="no" dir="ltr">ifft3d(...)</code></a>: Inverse 3D fast Fourier transform.</p> <p><a href="math/igamma"><code translate="no" dir="ltr">igamma(...)</code></a>: Compute the lower regularized incomplete Gamma function <code translate="no" dir="ltr">P(a, x)</code>.</p> <p><a href="math/igammac"><code translate="no" dir="ltr">igammac(...)</code></a>: Compute the upper regularized incomplete Gamma function <code translate="no" dir="ltr">Q(a, x)</code>.</p> <p><a href="math/imag"><code translate="no" dir="ltr">imag(...)</code></a>: Returns the imaginary part of a complex (or real) tensor.</p> <p><a href="graph_util/import_graph_def"><code translate="no" dir="ltr">import_graph_def(...)</code></a>: Imports the graph from <code translate="no" dir="ltr">graph_def</code> into the current default <code translate="no" dir="ltr">Graph</code>. (deprecated arguments)</p> <p><a href="init_scope"><code translate="no" dir="ltr">init_scope(...)</code></a>: A context manager that lifts ops out of control-flow scopes and function-building graphs.</p> <p><a href="initialize_all_tables"><code translate="no" dir="ltr">initialize_all_tables(...)</code></a>: Returns an Op that initializes all tables of the default graph. (deprecated)</p> <p><a href="initialize_all_variables"><code translate="no" dir="ltr">initialize_all_variables(...)</code></a>: See <a href="initializers/global_variables"><code translate="no" dir="ltr">tf.compat.v1.global_variables_initializer</code></a>. (deprecated)</p> <p><a href="initialize_local_variables"><code translate="no" dir="ltr">initialize_local_variables(...)</code></a>: See <a href="initializers/local_variables"><code translate="no" dir="ltr">tf.compat.v1.local_variables_initializer</code></a>. (deprecated)</p> <p><a href="initialize_variables"><code translate="no" dir="ltr">initialize_variables(...)</code></a>: See <a href="initializers/variables"><code translate="no" dir="ltr">tf.compat.v1.variables_initializer</code></a>. (deprecated)</p> <p><a href="math/invert_permutation"><code translate="no" dir="ltr">invert_permutation(...)</code></a>: Computes the inverse permutation of a tensor.</p> <p><a href="math/is_finite"><code translate="no" dir="ltr">is_finite(...)</code></a>: Returns which elements of x are finite.</p> <p><a href="math/is_inf"><code translate="no" dir="ltr">is_inf(...)</code></a>: Returns which elements of x are Inf.</p> <p><a href="math/is_nan"><code translate="no" dir="ltr">is_nan(...)</code></a>: Returns which elements of x are NaN.</p> <p><a href="math/is_non_decreasing"><code translate="no" dir="ltr">is_non_decreasing(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if <code translate="no" dir="ltr">x</code> is non-decreasing.</p> <p><a href="debugging/is_numeric_tensor"><code translate="no" dir="ltr">is_numeric_tensor(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if the elements of <code translate="no" dir="ltr">tensor</code> are numbers.</p> <p><a href="math/is_strictly_increasing"><code translate="no" dir="ltr">is_strictly_increasing(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if <code translate="no" dir="ltr">x</code> is strictly increasing.</p> <p><a href="is_tensor"><code translate="no" dir="ltr">is_tensor(...)</code></a>: Checks whether <code translate="no" dir="ltr">x</code> is a tensor or "tensor-like".</p> <p><a href="is_variable_initialized"><code translate="no" dir="ltr">is_variable_initialized(...)</code></a>: Tests if a variable has been initialized.</p> <p><a href="math/lbeta"><code translate="no" dir="ltr">lbeta(...)</code></a>: Computes \(ln(|Beta(x)|)\), reducing along the last dimension.</p> <p><a href="math/less"><code translate="no" dir="ltr">less(...)</code></a>: Returns the truth value of (x &lt; y) element-wise.</p> <p><a href="math/less_equal"><code translate="no" dir="ltr">less_equal(...)</code></a>: Returns the truth value of (x &lt;= y) element-wise.</p> <p><a href="math/lgamma"><code translate="no" dir="ltr">lgamma(...)</code></a>: Computes the log of the absolute value of <code translate="no" dir="ltr">Gamma(x)</code> element-wise.</p> <p><a href="linspace"><code translate="no" dir="ltr">lin_space(...)</code></a>: Generates values in an interval.</p> <p><a href="linspace"><code translate="no" dir="ltr">linspace(...)</code></a>: Generates values in an interval.</p> <p><a href="load_file_system_library"><code translate="no" dir="ltr">load_file_system_library(...)</code></a>: Loads a TensorFlow plugin, containing file system implementation. (deprecated)</p> <p><a href="load_library"><code translate="no" dir="ltr">load_library(...)</code></a>: Loads a TensorFlow plugin.</p> <p><a href="load_op_library"><code translate="no" dir="ltr">load_op_library(...)</code></a>: Loads a TensorFlow plugin, containing custom ops and kernels.</p> <p><a href="local_variables"><code translate="no" dir="ltr">local_variables(...)</code></a>: Returns local variables.</p> <p><a href="initializers/local_variables"><code translate="no" dir="ltr">local_variables_initializer(...)</code></a>: Returns an Op that initializes all local variables.</p> <p><a href="math/log"><code translate="no" dir="ltr">log(...)</code></a>: Computes natural logarithm of x element-wise.</p> <p><a href="math/log1p"><code translate="no" dir="ltr">log1p(...)</code></a>: Computes natural logarithm of (1 + x) element-wise.</p> <p><a href="math/log_sigmoid"><code translate="no" dir="ltr">log_sigmoid(...)</code></a>: Computes log sigmoid of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="math/logical_and"><code translate="no" dir="ltr">logical_and(...)</code></a>: Returns the truth value of x AND y element-wise.</p> <p><a href="math/logical_not"><code translate="no" dir="ltr">logical_not(...)</code></a>: Returns the truth value of NOT x element-wise.</p> <p><a href="math/logical_or"><code translate="no" dir="ltr">logical_or(...)</code></a>: Returns the truth value of x OR y element-wise.</p> <p><a href="math/logical_xor"><code translate="no" dir="ltr">logical_xor(...)</code></a>: Logical XOR function.</p> <p><a href="make_ndarray"><code translate="no" dir="ltr">make_ndarray(...)</code></a>: Create a numpy ndarray from a tensor.</p> <p><a href="make_template"><code translate="no" dir="ltr">make_template(...)</code></a>: Given an arbitrary function, wrap it so that it does variable sharing.</p> <p><a href="make_tensor_proto"><code translate="no" dir="ltr">make_tensor_proto(...)</code></a>: Create a TensorProto.</p> <p><a href="map_fn"><code translate="no" dir="ltr">map_fn(...)</code></a>: map on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="io/matching_files"><code translate="no" dir="ltr">matching_files(...)</code></a>: Returns the set of files matching one or more glob patterns.</p> <p><a href="linalg/matmul"><code translate="no" dir="ltr">matmul(...)</code></a>: Multiplies matrix <code translate="no" dir="ltr">a</code> by matrix <code translate="no" dir="ltr">b</code>, producing <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code>.</p> <p><a href="linalg/band_part"><code translate="no" dir="ltr">matrix_band_part(...)</code></a>: Copy a tensor setting everything outside a central band in each innermost matrix</p> <p><a href="linalg/det"><code translate="no" dir="ltr">matrix_determinant(...)</code></a>: Computes the determinant of one or more square matrices.</p> <p><a href="linalg/diag"><code translate="no" dir="ltr">matrix_diag(...)</code></a>: Returns a batched diagonal tensor with given batched diagonal values.</p> <p><a href="linalg/diag_part"><code translate="no" dir="ltr">matrix_diag_part(...)</code></a>: Returns the batched diagonal part of a batched tensor.</p> <p><a href="linalg/inv"><code translate="no" dir="ltr">matrix_inverse(...)</code></a>: Computes the inverse of one or more square invertible matrices or their</p> <p><a href="linalg/set_diag"><code translate="no" dir="ltr">matrix_set_diag(...)</code></a>: Returns a batched matrix tensor with new batched diagonal values.</p> <p><a href="linalg/solve"><code translate="no" dir="ltr">matrix_solve(...)</code></a>: Solves systems of linear equations.</p> <p><a href="linalg/lstsq"><code translate="no" dir="ltr">matrix_solve_ls(...)</code></a>: Solves one or more linear least-squares problems.</p> <p><a href="linalg/sqrtm"><code translate="no" dir="ltr">matrix_square_root(...)</code></a>: Computes the matrix square root of one or more square matrices:</p> <p><a href="linalg/matrix_transpose"><code translate="no" dir="ltr">matrix_transpose(...)</code></a>: Transposes last two dimensions of tensor <code translate="no" dir="ltr">a</code>.</p> <p><a href="linalg/triangular_solve"><code translate="no" dir="ltr">matrix_triangular_solve(...)</code></a>: Solves systems of linear equations with upper or lower triangular matrices by backsubstitution.</p> <p><a href="math/maximum"><code translate="no" dir="ltr">maximum(...)</code></a>: Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p> <p><a href="meshgrid"><code translate="no" dir="ltr">meshgrid(...)</code></a>: Broadcasts parameters for evaluation on an N-D grid.</p> <p><a href="min_max_variable_partitioner"><code translate="no" dir="ltr">min_max_variable_partitioner(...)</code></a>: Partitioner to allocate minimum size per slice.</p> <p><a href="math/minimum"><code translate="no" dir="ltr">minimum(...)</code></a>: Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p> <p><a href="math/floormod"><code translate="no" dir="ltr">mod(...)</code></a>: Returns element-wise remainder of division. When <code translate="no" dir="ltr">x &lt; 0</code> xor <code translate="no" dir="ltr">y &lt; 0</code> is</p> <p><a href="model_variables"><code translate="no" dir="ltr">model_variables(...)</code></a>: Returns all variables in the MODEL_VARIABLES collection.</p> <p><a href="moving_average_variables"><code translate="no" dir="ltr">moving_average_variables(...)</code></a>: Returns all variables that maintain their moving averages.</p> <p><a href="random/multinomial"><code translate="no" dir="ltr">multinomial(...)</code></a>: Draws samples from a multinomial distribution. (deprecated)</p> <p><a href="math/multiply"><code translate="no" dir="ltr">multiply(...)</code></a>: Returns x * y element-wise.</p> <p><a href="math/negative"><code translate="no" dir="ltr">negative(...)</code></a>: Computes numerical negative value element-wise.</p> <p><a href="no_gradient"><code translate="no" dir="ltr">no_gradient(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="no_op"><code translate="no" dir="ltr">no_op(...)</code></a>: Does nothing. Only useful as a placeholder for control edges.</p> <p><a href="no_regularizer"><code translate="no" dir="ltr">no_regularizer(...)</code></a>: Use this function to prevent regularization of variables.</p> <p><a href="nondifferentiable_batch_function"><code translate="no" dir="ltr">nondifferentiable_batch_function(...)</code></a>: Batches the computation done by the decorated function.</p> <p><a href="norm"><code translate="no" dir="ltr">norm(...)</code></a>: Computes the norm of vectors, matrices, and tensors. (deprecated arguments)</p> <p><a href="math/not_equal"><code translate="no" dir="ltr">not_equal(...)</code></a>: Returns the truth value of (x != y) element-wise.</p> <p><a href="numpy_function"><code translate="no" dir="ltr">numpy_function(...)</code></a>: Wraps a python function and uses it as a TensorFlow op.</p> <p><a href="one_hot"><code translate="no" dir="ltr">one_hot(...)</code></a>: Returns a one-hot tensor.</p> <p><a href="ones"><code translate="no" dir="ltr">ones(...)</code></a>: Creates a tensor with all elements set to 1.</p> <p><a href="ones_like"><code translate="no" dir="ltr">ones_like(...)</code></a>: Creates a tensor with all elements set to 1.</p> <p><a href="op_scope"><code translate="no" dir="ltr">op_scope(...)</code></a>: DEPRECATED. Same as name_scope above, just different argument order.</p> <p><a href="pad"><code translate="no" dir="ltr">pad(...)</code></a>: Pads a tensor.</p> <p><a href="parallel_stack"><code translate="no" dir="ltr">parallel_stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor in parallel.</p> <p><a href="io/parse_example"><code translate="no" dir="ltr">parse_example(...)</code></a>: Parses <code translate="no" dir="ltr">Example</code> protos into a <code translate="no" dir="ltr">dict</code> of tensors.</p> <p><a href="io/parse_single_example"><code translate="no" dir="ltr">parse_single_example(...)</code></a>: Parses a single <code translate="no" dir="ltr">Example</code> proto.</p> <p><a href="io/parse_single_sequence_example"><code translate="no" dir="ltr">parse_single_sequence_example(...)</code></a>: Parses a single <code translate="no" dir="ltr">SequenceExample</code> proto.</p> <p><a href="io/parse_tensor"><code translate="no" dir="ltr">parse_tensor(...)</code></a>: Transforms a serialized tensorflow.TensorProto proto into a Tensor.</p> <p><a href="placeholder"><code translate="no" dir="ltr">placeholder(...)</code></a>: Inserts a placeholder for a tensor that will be always fed.</p> <p><a href="placeholder_with_default"><code translate="no" dir="ltr">placeholder_with_default(...)</code></a>: A placeholder op that passes through <code translate="no" dir="ltr">input</code> when its output is not fed.</p> <p><a href="math/polygamma"><code translate="no" dir="ltr">polygamma(...)</code></a>: Compute the polygamma function \(\psi^{(n)}(x)\).</p> <p><a href="math/pow"><code translate="no" dir="ltr">pow(...)</code></a>: Computes the power of one value to another.</p> <p><a href="print"><code translate="no" dir="ltr">print(...)</code></a>: Print the specified inputs.</p> <p><a href="py_func"><code translate="no" dir="ltr">py_func(...)</code></a>: Wraps a python function and uses it as a TensorFlow op.</p> <p><a href="py_function"><code translate="no" dir="ltr">py_function(...)</code></a>: Wraps a python function into a TensorFlow op that executes it eagerly.</p> <p><a href="linalg/qr"><code translate="no" dir="ltr">qr(...)</code></a>: Computes the QR decompositions of one or more matrices.</p> <p><a href="quantization/quantize"><code translate="no" dir="ltr">quantize(...)</code></a>: Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.</p> <p><a href="quantize_v2"><code translate="no" dir="ltr">quantize_v2(...)</code></a>: Please use <a href="quantization/quantize"><code translate="no" dir="ltr">tf.quantization.quantize</code></a> instead.</p> <p><a href="quantization/quantized_concat"><code translate="no" dir="ltr">quantized_concat(...)</code></a>: Concatenates quantized tensors along one dimension.</p> <p><a href="image/random_crop"><code translate="no" dir="ltr">random_crop(...)</code></a>: Randomly crops a tensor to a given size.</p> <p><a href="random/gamma"><code translate="no" dir="ltr">random_gamma(...)</code></a>: Draws <code translate="no" dir="ltr">shape</code> samples from each of the given Gamma distribution(s).</p> <p><a href="random/normal"><code translate="no" dir="ltr">random_normal(...)</code></a>: Outputs random values from a normal distribution.</p> <p><a href="random/poisson"><code translate="no" dir="ltr">random_poisson(...)</code></a>: Draws <code translate="no" dir="ltr">shape</code> samples from each of the given Poisson distribution(s).</p> <p><a href="random/shuffle"><code translate="no" dir="ltr">random_shuffle(...)</code></a>: Randomly shuffles a tensor along its first dimension.</p> <p><a href="random/uniform"><code translate="no" dir="ltr">random_uniform(...)</code></a>: Outputs random values from a uniform distribution.</p> <p><a href="range"><code translate="no" dir="ltr">range(...)</code></a>: Creates a sequence of numbers.</p> <p><a href="rank"><code translate="no" dir="ltr">rank(...)</code></a>: Returns the rank of a tensor.</p> <p><a href="io/read_file"><code translate="no" dir="ltr">read_file(...)</code></a>: Reads and outputs the entire contents of the input filename.</p> <p><a href="math/real"><code translate="no" dir="ltr">real(...)</code></a>: Returns the real part of a complex (or real) tensor.</p> <p><a href="realdiv"><code translate="no" dir="ltr">realdiv(...)</code></a>: Returns x / y element-wise for real types.</p> <p><a href="math/reciprocal"><code translate="no" dir="ltr">reciprocal(...)</code></a>: Computes the reciprocal of x element-wise.</p> <p><a href="recompute_grad"><code translate="no" dir="ltr">recompute_grad(...)</code></a>: An eager-compatible version of recompute_grad.</p> <p><a href="math/reduce_all"><code translate="no" dir="ltr">reduce_all(...)</code></a>: Computes the "logical and" of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="math/reduce_any"><code translate="no" dir="ltr">reduce_any(...)</code></a>: Computes the "logical or" of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="strings/reduce_join"><code translate="no" dir="ltr">reduce_join(...)</code></a>: Joins a string Tensor across the given dimensions.</p> <p><a href="math/reduce_logsumexp"><code translate="no" dir="ltr">reduce_logsumexp(...)</code></a>: Computes log(sum(exp(elements across dimensions of a tensor))). (deprecated arguments)</p> <p><a href="math/reduce_max"><code translate="no" dir="ltr">reduce_max(...)</code></a>: Computes the maximum of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="math/reduce_mean"><code translate="no" dir="ltr">reduce_mean(...)</code></a>: Computes the mean of elements across dimensions of a tensor.</p> <p><a href="math/reduce_min"><code translate="no" dir="ltr">reduce_min(...)</code></a>: Computes the minimum of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="math/reduce_prod"><code translate="no" dir="ltr">reduce_prod(...)</code></a>: Computes the product of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="math/reduce_sum"><code translate="no" dir="ltr">reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="strings/regex_replace"><code translate="no" dir="ltr">regex_replace(...)</code></a>: Replace elements of <code translate="no" dir="ltr">input</code> matching regex <code translate="no" dir="ltr">pattern</code> with <code translate="no" dir="ltr">rewrite</code>.</p> <p><a href="register_tensor_conversion_function"><code translate="no" dir="ltr">register_tensor_conversion_function(...)</code></a>: Registers a function for converting objects of <code translate="no" dir="ltr">base_type</code> to <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="repeat"><code translate="no" dir="ltr">repeat(...)</code></a>: Repeat elements of <code translate="no" dir="ltr">input</code></p> <p><a href="report_uninitialized_variables"><code translate="no" dir="ltr">report_uninitialized_variables(...)</code></a>: Adds ops to list the names of uninitialized variables.</p> <p><a href="required_space_to_batch_paddings"><code translate="no" dir="ltr">required_space_to_batch_paddings(...)</code></a>: Calculate padding required to make block_shape divide input_shape.</p> <p><a href="reset_default_graph"><code translate="no" dir="ltr">reset_default_graph(...)</code></a>: Clears the default graph stack and resets the global default graph.</p> <p><a href="reshape"><code translate="no" dir="ltr">reshape(...)</code></a>: Reshapes a tensor.</p> <p><a href="resource_variables_enabled"><code translate="no" dir="ltr">resource_variables_enabled(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if resource variables are enabled.</p> <p><a href="reverse"><code translate="no" dir="ltr">reverse(...)</code></a>: Reverses specific dimensions of a tensor.</p> <p><a href="reverse_sequence"><code translate="no" dir="ltr">reverse_sequence(...)</code></a>: Reverses variable length slices.</p> <p><a href="reverse"><code translate="no" dir="ltr">reverse_v2(...)</code></a>: Reverses specific dimensions of a tensor.</p> <p><a href="math/rint"><code translate="no" dir="ltr">rint(...)</code></a>: Returns element-wise integer closest to x.</p> <p><a href="roll"><code translate="no" dir="ltr">roll(...)</code></a>: Rolls the elements of a tensor along an axis.</p> <p><a href="math/round"><code translate="no" dir="ltr">round(...)</code></a>: Rounds the values of a tensor to the nearest integer, element-wise.</p> <p><a href="math/rsqrt"><code translate="no" dir="ltr">rsqrt(...)</code></a>: Computes reciprocal of square root of x element-wise.</p> <p><a href="dtypes/saturate_cast"><code translate="no" dir="ltr">saturate_cast(...)</code></a>: Performs a safe saturating cast of <code translate="no" dir="ltr">value</code> to <code translate="no" dir="ltr">dtype</code>.</p> <p><a href="math/scalar_mul"><code translate="no" dir="ltr">scalar_mul(...)</code></a>: Multiplies a scalar times a <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">IndexedSlices</code> object.</p> <p><a href="scan"><code translate="no" dir="ltr">scan(...)</code></a>: scan on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="scatter_add"><code translate="no" dir="ltr">scatter_add(...)</code></a>: Adds sparse updates to the variable referenced by <code translate="no" dir="ltr">resource</code>.</p> <p><a href="scatter_div"><code translate="no" dir="ltr">scatter_div(...)</code></a>: Divides a variable reference by sparse updates.</p> <p><a href="scatter_max"><code translate="no" dir="ltr">scatter_max(...)</code></a>: Reduces sparse updates into a variable reference using the <code translate="no" dir="ltr">max</code> operation.</p> <p><a href="scatter_min"><code translate="no" dir="ltr">scatter_min(...)</code></a>: Reduces sparse updates into a variable reference using the <code translate="no" dir="ltr">min</code> operation.</p> <p><a href="scatter_mul"><code translate="no" dir="ltr">scatter_mul(...)</code></a>: Multiplies sparse updates into a variable reference.</p> <p><a href="scatter_nd"><code translate="no" dir="ltr">scatter_nd(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into a new tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="scatter_nd_add"><code translate="no" dir="ltr">scatter_nd_add(...)</code></a>: Applies sparse addition to individual values or slices in a Variable.</p> <p><a href="scatter_nd_sub"><code translate="no" dir="ltr">scatter_nd_sub(...)</code></a>: Applies sparse subtraction to individual values or slices in a Variable.</p> <p><a href="scatter_nd_update"><code translate="no" dir="ltr">scatter_nd_update(...)</code></a>: Applies sparse <code translate="no" dir="ltr">updates</code> to individual values or slices in a Variable.</p> <p><a href="scatter_sub"><code translate="no" dir="ltr">scatter_sub(...)</code></a>: Subtracts sparse updates to a variable reference.</p> <p><a href="scatter_update"><code translate="no" dir="ltr">scatter_update(...)</code></a>: Applies sparse updates to a variable reference.</p> <p><a href="searchsorted"><code translate="no" dir="ltr">searchsorted(...)</code></a>: Searches input tensor for values on the innermost dimension.</p> <p><a href="math/segment_max"><code translate="no" dir="ltr">segment_max(...)</code></a>: Computes the maximum along segments of a tensor.</p> <p><a href="math/segment_mean"><code translate="no" dir="ltr">segment_mean(...)</code></a>: Computes the mean along segments of a tensor.</p> <p><a href="math/segment_min"><code translate="no" dir="ltr">segment_min(...)</code></a>: Computes the minimum along segments of a tensor.</p> <p><a href="math/segment_prod"><code translate="no" dir="ltr">segment_prod(...)</code></a>: Computes the product along segments of a tensor.</p> <p><a href="math/segment_sum"><code translate="no" dir="ltr">segment_sum(...)</code></a>: Computes the sum along segments of a tensor.</p> <p><a href="linalg/eigh"><code translate="no" dir="ltr">self_adjoint_eig(...)</code></a>: Computes the eigen decomposition of a batch of self-adjoint matrices.</p> <p><a href="linalg/eigvalsh"><code translate="no" dir="ltr">self_adjoint_eigvals(...)</code></a>: Computes the eigenvalues of one or more self-adjoint matrices.</p> <p><a href="sequence_mask"><code translate="no" dir="ltr">sequence_mask(...)</code></a>: Returns a mask tensor representing the first N positions of each cell.</p> <p><a href="io/serialize_many_sparse"><code translate="no" dir="ltr">serialize_many_sparse(...)</code></a>: Serialize <code translate="no" dir="ltr">N</code>-minibatch <code translate="no" dir="ltr">SparseTensor</code> into an <code translate="no" dir="ltr">[N, 3]</code> <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="io/serialize_sparse"><code translate="no" dir="ltr">serialize_sparse(...)</code></a>: Serialize a <code translate="no" dir="ltr">SparseTensor</code> into a 3-vector (1-D <code translate="no" dir="ltr">Tensor</code>) object.</p> <p><a href="io/serialize_tensor"><code translate="no" dir="ltr">serialize_tensor(...)</code></a>: Transforms a Tensor into a serialized TensorProto proto.</p> <p><a href="random/set_random_seed"><code translate="no" dir="ltr">set_random_seed(...)</code></a>: Sets the graph-level random seed for the default graph.</p> <p><a href="setdiff1d"><code translate="no" dir="ltr">setdiff1d(...)</code></a>: Computes the difference between two lists of numbers or strings.</p> <p><a href="shape"><code translate="no" dir="ltr">shape(...)</code></a>: Returns the shape of a tensor.</p> <p><a href="shape_n"><code translate="no" dir="ltr">shape_n(...)</code></a>: Returns shape of tensors.</p> <p><a href="math/sigmoid"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Computes sigmoid of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="math/sign"><code translate="no" dir="ltr">sign(...)</code></a>: Returns an element-wise indication of the sign of a number.</p> <p><a href="math/sin"><code translate="no" dir="ltr">sin(...)</code></a>: Computes sine of x element-wise.</p> <p><a href="math/sinh"><code translate="no" dir="ltr">sinh(...)</code></a>: Computes hyperbolic sine of x element-wise.</p> <p><a href="size"><code translate="no" dir="ltr">size(...)</code></a>: Returns the size of a tensor.</p> <p><a href="slice"><code translate="no" dir="ltr">slice(...)</code></a>: Extracts a slice from a tensor.</p> <p><a href="sort"><code translate="no" dir="ltr">sort(...)</code></a>: Sorts a tensor.</p> <p><a href="nn/space_to_batch"><code translate="no" dir="ltr">space_to_batch(...)</code></a>: SpaceToBatch for 4-D tensors of type T.</p> <p><a href="space_to_batch_nd"><code translate="no" dir="ltr">space_to_batch_nd(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href="nn/space_to_depth"><code translate="no" dir="ltr">space_to_depth(...)</code></a>: SpaceToDepth for tensors of type T.</p> <p><a href="sparse/add"><code translate="no" dir="ltr">sparse_add(...)</code></a>: Adds two tensors, at least one of each is a <code translate="no" dir="ltr">SparseTensor</code>. (deprecated arguments)</p> <p><a href="sparse/concat"><code translate="no" dir="ltr">sparse_concat(...)</code></a>: Concatenates a list of <code translate="no" dir="ltr">SparseTensor</code> along the specified dimension. (deprecated arguments)</p> <p><a href="sparse/fill_empty_rows"><code translate="no" dir="ltr">sparse_fill_empty_rows(...)</code></a>: Fills empty rows in the input 2-D <code translate="no" dir="ltr">SparseTensor</code> with a default value.</p> <p><a href="sparse/mask"><code translate="no" dir="ltr">sparse_mask(...)</code></a>: Masks elements of <code translate="no" dir="ltr">IndexedSlices</code>.</p> <p><a href="sparse_matmul"><code translate="no" dir="ltr">sparse_matmul(...)</code></a>: Multiply matrix "a" by matrix "b".</p> <p><a href="sparse/maximum"><code translate="no" dir="ltr">sparse_maximum(...)</code></a>: Returns the element-wise max of two SparseTensors.</p> <p><a href="sparse/merge"><code translate="no" dir="ltr">sparse_merge(...)</code></a>: Combines a batch of feature ids and values into a single <code translate="no" dir="ltr">SparseTensor</code>. (deprecated)</p> <p><a href="sparse/minimum"><code translate="no" dir="ltr">sparse_minimum(...)</code></a>: Returns the element-wise min of two SparseTensors.</p> <p><a href="sparse/placeholder"><code translate="no" dir="ltr">sparse_placeholder(...)</code></a>: Inserts a placeholder for a sparse tensor that will be always fed.</p> <p><a href="sparse/reduce_max"><code translate="no" dir="ltr">sparse_reduce_max(...)</code></a>: Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)</p> <p><a href="sparse/reduce_max_sparse"><code translate="no" dir="ltr">sparse_reduce_max_sparse(...)</code></a>: Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments)</p> <p><a href="sparse/reduce_sum"><code translate="no" dir="ltr">sparse_reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)</p> <p><a href="sparse/reduce_sum_sparse"><code translate="no" dir="ltr">sparse_reduce_sum_sparse(...)</code></a>: Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments)</p> <p><a href="sparse/reorder"><code translate="no" dir="ltr">sparse_reorder(...)</code></a>: Reorders a <code translate="no" dir="ltr">SparseTensor</code> into the canonical, row-major ordering.</p> <p><a href="sparse/reset_shape"><code translate="no" dir="ltr">sparse_reset_shape(...)</code></a>: Resets the shape of a <code translate="no" dir="ltr">SparseTensor</code> with indices and values unchanged.</p> <p><a href="sparse/reshape"><code translate="no" dir="ltr">sparse_reshape(...)</code></a>: Reshapes a <code translate="no" dir="ltr">SparseTensor</code> to represent values in a new dense shape.</p> <p><a href="sparse/retain"><code translate="no" dir="ltr">sparse_retain(...)</code></a>: Retains specified non-empty values within a <code translate="no" dir="ltr">SparseTensor</code>.</p> <p><a href="sparse/segment_mean"><code translate="no" dir="ltr">sparse_segment_mean(...)</code></a>: Computes the mean along sparse segments of a tensor.</p> <p><a href="sparse/segment_sqrt_n"><code translate="no" dir="ltr">sparse_segment_sqrt_n(...)</code></a>: Computes the sum along sparse segments of a tensor divided by the sqrt(N).</p> <p><a href="sparse/segment_sum"><code translate="no" dir="ltr">sparse_segment_sum(...)</code></a>: Computes the sum along sparse segments of a tensor.</p> <p><a href="sparse/slice"><code translate="no" dir="ltr">sparse_slice(...)</code></a>: Slice a <code translate="no" dir="ltr">SparseTensor</code> based on the <code translate="no" dir="ltr">start</code> and `size.</p> <p><a href="sparse/softmax"><code translate="no" dir="ltr">sparse_softmax(...)</code></a>: Applies softmax to a batched N-D <code translate="no" dir="ltr">SparseTensor</code>.</p> <p><a href="sparse/split"><code translate="no" dir="ltr">sparse_split(...)</code></a>: Split a <code translate="no" dir="ltr">SparseTensor</code> into <code translate="no" dir="ltr">num_split</code> tensors along <code translate="no" dir="ltr">axis</code>. (deprecated arguments)</p> <p><a href="sparse/sparse_dense_matmul"><code translate="no" dir="ltr">sparse_tensor_dense_matmul(...)</code></a>: Multiply SparseTensor (of rank 2) "A" by dense matrix "B".</p> <p><a href="sparse/to_dense"><code translate="no" dir="ltr">sparse_tensor_to_dense(...)</code></a>: Converts a <code translate="no" dir="ltr">SparseTensor</code> into a dense tensor.</p> <p><a href="sparse_to_dense"><code translate="no" dir="ltr">sparse_to_dense(...)</code></a>: Converts a sparse representation into a dense tensor. (deprecated)</p> <p><a href="sparse/to_indicator"><code translate="no" dir="ltr">sparse_to_indicator(...)</code></a>: Converts a <code translate="no" dir="ltr">SparseTensor</code> of ids into a dense bool indicator tensor.</p> <p><a href="sparse/transpose"><code translate="no" dir="ltr">sparse_transpose(...)</code></a>: Transposes a <code translate="no" dir="ltr">SparseTensor</code></p> <p><a href="split"><code translate="no" dir="ltr">split(...)</code></a>: Splits a tensor into sub tensors.</p> <p><a href="math/sqrt"><code translate="no" dir="ltr">sqrt(...)</code></a>: Computes square root of x element-wise.</p> <p><a href="math/square"><code translate="no" dir="ltr">square(...)</code></a>: Computes square of x element-wise.</p> <p><a href="math/squared_difference"><code translate="no" dir="ltr">squared_difference(...)</code></a>: Returns (x - y)(x - y) element-wise.</p> <p><a href="squeeze"><code translate="no" dir="ltr">squeeze(...)</code></a>: Removes dimensions of size 1 from the shape of a tensor. (deprecated arguments)</p> <p><a href="stack"><code translate="no" dir="ltr">stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor.</p> <p><a href="stop_gradient"><code translate="no" dir="ltr">stop_gradient(...)</code></a>: Stops gradient computation.</p> <p><a href="strided_slice"><code translate="no" dir="ltr">strided_slice(...)</code></a>: Extracts a strided slice of a tensor (generalized python array indexing).</p> <p><a href="strings/join"><code translate="no" dir="ltr">string_join(...)</code></a>: Joins the strings in the given list of string tensors into one tensor;</p> <p><a href="string_split"><code translate="no" dir="ltr">string_split(...)</code></a>: Split elements of <code translate="no" dir="ltr">source</code> based on <code translate="no" dir="ltr">delimiter</code>. (deprecated arguments)</p> <p><a href="strings/strip"><code translate="no" dir="ltr">string_strip(...)</code></a>: Strip leading and trailing whitespaces from the Tensor.</p> <p><a href="strings/to_hash_bucket"><code translate="no" dir="ltr">string_to_hash_bucket(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href="strings/to_hash_bucket_fast"><code translate="no" dir="ltr">string_to_hash_bucket_fast(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href="strings/to_hash_bucket_strong"><code translate="no" dir="ltr">string_to_hash_bucket_strong(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href="strings/to_number"><code translate="no" dir="ltr">string_to_number(...)</code></a>: Converts each string in the input Tensor to the specified numeric type.</p> <p><a href="substr"><code translate="no" dir="ltr">substr(...)</code></a>: Return substrings from <code translate="no" dir="ltr">Tensor</code> of strings.</p> <p><a href="math/subtract"><code translate="no" dir="ltr">subtract(...)</code></a>: Returns x - y element-wise.</p> <p><a href="linalg/svd"><code translate="no" dir="ltr">svd(...)</code></a>: Computes the singular value decompositions of one or more matrices.</p> <p><a href="switch_case"><code translate="no" dir="ltr">switch_case(...)</code></a>: Create a switch/case operation, i.e. an integer-indexed conditional.</p> <p><a href="initializers/tables_initializer"><code translate="no" dir="ltr">tables_initializer(...)</code></a>: Returns an Op that initializes all tables of the default graph.</p> <p><a href="math/tan"><code translate="no" dir="ltr">tan(...)</code></a>: Computes tan of x element-wise.</p> <p><a href="math/tanh"><code translate="no" dir="ltr">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="tensor_scatter_nd_add"><code translate="no" dir="ltr">tensor_scatter_add(...)</code></a>: Adds sparse <code translate="no" dir="ltr">updates</code> to an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensor_scatter_nd_add"><code translate="no" dir="ltr">tensor_scatter_nd_add(...)</code></a>: Adds sparse <code translate="no" dir="ltr">updates</code> to an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensor_scatter_nd_sub"><code translate="no" dir="ltr">tensor_scatter_nd_sub(...)</code></a>: Subtracts sparse <code translate="no" dir="ltr">updates</code> from an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensor_scatter_nd_update"><code translate="no" dir="ltr">tensor_scatter_nd_update(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensor_scatter_nd_sub"><code translate="no" dir="ltr">tensor_scatter_sub(...)</code></a>: Subtracts sparse <code translate="no" dir="ltr">updates</code> from an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensor_scatter_nd_update"><code translate="no" dir="ltr">tensor_scatter_update(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensordot"><code translate="no" dir="ltr">tensordot(...)</code></a>: Tensor contraction of a and b along specified axes and outer product.</p> <p><a href="tile"><code translate="no" dir="ltr">tile(...)</code></a>: Constructs a tensor by tiling a given tensor.</p> <p><a href="timestamp"><code translate="no" dir="ltr">timestamp(...)</code></a>: Provides the time since epoch in seconds.</p> <p><a href="to_bfloat16"><code translate="no" dir="ltr">to_bfloat16(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">bfloat16</code>. (deprecated)</p> <p><a href="to_complex128"><code translate="no" dir="ltr">to_complex128(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">complex128</code>. (deprecated)</p> <p><a href="to_complex64"><code translate="no" dir="ltr">to_complex64(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">complex64</code>. (deprecated)</p> <p><a href="to_double"><code translate="no" dir="ltr">to_double(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">float64</code>. (deprecated)</p> <p><a href="to_float"><code translate="no" dir="ltr">to_float(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">float32</code>. (deprecated)</p> <p><a href="to_int32"><code translate="no" dir="ltr">to_int32(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">int32</code>. (deprecated)</p> <p><a href="to_int64"><code translate="no" dir="ltr">to_int64(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">int64</code>. (deprecated)</p> <p><a href="linalg/trace"><code translate="no" dir="ltr">trace(...)</code></a>: Compute the trace of a tensor <code translate="no" dir="ltr">x</code>.</p> <p><a href="trainable_variables"><code translate="no" dir="ltr">trainable_variables(...)</code></a>: Returns all variables created with <code translate="no" dir="ltr">trainable=True</code>.</p> <p><a href="transpose"><code translate="no" dir="ltr">transpose(...)</code></a>: Transposes <code translate="no" dir="ltr">a</code>.</p> <p><a href="math/truediv"><code translate="no" dir="ltr">truediv(...)</code></a>: Divides x / y elementwise (using Python 3 division operator semantics).</p> <p><a href="random/truncated_normal"><code translate="no" dir="ltr">truncated_normal(...)</code></a>: Outputs random values from a truncated normal distribution.</p> <p><a href="truncatediv"><code translate="no" dir="ltr">truncatediv(...)</code></a>: Returns x / y element-wise for integer types.</p> <p><a href="truncatemod"><code translate="no" dir="ltr">truncatemod(...)</code></a>: Returns element-wise remainder of division. This emulates C semantics in that</p> <p><a href="tuple"><code translate="no" dir="ltr">tuple(...)</code></a>: Group tensors together.</p> <p><a href="unique"><code translate="no" dir="ltr">unique(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="unique_with_counts"><code translate="no" dir="ltr">unique_with_counts(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="unravel_index"><code translate="no" dir="ltr">unravel_index(...)</code></a>: Converts an array of flat indices into a tuple of coordinate arrays.</p> <p><a href="math/unsorted_segment_max"><code translate="no" dir="ltr">unsorted_segment_max(...)</code></a>: Computes the maximum along segments of a tensor.</p> <p><a href="math/unsorted_segment_mean"><code translate="no" dir="ltr">unsorted_segment_mean(...)</code></a>: Computes the mean along segments of a tensor.</p> <p><a href="math/unsorted_segment_min"><code translate="no" dir="ltr">unsorted_segment_min(...)</code></a>: Computes the minimum along segments of a tensor.</p> <p><a href="math/unsorted_segment_prod"><code translate="no" dir="ltr">unsorted_segment_prod(...)</code></a>: Computes the product along segments of a tensor.</p> <p><a href="math/unsorted_segment_sqrt_n"><code translate="no" dir="ltr">unsorted_segment_sqrt_n(...)</code></a>: Computes the sum along segments of a tensor divided by the sqrt(N).</p> <p><a href="math/unsorted_segment_sum"><code translate="no" dir="ltr">unsorted_segment_sum(...)</code></a>: Computes the sum along segments of a tensor.</p> <p><a href="unstack"><code translate="no" dir="ltr">unstack(...)</code></a>: Unpacks the given dimension of a rank-<code translate="no" dir="ltr">R</code> tensor into rank-<code translate="no" dir="ltr">(R-1)</code> tensors.</p> <p><a href="variable_axis_size_partitioner"><code translate="no" dir="ltr">variable_axis_size_partitioner(...)</code></a>: Get a partitioner for VariableScope to keep shards below <code translate="no" dir="ltr">max_shard_bytes</code>.</p> <p><a href="variable_creator_scope"><code translate="no" dir="ltr">variable_creator_scope(...)</code></a>: Scope which defines a variable creation function to be used by variable().</p> <p><a href="variable_op_scope"><code translate="no" dir="ltr">variable_op_scope(...)</code></a>: Deprecated: context manager for defining an op that creates variables.</p> <p><a href="initializers/variables"><code translate="no" dir="ltr">variables_initializer(...)</code></a>: Returns an Op that initializes a list of variables.</p> <p><a href="vectorized_map"><code translate="no" dir="ltr">vectorized_map(...)</code></a>: Parallel map on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="debugging/assert_all_finite"><code translate="no" dir="ltr">verify_tensor_all_finite(...)</code></a>: Assert that the tensor does not contain any NaN's or Inf's.</p> <p><a href="where"><code translate="no" dir="ltr">where(...)</code></a>: Return the elements, either from <code translate="no" dir="ltr">x</code> or <code translate="no" dir="ltr">y</code>, depending on the <code translate="no" dir="ltr">condition</code>. (deprecated)</p> <p><a href="where_v2"><code translate="no" dir="ltr">where_v2(...)</code></a>: Return the elements, either from <code translate="no" dir="ltr">x</code> or <code translate="no" dir="ltr">y</code>, depending on the <code translate="no" dir="ltr">condition</code>.</p> <p><a href="while_loop"><code translate="no" dir="ltr">while_loop(...)</code></a>: Repeat <code translate="no" dir="ltr">body</code> while the condition <code translate="no" dir="ltr">cond</code> is true.</p> <p><a href="wrap_function"><code translate="no" dir="ltr">wrap_function(...)</code></a>: Wraps the TF 1.x function fn into a graph function.</p> <p><a href="io/write_file"><code translate="no" dir="ltr">write_file(...)</code></a>: Writes contents to the file at input filename. Creates file and recursively</p> <p><a href="zeros"><code translate="no" dir="ltr">zeros(...)</code></a>: Creates a tensor with all elements set to zero.</p> <p><a href="zeros_like"><code translate="no" dir="ltr">zeros_like(...)</code></a>: Creates a tensor with all elements set to zero.</p> <p><a href="math/zeta"><code translate="no" dir="ltr">zeta(...)</code></a>: Compute the Hurwitz zeta function \(\zeta(x, q)\).</p> <h2 id="other_members" data-text="Other Members" tabindex="0">Other Members</h2> <ul> <li>
<code translate="no" dir="ltr">AUTO_REUSE</code> 
</li> <li>
<code translate="no" dir="ltr">COMPILER_VERSION = '7.3.1 20180303'</code> 
</li> <li>
<code translate="no" dir="ltr">CXX11_ABI_FLAG = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GIT_VERSION = 'v1.15.0-rc3-22-g590d6ee'</code> 
</li> <li>
<code translate="no" dir="ltr">GRAPH_DEF_VERSION = 134</code> 
</li> <li>
<code translate="no" dir="ltr">GRAPH_DEF_VERSION_MIN_CONSUMER = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GRAPH_DEF_VERSION_MIN_PRODUCER = 0</code> 
</li> <li>
<code translate="no" dir="ltr">MONOLITHIC_BUILD = 0</code> 
</li> <li>
<code translate="no" dir="ltr">QUANTIZED_DTYPES</code> 
</li> <li>
<code translate="no" dir="ltr">VERSION = '1.15.0'</code> 
</li> <li>
<code translate="no" dir="ltr">__version__ = '1.15.0'</code> 
</li> <li>
<code translate="no" dir="ltr">bfloat16</code> 
</li> <li>
<code translate="no" dir="ltr">bool</code> 
</li> <li>
<code translate="no" dir="ltr">complex128</code> 
</li> <li>
<code translate="no" dir="ltr">complex64</code> 
</li> <li>
<code translate="no" dir="ltr">double</code> 
</li> <li>
<code translate="no" dir="ltr">float16</code> 
</li> <li>
<code translate="no" dir="ltr">float32</code> 
</li> <li>
<code translate="no" dir="ltr">float64</code> 
</li> <li>
<code translate="no" dir="ltr">half</code> 
</li> <li>
<code translate="no" dir="ltr">int16</code> 
</li> <li>
<code translate="no" dir="ltr">int32</code> 
</li> <li>
<code translate="no" dir="ltr">int64</code> 
</li> <li>
<code translate="no" dir="ltr">int8</code> 
</li> <li>
<code translate="no" dir="ltr">newaxis = None</code> 
</li> <li>
<code translate="no" dir="ltr">qint16</code> 
</li> <li>
<code translate="no" dir="ltr">qint32</code> 
</li> <li>
<code translate="no" dir="ltr">qint8</code> 
</li> <li>
<code translate="no" dir="ltr">quint16</code> 
</li> <li>
<code translate="no" dir="ltr">quint8</code> 
</li> <li>
<code translate="no" dir="ltr">resource</code> 
</li> <li>
<code translate="no" dir="ltr">string</code> 
</li> <li>
<code translate="no" dir="ltr">uint16</code> 
</li> <li>
<code translate="no" dir="ltr">uint32</code> 
</li> <li>
<code translate="no" dir="ltr">uint64</code> 
</li> <li>
<code translate="no" dir="ltr">uint8</code> 
</li> <li>
<code translate="no" dir="ltr">variant</code> 
</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
     2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
