
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.quantization.quantize - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" Quantize the &#39;input&#39; tensor of type float to &#39;output&#39; tensor of type &#39;T&#39;. ">
  <meta name="keywords" content="tf, quantization, quantize, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/quantization/quantize.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.quantization.quantize</h1>      <table class="tfo-notebook-buttons tfo-api" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/array_ops.py#L4403-L4419">  View source on GitHub </a> </td> </table> <p>Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="0">View aliases</h4> <p> <b>Main aliases</b> </p>
<p>`tf.quantize`</p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/quantization/quantize"><code translate="no" dir="ltr">tf.compat.v1.quantization.quantize</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/quantization/quantize"><code translate="no" dir="ltr">tf.compat.v1.quantize</code></a>, `tf.compat.v2.quantization.quantize`</p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.quantization.quantize(
    input, min_range, max_range, T, mode='MIN_COMBINED',
    round_mode='HALF_AWAY_FROM_ZERO', name=None
)
</pre>  <p>[min_range, max_range] are scalar floats that specify the range for the 'input' data. The 'mode' attribute controls exactly which calculations are used to convert the float values to their quantized equivalents. The 'round_mode' attribute controls which rounding tie-breaking algorithm is used when rounding float values to their quantized equivalents.</p> <p>In 'MIN_COMBINED' mode, each value of the tensor will undergo the following:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
</pre> <p>here <code translate="no" dir="ltr">range(T) = numeric_limits&lt;T&gt;::max() - numeric_limits&lt;T&gt;::min()</code></p> <p><em>MIN_COMBINED Mode Example</em></p> <p>Assume the input is type float and has a possible range of [0.0, 6.0] and the output type is quint8 ([0, 255]). The min_range and max_range values should be specified as 0.0 and 6.0. Quantizing from float to quint8 will multiply each value of the input by 255/6 and cast to quint8.</p> <p>If the output type was qint8 ([-128, 127]), the operation will additionally subtract each value by 128 prior to casting, so that the range of values aligns with the range of qint8.</p> <p>If the mode is 'MIN_FIRST', then this approach is used:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">num_discrete_values = 1 &lt;&lt; (# of bits in T)
range_adjust = num_discrete_values / (num_discrete_values - 1)
range = (range_max - range_min) * range_adjust
range_scale = num_discrete_values / range
quantized = round(input * range_scale) - round(range_min * range_scale) +
  numeric_limits&lt;T&gt;::min()
quantized = max(quantized, numeric_limits&lt;T&gt;::min())
quantized = min(quantized, numeric_limits&lt;T&gt;::max())
</pre> <p>The biggest difference between this and MIN_COMBINED is that the minimum range is rounded first, before it's subtracted from the rounded value. With MIN_COMBINED, a small bias is introduced where repeated iterations of quantizing and dequantizing will introduce a larger and larger error.</p> <p><em>SCALED mode Example</em></p> <p><code translate="no" dir="ltr">SCALED</code> mode matches the quantization approach used in <code translate="no" dir="ltr">QuantizeAndDequantize{V2|V3}</code>.</p> <p>If the mode is <code translate="no" dir="ltr">SCALED</code>, we do not use the full range of the output type, choosing to elide the lowest possible value for symmetry (e.g., output range is -127 to 127, not -128 to 127 for signed 8 bit quantization), so that 0.0 maps to 0.</p> <p>We first find the range of values in our tensor. The range we use is always centered on 0, so we find m such that</p> <pre class="prettyprint lang-c++" translate="no" dir="ltr" data-language="cpp">m = max(abs(input_min), abs(input_max))
</pre> <p>Our input tensor range is then <code translate="no" dir="ltr">[-m, m]</code>.</p> <p>Next, we choose our fixed-point quantization buckets, <code translate="no" dir="ltr">[min_fixed, max_fixed]</code>. If T is signed, this is</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">num_bits = sizeof(T) * 8
[min_fixed, max_fixed] =
    [-(1 &lt;&lt; (num_bits - 1) - 1), (1 &lt;&lt; (num_bits - 1)) - 1]
</pre> <p>Otherwise, if T is unsigned, the fixed-point range is</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">[min_fixed, max_fixed] = [0, (1 &lt;&lt; num_bits) - 1]
</pre> <p>From this we compute our scaling factor, s:</p> <pre class="prettyprint lang-c++" translate="no" dir="ltr" data-language="cpp">s = (max_fixed - min_fixed) / (2 * m)
</pre> <p>Now we can quantize the elements of our tensor:</p> <pre class="prettyprint lang-c++" translate="no" dir="ltr" data-language="cpp">result = round(input * s)
</pre> <p>One thing to watch out for is that the operator may choose to adjust the requested minimum and maximum values slightly during the quantization process, so you should always use the output ports as the range for further calculations. For example, if the requested minimum and maximum values are close to equal, they will be separated by a small epsilon value to prevent ill-formed quantized buffers from being created. Otherwise, you can end up with buffers where all the quantized values map to the same float value, which causes problems for operations that have to perform further calculations on them.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">min_range</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code>. The minimum scalar value possibly produced for the input. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_range</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code>. The maximum scalar value possibly produced for the input. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">T</code> </td> <td> A <a href="../dtypes/dtype"><code translate="no" dir="ltr">tf.DType</code></a> from: <code translate="no" dir="ltr">tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mode</code> </td> <td> An optional <code translate="no" dir="ltr">string</code> from: <code translate="no" dir="ltr">"MIN_COMBINED", "MIN_FIRST", "SCALED"</code>. Defaults to <code translate="no" dir="ltr">"MIN_COMBINED"</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">round_mode</code> </td> <td> An optional <code translate="no" dir="ltr">string</code> from: <code translate="no" dir="ltr">"HALF_AWAY_FROM_ZERO", "HALF_TO_EVEN"</code>. Defaults to <code translate="no" dir="ltr">"HALF_AWAY_FROM_ZERO"</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tuple of <code translate="no" dir="ltr">Tensor</code> objects (output, output_min, output_max). </td> </tr> <tr> <td> <code translate="no" dir="ltr">output</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">T</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_min</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_max</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code>. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/quantization/quantize" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/quantization/quantize</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
