
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.contrib.tpu.InfeedQueue - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" A helper object to build a device infeed queue. ">
  <meta name="keywords" content="tf, contrib, tpu, infeedqueue, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/contrib/tpu/infeedqueue.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.contrib.tpu.InfeedQueue</h1>       <p>A helper object to build a device infeed queue.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.contrib.tpu.InfeedQueue(
    number_of_tuple_elements=None, tuple_types=None, tuple_shapes=None,
    shard_dimensions=None, name=None
)
</pre>  <p>The InfeedQueue builds the host-side and device-side Ops to enqueue and dequeue elements, respectively, and ensures that their types and shapes match.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">number_of_tuple_elements</code> </td> <td> the number of Tensors fed atomically through the queue, must be present unless it can be inferred from other arguments. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">tuple_types</code> </td> <td> if not None, a list of types of the elements of the queue. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">tuple_shapes</code> </td> <td> if not None, a list of shapes of the elements of the queue. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shard_dimensions</code> </td> <td> if not None, a list of dimensions on which the elements of the queue should be sharded during automatic parallelization. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> the name of the queue. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if number_of_tuple_elements &lt;= 0; or number_of_tuple_arguments, tuple_types, tuple_shapes, and shard_dimensions are all None; or the length of tuple_types, tuple_shapes, or shard_dimensions is not equal to number_of_tuple_elements; or any element of shard_dimensions can't be converted to a Dimension. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if any element of tuple_types or tuple_shapes can't be converted to a dtype or TensorShape, respectively. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">number_of_shards</code> </td> <td> Gets the number of shards to use for the InfeedQueue. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">number_of_tuple_elements</code> </td> <td> Returns the number of InfeedQueue tuple elements. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shard_dimensions</code> </td> <td> Gets the shard dimension of each tuple element. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sharding_policies</code> </td> <td> Returns the sharding policies of the InfeedQueue tuple elements. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">tuple_shapes</code> </td> <td> Returns the shapes of the InfeedQueue tuple elements. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">tuple_types</code> </td> <td> Returns the types of the InfeedQueue tuple elements. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="freeze" data-text="freeze" tabindex="0"><code translate="no" dir="ltr">freeze</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L434-L458">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
freeze()
</pre> <p>Freezes the InfeedQueue so it can no longer be modified.</p> <p>The configuration is implicitly frozen before any host-side or device-side Ops are generated. The configuration cannot be frozen until the types and shapes of the tuple elements have been set.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the types or shapes of the tuple elements have not been set. </td> </tr> </table> <h3 id="generate_dequeue_op" data-text="generate_dequeue_op" tabindex="0"><code translate="no" dir="ltr">generate_dequeue_op</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L460-L497">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
generate_dequeue_op(
    tpu_device=0
)
</pre> <p>Generates the device-side Op to dequeue a tuple from the queue.</p> <p>Implicitly freezes the queue configuration if it is not already frozen, which will raise errors if the shapes and types have not been fully specified.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tpu_device</code> </td> <td> The TPU device ordinal where the infeed instruction should be placed. If None, no explicit placement will be performed, and it is up to the user to call this API from within a proper TPU device scope. The XLA code will fail if the TPU dequeue instruction is not bound to any device. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of Outputs corresponding to a shard of infeed dequeued into XLA, suitable for use within a replicated block. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the types or shapes of the tuple elements have not been set; or if a dequeue op has already been generated. </td> </tr> </table> <h3 id="generate_enqueue_ops" data-text="generate_enqueue_ops" tabindex="0"><code translate="no" dir="ltr">generate_enqueue_ops</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L552-L612">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
generate_enqueue_ops(
    sharded_inputs, tpu_ordinal_function=None, placement_function=None
)
</pre> <p>Generates the host-side Ops to enqueue the shards of a tuple.</p> <p>sharded_inputs is a list, one for each shard, of lists of Tensors. sharded_inputs[0] is the tuple of Tensors to use to feed shard 0 if the queue. Returns the host-side Ops that must be run to enqueue the sharded tuple. The Op for shard i is colocated with the inputs for shard i.</p> <p>Implicitly freezes the queue configuration if it is not already frozen. If the configuration has already been frozen, and is not compatible with the types and shapes of sharded_inputs, an error will be raised.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">sharded_inputs</code> </td> <td> a list of lists of Tensors. The length of the outer list determines the number of shards. Each inner list indicates the types and shapes of the tuples in the corresponding shard. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">tpu_ordinal_function</code> </td> <td> if not None, a function that takes the shard index as input and returns the ordinal of the TPU device the shard's infeed should be placed on. tpu_ordinal_function must be set if the inputs are placed on CPU devices. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">placement_function</code> </td> <td> if not None, a function that takes the shard index as input and returns the host device where the enqueue op should be placed on. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of host-side Ops, one for each shard, that when executed together will enqueue a full-size element of infeed. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the queue configuration has previously been frozen and the shapes of the elements of sharded_inputs are not compatible with the frozen configuration; or if the shapes of the elements of sharded_inputs don't form a consistent unsharded tuple; or if the elements of a tuple have different device constraints. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if the queue configuration has previously been frozen and the types of the elements of sharded_inputs are not compatible with the frozen configuration; or if the types of the elements of sharded_inputs don't form a consistent unsharded tuple. </td> </tr> </table> <h3 id="set_configuration_from_input_tensors" data-text="set_configuration_from_input_tensors" tabindex="0"><code translate="no" dir="ltr">set_configuration_from_input_tensors</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L364-L382">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_configuration_from_input_tensors(
    input_tensors
)
</pre> <p>Sets the shapes and types of the queue tuple elements.</p> <p>input_tensors is a list of Tensors whose types and shapes are used to set the queue configuration.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_tensors</code> </td> <td> list of Tensors of the same types and shapes as the desired queue Tuple. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if input_tensors is not a list of length self.number_of_tuple_elements </td> </tr> </table> <h3 id="set_configuration_from_sharded_input_tensors" data-text="set_configuration_from_sharded_input_tensors" tabindex="0"><code translate="no" dir="ltr">set_configuration_from_sharded_input_tensors</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L384-L432">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_configuration_from_sharded_input_tensors(
    input_tensors
)
</pre> <p>Sets the shapes and types of the queue tuple elements.</p> <p>input_tensors is a list of lists of Tensors whose types and shapes are used to set the queue configuration. The length of the outer list is the number of shards required, and each inner list is the tuple of Tensors to use to determine the types and shapes of the corresponding shard. This method depends on the shard dimension, and calling it freezes the shard policy.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_tensors</code> </td> <td> list of lists of Tensors. The outer list length corresponds to the desired number of shards, and each inner list is the size and shape of the desired configuration of the corresponding shard. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if any inner list is not a list of length self.number_of_tuple_elements; or the inner lists do not combine to form a consistent unsharded shape. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if the types of the Tensors in the inner lists do not match. </td> </tr> </table> <h3 id="set_number_of_shards" data-text="set_number_of_shards" tabindex="0"><code translate="no" dir="ltr">set_number_of_shards</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L349-L362">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_number_of_shards(
    number_of_shards
)
</pre> <p>Sets the number of shards to use for the InfeedQueue.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">number_of_shards</code> </td> <td> number of ways to shard the InfeedQueue. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if number_of_shards is not &gt; 0; or the policies have been frozen and number_of_shards was already set to something else. </td> </tr> </table> <h3 id="set_shard_dimensions" data-text="set_shard_dimensions" tabindex="0"><code translate="no" dir="ltr">set_shard_dimensions</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L314-L337">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_shard_dimensions(
    shard_dimensions
)
</pre> <p>Sets the shard_dimension of each element of the queue.</p> <p>shard_dimensions must be a list of length self.number_of_tuple_elements, and each element must be convertible to a Dimension compatible with self.tuple_shapes.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">shard_dimensions</code> </td> <td> the dimensions of each queue element. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if shard_dimensions is not of length self.number_of_tuple_elements; or an element of shard_dimensions cannot be converted to a Dimension; or an element of shard_dimensions is a Dimension that is out of range for the corresponding tuple element shape. </td> </tr> </table> <h3 id="set_tuple_shapes" data-text="set_tuple_shapes" tabindex="0"><code translate="no" dir="ltr">set_tuple_shapes</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L260-L295">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_tuple_shapes(
    tuple_shapes
)
</pre> <p>Sets the shape of each element of the queue.</p> <p>tuple_shapes must be a list of length self.number_of_tuple_elements, and each element must be convertible to a TensorShape.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tuple_shapes</code> </td> <td> the shapes of each queue element. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if tuple_shapes is not of length self.number_of_tuple_elements. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if an element of tuple_shapes cannot be converted to a TensorShape. </td> </tr> </table> <h3 id="set_tuple_types" data-text="set_tuple_types" tabindex="0"><code translate="no" dir="ltr">set_tuple_types</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L221-L253">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_tuple_types(
    tuple_types
)
</pre> <p>Sets the type of each element of the queue.</p> <p>tuple_types must be a list of length self.number_of_tuple_elements, and each element must be convertible to a dtype.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tuple_types</code> </td> <td> the types of each queue element. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if tuple_types is not of length self.number_of_tuple_elements. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if an element of tuple_types cannot be converted to a dtype. </td> </tr> </table> <h3 id="split_inputs_and_generate_enqueue_ops" data-text="split_inputs_and_generate_enqueue_ops" tabindex="0"><code translate="no" dir="ltr">split_inputs_and_generate_enqueue_ops</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_feed.py#L625-L730">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
split_inputs_and_generate_enqueue_ops(
    inputs, device_assignment=None, placement_function=None,
    tpu_ordinal_function=None
)
</pre> <p>POORLY-PERFORMING ON MULTI-HOST SYSTEMS.</p> <p>Generates the host-side Ops to enqueue a tuple.</p> <p>This method performs poorly because it takes an entire input on a single host, splits it, and distributes it to all of the cores. It is present only to simplify tutorial examples.</p> <p>inputs is a list of Tensors to use to feed the queue. Each input is split into self.number_of_shards shards. Returns an Op for each shard to enqueue the shard. The Op for shard i is placed on device placement_function(i).</p> <p>Implicitly freezes the queue configuration if it is not already frozen. If the configuration has already been frozen, and is not compatible with the types and shapes of inputs, an error will be raised.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> <tr class="alt"> <td colspan="2"> inputs: a list of Tensors which indicates the types and shapes of the queue tuple. </td> </tr> <tr> <td> <code translate="no" dir="ltr">device_assignment</code> </td> <td> if not <code translate="no" dir="ltr">None</code>, a TPU <code translate="no" dir="ltr">DeviceAssignment</code>. If device_assignment is not <code translate="no" dir="ltr">None</code>, but <code translate="no" dir="ltr">placement_function</code> and <code translate="no" dir="ltr">ordinal_function</code> are None, then <code translate="no" dir="ltr">device_assignment</code> will be used to place infeeds on the first k TPU shards, where k is the number of shards in the queue. If all three are <code translate="no" dir="ltr">None</code>, then default placement and ordinal functions are used. placement_function: if not None, a function that takes the shard index as input and returns a device string indicating which device the shard's infeed should be placed on. If placement_function and tpu_ordinal_function are None, inputs are sharded round-robin across the devices in the system. tpu_ordinal_function: if not None, a function that takes the shard index as input and returns the ordinal of the TPU device the shard's infeed should be placed on. If placement_function and tpu_ordinal_function are None, inputs are sharded round-robin across the devices in the system. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of host-side Ops, one for each shard, that when executed together will enqueue a full-size element of infeed. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the queue configuration has previously been frozen and the shapes of the elements of inputs are not compatible with the frozen configuration. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if the queue configuration has previously been frozen and the types of the elements of inputs are not compatible with the frozen configuration. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/tpu/InfeedQueue" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/tpu/InfeedQueue</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
