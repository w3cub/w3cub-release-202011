
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" Affine MaskedAutoregressiveFlow bijector for vector-valued events. ">
  <meta name="keywords" content="tf, contrib, distributions, bijectors, maskedautoregressiveflow, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/contrib/distributions/bijectors/maskedautoregressiveflow.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow</h1>       <p>Affine MaskedAutoregressiveFlow bijector for vector-valued events.</p> <p>Inherits From: <a href="bijector"><code translate="no" dir="ltr">Bijector</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow(
    shift_and_log_scale_fn, is_constant_jacobian=False, validate_args=False,
    unroll_loop=False, name=None
)
</pre>  <p>The affine autoregressive flow [(Papamakarios et al., 2016)][3] provides a relatively simple framework for user-specified (deep) architectures to learn a distribution over vector-valued events. Regarding terminology,</p> <p>"Autoregressive models decompose the joint density as a product of conditionals, and model each conditional in turn. Normalizing flows transform a base density (e.g. a standard Gaussian) into the target density by an invertible transformation with tractable Jacobian." [(Papamakarios et al., 2016)][3]</p> <p>In other words, the "autoregressive property" is equivalent to the decomposition, <code translate="no" dir="ltr">p(x) = prod{ p(x[i] | x[0:i]) : i=0, ..., d }</code>. The provided <code translate="no" dir="ltr">shift_and_log_scale_fn</code>, <code translate="no" dir="ltr">masked_autoregressive_default_template</code>, achieves this property by zeroing out weights in its <code translate="no" dir="ltr">masked_dense</code> layers.</p> <p>In the <a href="https://www.tensorflow.org/probability/api_docs/python/tfp"><code translate="no" dir="ltr">tfp</code></a> framework, a "normalizing flow" is implemented as a <a href="https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Bijector"><code translate="no" dir="ltr">tfp.bijectors.Bijector</code></a>. The <code translate="no" dir="ltr">forward</code> "autoregression" is implemented using a <a href="../../../while_loop"><code translate="no" dir="ltr">tf.while_loop</code></a> and a deep neural network (DNN) with masked weights such that the autoregressive property is automatically met in the <code translate="no" dir="ltr">inverse</code>.</p> <p>A <code translate="no" dir="ltr">TransformedDistribution</code> using <code translate="no" dir="ltr">MaskedAutoregressiveFlow(...)</code> uses the (expensive) forward-mode calculation to draw samples and the (cheap) reverse-mode calculation to compute log-probabilities. Conversely, a <code translate="no" dir="ltr">TransformedDistribution</code> using <code translate="no" dir="ltr">Invert(MaskedAutoregressiveFlow(...))</code> uses the (expensive) forward-mode calculation to compute log-probabilities and the (cheap) reverse-mode calculation to compute samples. See "Example Use" [below] for more details.</p> <p>Given a <code translate="no" dir="ltr">shift_and_log_scale_fn</code>, the forward and inverse transformations are (a sequence of) affine transformations. A "valid" <code translate="no" dir="ltr">shift_and_log_scale_fn</code> must compute each <code translate="no" dir="ltr">shift</code> (aka <code translate="no" dir="ltr">loc</code> or "mu" in [Germain et al. (2015)][1]) and <code translate="no" dir="ltr">log(scale)</code> (aka "alpha" in [Germain et al. (2015)][1]) such that each are broadcastable with the arguments to <code translate="no" dir="ltr">forward</code> and <code translate="no" dir="ltr">inverse</code>, i.e., such that the calculations in <code translate="no" dir="ltr">forward</code>, <code translate="no" dir="ltr">inverse</code> [below] are possible.</p> <p>For convenience, <code translate="no" dir="ltr">masked_autoregressive_default_template</code> is offered as a possible <code translate="no" dir="ltr">shift_and_log_scale_fn</code> function. It implements the MADE architecture [(Germain et al., 2015)][1]. MADE is a feed-forward network that computes a <code translate="no" dir="ltr">shift</code> and <code translate="no" dir="ltr">log(scale)</code> using <code translate="no" dir="ltr">masked_dense</code> layers in a deep neural network. Weights are masked to ensure the autoregressive property. It is possible that this architecture is suboptimal for your task. To build alternative networks, either change the arguments to <code translate="no" dir="ltr">masked_autoregressive_default_template</code>, use the <code translate="no" dir="ltr">masked_dense</code> function to roll-out your own, or use some other architecture, e.g., using <a href="../../../layers"><code translate="no" dir="ltr">tf.layers</code></a>.</p> <aside class="warning"><strong>Warning:</strong><span> no attempt is made to validate that the <code translate="no" dir="ltr">shift_and_log_scale_fn</code> enforces the "autoregressive property".</span></aside> <p>Assuming <code translate="no" dir="ltr">shift_and_log_scale_fn</code> has valid shape and autoregressive semantics, the forward transformation is</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def forward(x):
  y = zeros_like(x)
  event_size = x.shape[-1]
  for _ in range(event_size):
    shift, log_scale = shift_and_log_scale_fn(y)
    y = x * math_ops.exp(log_scale) + shift
  return y
</pre> <p>and the inverse transformation is</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def inverse(y):
  shift, log_scale = shift_and_log_scale_fn(y)
  return (y - shift) / math_ops.exp(log_scale)
</pre> <p>Notice that the <code translate="no" dir="ltr">inverse</code> does not need a for-loop. This is because in the forward pass each calculation of <code translate="no" dir="ltr">shift</code> and <code translate="no" dir="ltr">log_scale</code> is based on the <code translate="no" dir="ltr">y</code> calculated so far (not <code translate="no" dir="ltr">x</code>). In the <code translate="no" dir="ltr">inverse</code>, the <code translate="no" dir="ltr">y</code> is fully known, thus is equivalent to the scaling used in <code translate="no" dir="ltr">forward</code> after <code translate="no" dir="ltr">event_size</code> passes, i.e., the "last" <code translate="no" dir="ltr">y</code> used to compute <code translate="no" dir="ltr">shift</code>, <code translate="no" dir="ltr">log_scale</code>. (Roughly speaking, this also proves the transform is bijective.)</p> <h4 id="examples" data-text="Examples" tabindex="0">Examples</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">import tensorflow_probability as tfp
tfd = tfp.distributions
tfb = tfp.bijectors

dims = 5

# A common choice for a normalizing flow is to use a Gaussian for the base
# distribution. (However, any continuous distribution would work.) E.g.,
maf = tfd.TransformedDistribution(
    distribution=tfd.Normal(loc=0., scale=1.),
    bijector=tfb.MaskedAutoregressiveFlow(
        shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(
            hidden_layers=[512, 512])),
    event_shape=[dims])

x = maf.sample()  # Expensive; uses `tf.while_loop`, no Bijector caching.
maf.log_prob(x)   # Almost free; uses Bijector caching.
maf.log_prob(0.)  # Cheap; no `tf.while_loop` despite no Bijector caching.

# [Papamakarios et al. (2016)][3] also describe an Inverse Autoregressive
# Flow [(Kingma et al., 2016)][2]:
iaf = tfd.TransformedDistribution(
    distribution=tfd.Normal(loc=0., scale=1.),
    bijector=tfb.Invert(tfb.MaskedAutoregressiveFlow(
        shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(
            hidden_layers=[512, 512]))),
    event_shape=[dims])

x = iaf.sample()  # Cheap; no `tf.while_loop` despite no Bijector caching.
iaf.log_prob(x)   # Almost free; uses Bijector caching.
iaf.log_prob(0.)  # Expensive; uses `tf.while_loop`, no Bijector caching.

# In many (if not most) cases the default `shift_and_log_scale_fn` will be a
# poor choice. Here's an example of using a "shift only" version and with a
# different number/depth of hidden layers.
shift_only = True
maf_no_scale_hidden2 = tfd.TransformedDistribution(
    distribution=tfd.Normal(loc=0., scale=1.),
    bijector=tfb.MaskedAutoregressiveFlow(
        tfb.masked_autoregressive_default_template(
            hidden_layers=[32],
            shift_only=shift_only),
        is_constant_jacobian=shift_only),
    event_shape=[dims])
</pre> <h4 id="references" data-text="References" tabindex="0">References</h4> <p>[1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked Autoencoder for Distribution Estimation. In <em>International Conference on Machine Learning</em>, 2015. <a href="https://arxiv.org/abs/1502.03509">https://arxiv.org/abs/1502.03509</a></p> <p>[2]: Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improving Variational Inference with Inverse Autoregressive Flow. In <em>Neural Information Processing Systems</em>, 2016. <a href="https://arxiv.org/abs/1606.04934">https://arxiv.org/abs/1606.04934</a></p> <p>[3]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017. <a href="https://arxiv.org/abs/1705.07057">https://arxiv.org/abs/1705.07057</a></p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">shift_and_log_scale_fn</code> </td> <td> Python <code translate="no" dir="ltr">callable</code> which computes <code translate="no" dir="ltr">shift</code> and <code translate="no" dir="ltr">log_scale</code> from both the forward domain (<code translate="no" dir="ltr">x</code>) and the inverse domain (<code translate="no" dir="ltr">y</code>). Calculation must respect the "autoregressive property" (see class docstring). Suggested default <code translate="no" dir="ltr">masked_autoregressive_default_template(hidden_layers=...)</code>. Typically the function contains <code translate="no" dir="ltr">tf.Variables</code> and is wrapped using <a href="../../../make_template"><code translate="no" dir="ltr">tf.compat.v1.make_template</code></a>. Returning <code translate="no" dir="ltr">None</code> for either (both) <code translate="no" dir="ltr">shift</code>, <code translate="no" dir="ltr">log_scale</code> is equivalent to (but more efficient than) returning zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">is_constant_jacobian</code> </td> <td> Python <code translate="no" dir="ltr">bool</code>. Default: <code translate="no" dir="ltr">False</code>. When <code translate="no" dir="ltr">True</code> the implementation assumes <code translate="no" dir="ltr">log_scale</code> does not depend on the forward domain (<code translate="no" dir="ltr">x</code>) or inverse domain (<code translate="no" dir="ltr">y</code>) values. (No validation is made; <code translate="no" dir="ltr">is_constant_jacobian=False</code> is always safe but possibly computationally inefficient.) </td> </tr>
<tr> <td> <code translate="no" dir="ltr">validate_args</code> </td> <td> Python <code translate="no" dir="ltr">bool</code> indicating whether arguments should be checked for correctness. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">unroll_loop</code> </td> <td> Python <code translate="no" dir="ltr">bool</code> indicating whether the <a href="../../../while_loop"><code translate="no" dir="ltr">tf.while_loop</code></a> in <code translate="no" dir="ltr">_forward</code> should be replaced with a static for loop. Requires that the final dimension of <code translate="no" dir="ltr">x</code> be known at graph construction time. Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Python <code translate="no" dir="ltr">str</code>, name given to ops managed by this object. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> dtype of <code translate="no" dir="ltr">Tensor</code>s transformable by this distribution. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">forward_min_event_ndims</code> </td> <td> Returns the minimal number of dimensions bijector.forward operates on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">graph_parents</code> </td> <td> Returns this <code translate="no" dir="ltr">Bijector</code>'s graph_parents as a Python list. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">inverse_min_event_ndims</code> </td> <td> Returns the minimal number of dimensions bijector.inverse operates on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">is_constant_jacobian</code> </td> <td> Returns true iff the Jacobian matrix is not a function of x. <blockquote class="note">
<strong>Note:</strong><span> Jacobian matrix is either constant for both forward and inverse or neither. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Returns the string name of this <code translate="no" dir="ltr">Bijector</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">validate_args</code> </td> <td> Returns True if Tensor arguments will be validated. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="forward" data-text="forward" tabindex="0"><code translate="no" dir="ltr">forward</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L753-L768">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward(
    x, name='forward'
)
</pre> <p>Returns the forward <code translate="no" dir="ltr">Bijector</code> evaluation, i.e., X = g(Y).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "forward" evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">x.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if <code translate="no" dir="ltr">_forward</code> is not implemented. </td> </tr> </table> <h3 id="forward_event_shape" data-text="forward_event_shape" tabindex="0"><code translate="no" dir="ltr">forward_event_shape</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L677-L690">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward_event_shape(
    input_shape
)
</pre> <p>Shape of a single sample from a single batch as a <code translate="no" dir="ltr">TensorShape</code>.</p> <p>Same meaning as <code translate="no" dir="ltr">forward_event_shape_tensor</code>. May be only partially defined.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_shape</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape passed into <code translate="no" dir="ltr">forward</code> function. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">forward_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape after applying <code translate="no" dir="ltr">forward</code>. Possibly unknown. </td> </tr> </table> <h3 id="forward_event_shape_tensor" data-text="forward_event_shape_tensor" tabindex="0"><code translate="no" dir="ltr">forward_event_shape_tensor</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L653-L670">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward_event_shape_tensor(
    input_shape, name='forward_event_shape_tensor'
)
</pre> <p>Shape of a single sample from a single batch as an <code translate="no" dir="ltr">int32</code> 1D <code translate="no" dir="ltr">Tensor</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_shape</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape passed into <code translate="no" dir="ltr">forward</code> function. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> name to give to the op </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">forward_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape after applying <code translate="no" dir="ltr">forward</code>. </td> </tr> </table> <h3 id="forward_log_det_jacobian" data-text="forward_log_det_jacobian" tabindex="0"><code translate="no" dir="ltr">forward_log_det_jacobian</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L973-L997">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
forward_log_det_jacobian(
    x, event_ndims, name='forward_log_det_jacobian'
)
</pre> <p>Returns both the forward_log_det_jacobian.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "forward" Jacobian determinant evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">event_ndims</code> </td> <td> Number of dimensions in the probabilistic events being transformed. Must be greater than or equal to <code translate="no" dir="ltr">self.forward_min_event_ndims</code>. The result is summed over the final dimensions to produce a scalar Jacobian determinant for each event, i.e. it has shape <code translate="no" dir="ltr">x.shape.ndims - event_ndims</code> dimensions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>, if this bijector is injective. If not injective this is not implemented. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">y.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if neither <code translate="no" dir="ltr">_forward_log_det_jacobian</code> nor {<code translate="no" dir="ltr">_inverse</code>, <code translate="no" dir="ltr">_inverse_log_det_jacobian</code>} are implemented, or this is a non-injective bijector. </td> </tr> </table> <h3 id="inverse" data-text="inverse" tabindex="0"><code translate="no" dir="ltr">inverse</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L787-L804">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse(
    y, name='inverse'
)
</pre> <p>Returns the inverse <code translate="no" dir="ltr">Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">y</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "inverse" evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>, if this bijector is injective. If not injective, returns the k-tuple containing the unique <code translate="no" dir="ltr">k</code> points <code translate="no" dir="ltr">(x1, ..., xk)</code> such that <code translate="no" dir="ltr">g(xi) = y</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">y.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if <code translate="no" dir="ltr">_inverse</code> is not implemented. </td> </tr> </table> <h3 id="inverse_event_shape" data-text="inverse_event_shape" tabindex="0"><code translate="no" dir="ltr">inverse_event_shape</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L721-L734">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse_event_shape(
    output_shape
)
</pre> <p>Shape of a single sample from a single batch as a <code translate="no" dir="ltr">TensorShape</code>.</p> <p>Same meaning as <code translate="no" dir="ltr">inverse_event_shape_tensor</code>. May be only partially defined.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">output_shape</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape passed into <code translate="no" dir="ltr">inverse</code> function. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inverse_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">TensorShape</code> indicating event-portion shape after applying <code translate="no" dir="ltr">inverse</code>. Possibly unknown. </td> </tr> </table> <h3 id="inverse_event_shape_tensor" data-text="inverse_event_shape_tensor" tabindex="0"><code translate="no" dir="ltr">inverse_event_shape_tensor</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L697-L714">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse_event_shape_tensor(
    output_shape, name='inverse_event_shape_tensor'
)
</pre> <p>Shape of a single sample from a single batch as an <code translate="no" dir="ltr">int32</code> 1D <code translate="no" dir="ltr">Tensor</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">output_shape</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape passed into <code translate="no" dir="ltr">inverse</code> function. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> name to give to the op </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inverse_event_shape_tensor</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">int32</code> vector indicating event-portion shape after applying <code translate="no" dir="ltr">inverse</code>. </td> </tr> </table> <h3 id="inverse_log_det_jacobian" data-text="inverse_log_det_jacobian" tabindex="0"><code translate="no" dir="ltr">inverse_log_det_jacobian</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/distributions/bijector_impl.py#L871-L900">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
inverse_log_det_jacobian(
    y, event_ndims, name='inverse_log_det_jacobian'
)
</pre> <p>Returns the (log o det o Jacobian o inverse)(y).</p> <p>Mathematically, returns: <code translate="no" dir="ltr">log(det(dX/dY))(Y)</code>. (Recall that: <code translate="no" dir="ltr">X=g^{-1}(Y)</code>.)</p> <p>Note that <code translate="no" dir="ltr">forward_log_det_jacobian</code> is the negative of this function, evaluated at <code translate="no" dir="ltr">g^{-1}(y)</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">y</code> </td> <td> <code translate="no" dir="ltr">Tensor</code>. The input to the "inverse" Jacobian determinant evaluation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">event_ndims</code> </td> <td> Number of dimensions in the probabilistic events being transformed. Must be greater than or equal to <code translate="no" dir="ltr">self.inverse_min_event_ndims</code>. The result is summed over the final dimensions to produce a scalar Jacobian determinant for each event, i.e. it has shape <code translate="no" dir="ltr">y.shape.ndims - event_ndims</code> dimensions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name to give this op. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">Tensor</code>, if this bijector is injective. If not injective, returns the tuple of local log det Jacobians, <code translate="no" dir="ltr">log(det(Dg_i^{-1}(y)))</code>, where <code translate="no" dir="ltr">g_i</code> is the restriction of <code translate="no" dir="ltr">g</code> to the <code translate="no" dir="ltr">ith</code> partition <code translate="no" dir="ltr">Di</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">self.dtype</code> is specified and <code translate="no" dir="ltr">y.dtype</code> is not <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> if <code translate="no" dir="ltr">_inverse_log_det_jacobian</code> is not implemented. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
