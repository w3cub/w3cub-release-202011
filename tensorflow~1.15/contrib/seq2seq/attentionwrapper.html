
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.contrib.seq2seq.AttentionWrapper - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" Wraps another RNNCell with attention. ">
  <meta name="keywords" content="tf, contrib, seq, attentionwrapper, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/contrib/seq2seq/attentionwrapper.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.contrib.seq2seq.AttentionWrapper</h1>       <p>Wraps another <code translate="no" dir="ltr">RNNCell</code> with attention.</p> <p>Inherits From: <a href="../../nn/rnn_cell/rnncell"><code translate="no" dir="ltr">RNNCell</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.contrib.seq2seq.AttentionWrapper(
    cell, attention_mechanism, attention_layer_size=None, alignment_history=False,
    cell_input_fn=None, output_attention=True, initial_cell_state=None, name=None,
    attention_layer=None, attention_fn=None, dtype=None
)
</pre>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">cell</code> </td> <td> An instance of <code translate="no" dir="ltr">RNNCell</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_mechanism</code> </td> <td> A list of <code translate="no" dir="ltr">AttentionMechanism</code> instances or a single instance. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_layer_size</code> </td> <td> A list of Python integers or a single Python integer, the depth of the attention (output) layer(s). If None (default), use the context as attention at each time step. Otherwise, feed the context and cell output into the attention layer to generate attention at each time step. If attention_mechanism is a list, attention_layer_size must be a list of the same length. If attention_layer is set, this must be None. If attention_fn is set, it must guaranteed that the outputs of attention_fn also meet the above requirements. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">alignment_history</code> </td> <td> Python boolean, whether to store alignment history from all time steps in the final output state (currently stored as a time major <code translate="no" dir="ltr">TensorArray</code> on which you must call <code translate="no" dir="ltr">stack()</code>). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">cell_input_fn</code> </td> <td> (optional) A <code translate="no" dir="ltr">callable</code>. The default is: <code translate="no" dir="ltr">lambda inputs, attention: array_ops.concat([inputs, attention], -1)</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_attention</code> </td> <td> Python bool. If <code translate="no" dir="ltr">True</code> (default), the output at each time step is the attention value. This is the behavior of Luong-style attention mechanisms. If <code translate="no" dir="ltr">False</code>, the output at each time step is the output of <code translate="no" dir="ltr">cell</code>. This is the behavior of Bhadanau-style attention mechanisms. In both cases, the <code translate="no" dir="ltr">attention</code> tensor is propagated to the next time step via the state and is used there. This flag only controls whether the attention mechanism is propagated up to the next cell in an RNN stack or to the top RNN output. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initial_cell_state</code> </td> <td> The initial state value to use for the cell when the user calls <code translate="no" dir="ltr">zero_state()</code>. Note that if this value is provided now, and the user uses a <code translate="no" dir="ltr">batch_size</code> argument of <code translate="no" dir="ltr">zero_state</code> which does not match the batch size of <code translate="no" dir="ltr">initial_cell_state</code>, proper behavior is not guaranteed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Name to use when creating ops. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_layer</code> </td> <td> A list of <a href="../../layers/layer"><code translate="no" dir="ltr">tf.compat.v1.layers.Layer</code></a> instances or a single <a href="../../layers/layer"><code translate="no" dir="ltr">tf.compat.v1.layers.Layer</code></a> instance taking the context and cell output as inputs to generate attention at each time step. If None (default), use the context as attention at each time step. If attention_mechanism is a list, attention_layer must be a list of the same length. If attention_layers_size is set, this must be None. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_fn</code> </td> <td> An optional callable function that allows users to provide their own customized attention function, which takes input (attention_mechanism, cell_output, attention_state, attention_layer) and outputs (attention, alignments, next_attention_state). If provided, the attention_layer_size should be the size of the outputs of attention_fn. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> The cell dtype </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> <code translate="no" dir="ltr">attention_layer_size</code> is not None and (<code translate="no" dir="ltr">attention_mechanism</code> is a list but <code translate="no" dir="ltr">attention_layer_size</code> is not; or vice versa). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if <code translate="no" dir="ltr">attention_layer_size</code> is not None, <code translate="no" dir="ltr">attention_mechanism</code> is a list, and its length does not match that of <code translate="no" dir="ltr">attention_layer_size</code>; if <code translate="no" dir="ltr">attention_layer_size</code> and <code translate="no" dir="ltr">attention_layer</code> are set simultaneously. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">graph</code> </td> <td> DEPRECATED FUNCTION <aside class="warning"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Stop using this property because tf.layers layers no longer track their graph. </span></aside>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_size</code> </td> <td> Integer or TensorShape: size of outputs produced by this cell. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">scope_name</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">state_size</code> </td> <td> The <code translate="no" dir="ltr">state_size</code> property of <code translate="no" dir="ltr">AttentionWrapper</code>. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="get_initial_state" data-text="get_initial_state" tabindex="0"><code translate="no" dir="ltr">get_initial_state</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/rnn_cell_impl.py#L281-L309">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_initial_state(
    inputs=None, batch_size=None, dtype=None
)
</pre> <h3 id="zero_state" data-text="zero_state" tabindex="0"><code translate="no" dir="ltr">zero_state</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L2389-L2447">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
zero_state(
    batch_size, dtype
)
</pre> <p>Return an initial (zero) state tuple for this <code translate="no" dir="ltr">AttentionWrapper</code>.</p> <blockquote class="note">
<strong>Note:</strong><span> Please see the initializer documentation for details of how to call <code translate="no" dir="ltr">zero_state</code> if using an <code translate="no" dir="ltr">AttentionWrapper</code> with a <code translate="no" dir="ltr">BeamSearchDecoder</code>.</span>
</blockquote>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">batch_size</code> </td> <td> <code translate="no" dir="ltr">0D</code> integer tensor: the batch size. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> The internal state data type. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">AttentionWrapperState</code> tuple containing zeroed out tensors and, possibly, empty <code translate="no" dir="ltr">TensorArray</code> objects. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> (or, possibly at runtime, InvalidArgument), if <code translate="no" dir="ltr">batch_size</code> does not match the output size of the encoder passed to the wrapper object at initialization time. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/seq2seq/AttentionWrapper" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/seq2seq/AttentionWrapper</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
