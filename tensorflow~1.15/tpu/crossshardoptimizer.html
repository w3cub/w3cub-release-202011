
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.tpu.CrossShardOptimizer - TensorFlow 1.15 - W3cubDocs</title>
  
  <meta name="description" content=" An optimizer that averages gradients across TPU shards. ">
  <meta name="keywords" content="tf, tpu, crossshardoptimizer, tensorflow, tensorflow~1.15">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~1.15/tpu/crossshardoptimizer.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~1.15.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~1.15/" class="_nav-link" title="" style="margin-left:0;">TensorFlow 1.15</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 class="devsite-page-title">tf.tpu.CrossShardOptimizer</h1>       <p>An optimizer that averages gradients across TPU shards.</p> <p>Inherits From: <a href="../train/optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="0">View aliases</h4> <p> <b>Main aliases</b> </p>
<p>`tf.contrib.tpu.CrossShardOptimizer`</p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/tpu/CrossShardOptimizer"><code translate="no" dir="ltr">tf.compat.v1.tpu.CrossShardOptimizer</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.tpu.CrossShardOptimizer(
    opt, reduction=losses.Reduction.MEAN, name='CrossShardOptimizer',
    group_assignment=None
)
</pre>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">opt</code> </td> <td> An existing <code translate="no" dir="ltr">Optimizer</code> to encapsulate. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">reduction</code> </td> <td> The reduction to apply to the shard losses. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name prefix for the operations created when applying gradients. Defaults to "CrossShardOptimizer". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">group_assignment</code> </td> <td> Optional 2d int32 lists with shape [num_groups, num_replicas_per_group] which describles how to apply optimizer to subgroups. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If reduction is not a valid cross-shard reduction. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="0">Methods</h2> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="0"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_optimizer.py#L153-L182">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
apply_gradients(
    grads_and_vars, global_step=None, name=None
)
</pre> <p>Apply gradients to variables.</p> <p>Calls tpu_ops.cross_replica_sum() to sum gradient contributions across replicas, and then applies the real optimizer.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads_and_vars</code> </td> <td> List of (gradient, variable) pairs as returned by compute_gradients(). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional Variable to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. Default to the name passed to the Optimizer constructor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that applies the gradients. If <code translate="no" dir="ltr">global_step</code> was not None, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the grads_and_vars is malformed. </td> </tr> </table> <h3 id="compute_gradients" data-text="compute_gradients" tabindex="0"><code translate="no" dir="ltr">compute_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_optimizer.py#L111-L151">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_gradients(
    loss, var_list=None, **kwargs
)
</pre> <p>Compute gradients of "loss" for the variables in "var_list".</p> <p>This simply wraps the compute_gradients() from the real optimizer. The gradients will be aggregated in the apply_gradients() so that user can modify the gradients like clipping with per replica global norm if needed. The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A Tensor containing the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> Optional list or tuple of <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a> to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKey.TRAINABLE_VARIABLES</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Keyword arguments for compute_gradients(). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of (gradient, variable) pairs. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If not within a tpu_shard_context or group_assignment is invalid. </td> </tr> </table> <h3 id="get_name" data-text="get_name" tabindex="0"><code translate="no" dir="ltr">get_name</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/optimizer.py#L352-L353">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_name()
</pre> <h3 id="get_slot" data-text="get_slot" tabindex="0"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_optimizer.py#L184-L196">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot(
    *args, **kwargs
)
</pre> <p>Return a slot named "name" created for "var" by the Optimizer.</p> <p>This simply wraps the get_slot() from the actual optimizer.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">*args</code> </td> <td> Arguments for get_slot(). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Keyword arguments for get_slot(). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The <code translate="no" dir="ltr">Variable</code> for the slot if it was created, <code translate="no" dir="ltr">None</code> otherwise. </td> </tr> 
</table> <h3 id="get_slot_names" data-text="get_slot_names" tabindex="0"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_optimizer.py#L198-L210">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot_names(
    *args, **kwargs
)
</pre> <p>Return a list of the names of slots created by the <code translate="no" dir="ltr">Optimizer</code>.</p> <p>This simply wraps the get_slot_names() from the actual optimizer.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">*args</code> </td> <td> Arguments for get_slot(). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Keyword arguments for get_slot(). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of strings. </td> </tr> 
</table> <h3 id="minimize" data-text="minimize" tabindex="0"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/optimizer.py#L355-L413">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
minimize(
    loss, global_step=None, var_list=None, gate_gradients=GATE_OP,
    aggregation_method=None, colocate_gradients_with_ops=False, name=None,
    grad_loss=None
)
</pre> <p>Add operations to minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply combines calls <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying them call <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> containing the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> Optional list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <a href="../graphkeys#TRAINABLE_VARIABLES"><code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gate_gradients</code> </td> <td> How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation_method</code> </td> <td> Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">colocate_gradients_with_ops</code> </td> <td> If True, try colocating gradients with the corresponding op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_loss</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An Operation that updates the variables in <code translate="no" dir="ltr">var_list</code>. If <code translate="no" dir="ltr">global_step</code> was not <code translate="no" dir="ltr">None</code>, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects. </td> </tr> </table> <h4 id="eager_compatibility" data-text="Eager Compatibility" tabindex="0">Eager Compatibility</h4> <p>When eager execution is enabled, <code translate="no" dir="ltr">loss</code> should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of <code translate="no" dir="ltr">var_list</code> if not None, else with respect to any trainable variables created during the execution of the <code translate="no" dir="ltr">loss</code> function. <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, <code translate="no" dir="ltr">colocate_gradients_with_ops</code> and <code translate="no" dir="ltr">grad_loss</code> are ignored when eager execution is enabled.</p> <h3 id="variables" data-text="variables" tabindex="0"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/tpu/tpu_optimizer.py#L212-L214">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variables()
</pre> <p>Forwarding the variables from the underlying optimizer.</p> <h2 id="class_variables" data-text="Class Variables" tabindex="0">Class Variables</h2> <ul> <li>
<code translate="no" dir="ltr">GATE_GRAPH = 2</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_NONE = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_OP = 1</code> 
</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    Â© 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/tpu/CrossShardOptimizer" class="_attribution-link">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/tpu/CrossShardOptimizer</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
