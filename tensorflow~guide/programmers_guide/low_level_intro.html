
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>Introduction - TensorFlow Guide - W3cubDocs</title>
  
  <meta name="description" content=" This guide gets you started programming in the low-level TensorFlow APIs (TensorFlow Core), showing you how to&#58; ">
  <meta name="keywords" content="introduction, tensorflow, guide, tensorflow~guide">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~guide/programmers_guide/low_level_intro.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-01fda2ddb8339756caccf7add5ad4cf849ab52d069bd799015c7f04f93164f64753bff0d15a49d8060b1e66e41002bb301ccadc2350937df079cea3cd52d3cca.css">
  <script src="/assets/application-d9be6f56a823612443fc15b2e027a630e02c4ad2685bb750d13fa4fae28d46c3e7f7ebb69bd4bafddf116f218f9372e9be44021d4247dc20424e2fd1ff8cef81.js" type="text/javascript"></script>
  <script src="/json/tensorflow~guide.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~guide/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Guide</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> Introduction </h1>     <p>This guide gets you started programming in the low-level TensorFlow APIs (TensorFlow Core), showing you how to:</p> <ul> <li>Manage your own TensorFlow program (a <a href="https://www.tensorflow.org/api_docs/python/tf/Graph"><code>tf.Graph</code></a>) and TensorFlow runtime (a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code>tf.Session</code></a>), instead of relying on Estimators to manage them.</li> <li>Run TensorFlow operations, using a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code>tf.Session</code></a>.</li> <li>Use high level components (<a href="#datasets">datasets</a>, <a href="#layers">layers</a>, and <a href="#feature_columns">feature_columns</a>) in this low level environment.</li> <li>Build your own training loop, instead of using the one <a href="../get_started/premade_estimators">provided by Estimators</a>.</li> </ul> <p>We recommend using the higher level APIs to build models when possible. Knowing TensorFlow Core is valuable for the following reasons:</p> <ul> <li>Experimentation and debugging are both more straight forward when you can use low level TensorFlow operations directly.</li> <li>It gives you a mental model of how things work internally when using the higher level APIs.</li> </ul> <h2 id="setup">Setup</h2> <p>Before using this guide, <a href="https://www.tensorflow.org/install/index">install TensorFlow</a>.</p> <p>To get the most out of this guide, you should know the following:</p> <ul> <li>How to program in Python.</li> <li>At least a little bit about arrays.</li> <li>Ideally, something about machine learning.</li> </ul> <p>Feel free to launch <code>python</code> and follow along with this walkthrough. Run the following lines to set up your Python environment:</p> <pre class="prettyprint lang-python" data-language="python">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf
</pre> <h2 id="tensor_values">Tensor Values</h2> <p>The central unit of data in TensorFlow is the <strong>tensor</strong>. A tensor consists of a set of primitive values shaped into an array of any number of dimensions. A tensor's <strong>rank</strong> is its number of dimensions, while its <strong>shape</strong> is a tuple of integers specifying the array's length along each dimension. Here are some examples of tensor values:</p> <pre class="prettyprint lang-python" data-language="python">3. # a rank 0 tensor; a scalar with shape [],
[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]
[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]
[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]
</pre> <p>TensorFlow uses numpy arrays to represent tensor <strong>values</strong>.</p> <h2 id="tensorflow_core_walkthrough">TensorFlow Core Walkthrough</h2> <p>You might think of TensorFlow Core programs as consisting of two discrete sections:</p> <ol> <li>Building the computational graph (a <a href="https://www.tensorflow.org/api_docs/python/tf/Graph"><code>tf.Graph</code></a>).</li> <li>Running the computational graph (using a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code>tf.Session</code></a>).</li> </ol> <h3 id="graph">Graph</h3> <p>A <strong>computational graph</strong> is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.</p> <ul> <li>
<a href="https://www.tensorflow.org/api_docs/python/tf/Operation">Operations</a> (or "ops"): The nodes of the graph. Operations describe calculations that consume and produce tensors.</li> <li>
<a href="https://www.tensorflow.org/api_docs/python/tf/Tensor">Tensors</a>: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return <code>tf.Tensors</code>.</li> </ul> <aside class="special"><strong>Important:</strong><span> <code>tf.Tensors</code> do not have values, they are just handles to elements in the computation graph.</span></aside> <p>Let's build a simple computational graph. The most basic operation is a constant. The Python function that builds the operation takes a tensor value as input. The resulting operation takes no inputs. When run, it outputs the value that was passed to the constructor. We can create two floating point constants <code>a</code> and <code>b</code> as follows:</p> <pre class="prettyprint lang-python" data-language="python">a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)
</pre> <p>The print statements produce:</p> <pre class="prettyprint" data-language="cpp">Tensor("Const:0", shape=(), dtype=float32)
Tensor("Const_1:0", shape=(), dtype=float32)
Tensor("add:0", shape=(), dtype=float32)
</pre> <p>Notice that printing the tensors does not output the values <code>3.0</code>, <code>4.0</code>, and <code>7.0</code> as you might expect. The above statements only build the computation graph. These <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> objects just represent the results of the operations that will be run.</p> <p>Each operation in a graph is given a unique name. This name is independent of the names the objects are assigned to in Python. Tensors are named after the operation that produces them followed by an output index, as in <code>"add:0"</code> above.</p> <h3 id="tensorboard">TensorBoard</h3> <p>TensorFlow provides a utility called TensorBoard. One of TensorBoard's many capabilities is visualizing a computation graph. You can easily do this with a few simple commands.</p> <p>First you save the computation graph to a TensorBoard summary file as follows:</p> <pre class="prettyprint" data-language="cpp">writer = tf.summary.FileWriter('.')
writer.add_graph(tf.get_default_graph())
</pre> <p>This will produce an <code>event</code> file in the current directory with a name in the following format:</p> <pre class="prettyprint" data-language="cpp">events.out.tfevents.{timestamp}.{hostname}
</pre> <p>Now, in a new terminal, launch TensorBoard with the following shell command:</p> <pre class="prettyprint lang-bsh" data-language="cpp">tensorboard --logdir .
</pre> <p>Then open TensorBoard's <a href="http://localhost:6006/#graphs">graphs page</a> in your browser, and you should see a graph similar to the following:</p> <p><img alt="TensorBoard screenshot" src="data:image/png;base64,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"></p> <p>For more about TensorBoard's graph visualization tools see <a href="graph_viz">TensorBoard: Graph Visualization</a>.</p> <h3 id="session">Session</h3> <p>To evaluate tensors, instantiate a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code>tf.Session</code></a> object, informally known as a <strong>session</strong>. A session encapsulates the state of the TensorFlow runtime, and runs TensorFlow operations. If a <a href="https://www.tensorflow.org/api_docs/python/tf/Graph"><code>tf.Graph</code></a> is like a <code>.py</code> file, a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code>tf.Session</code></a> is like the <code>python</code> executable.</p> <p>The following code creates a <a href="https://www.tensorflow.org/api_docs/python/tf/Session"><code>tf.Session</code></a> object and then invokes its <code>run</code> method to evaluate the <code>total</code> tensor we created above:</p> <pre class="prettyprint lang-python" data-language="python">sess = tf.Session()
print(sess.run(total))
</pre> <p>When you request the output of a node with <code>Session.run</code> TensorFlow backtracks through the graph and runs all the nodes that provide input to the requested output node. So this prints the expected value of 7.0:</p> <pre class="prettyprint" data-language="cpp">7.0
</pre> <p>You can pass multiple tensors to <a href="https://www.tensorflow.org/api_docs/python/tf/InteractiveSession#run"><code>tf.Session.run</code></a>. The <code>run</code> method transparently handles any combination of tuples or dictionaries, as in the following example:</p> <pre class="prettyprint lang-python" data-language="python">print(sess.run({'ab':(a, b), 'total':total}))
</pre> <p>which returns the results in a structure of the same layout:</p> <pre class="prettyprint lang-None" data-language="cpp">{'total': 7.0, 'ab': (3.0, 4.0)}
</pre> <p>During a call to <a href="https://www.tensorflow.org/api_docs/python/tf/InteractiveSession#run"><code>tf.Session.run</code></a> any <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> only has a single value. For example, the following code calls <a href="https://www.tensorflow.org/api_docs/python/tf/random_uniform"><code>tf.random_uniform</code></a> to produce a <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> that generates a random 3-element vector (with values in <code>[0,1)</code>):</p> <pre class="prettyprint lang-python" data-language="python">vec = tf.random_uniform(shape=(3,))
out1 = vec + 1
out2 = vec + 2
print(sess.run(vec))
print(sess.run(vec))
print(sess.run((out1, out2)))
</pre> <p>The result shows a different random value on each call to <code>run</code>, but a consistent value during a single <code>run</code> (<code>out1</code> and <code>out2</code> receive the same random input):</p> <pre class="prettyprint" data-language="cpp">[ 0.52917576  0.64076328  0.68353939]
[ 0.66192627  0.89126778  0.06254101]
(
  array([ 1.88408756,  1.87149239,  1.84057522], dtype=float32),
  array([ 2.88408756,  2.87149239,  2.84057522], dtype=float32)
)
</pre> <p>Some TensorFlow functions return <code>tf.Operations</code> instead of <code>tf.Tensors</code>. The result of calling <code>run</code> on an Operation is <code>None</code>. You run an operation to cause a side-effect, not to retrieve a value. Examples of this include the <a href="#Initializing%20Layers">initialization</a>, and <a href="#Training">training</a> ops demonstrated later.</p> <h3 id="feeding">Feeding</h3> <p>As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as <strong>placeholders</strong>. A <strong>placeholder</strong> is a promise to provide a value later, like a function argument.</p> <pre class="prettyprint lang-python" data-language="python">x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
z = x + y
</pre> <p>The preceding three lines are a bit like a function in which we define two input parameters (<code>x</code> and <code>y</code>) and then an operation on them. We can evaluate this graph with multiple inputs by using the <code>feed_dict</code> argument of the <a href="https://www.tensorflow.org/api_docs/python/tf/InteractiveSession#run">run method</a> to feed concrete values to the placeholders:</p> <pre class="prettyprint lang-python" data-language="python">print(sess.run(z, feed_dict={x: 3, y: 4.5}))
print(sess.run(z, feed_dict={x: [1, 3], y: [2, 4]}))
</pre> <p>This results in the following output:</p> <pre class="prettyprint" data-language="cpp">7.5
[ 3.  7.]
</pre> <p>Also note that the <code>feed_dict</code> argument can be used to overwrite any tensor in the graph. The only difference between placeholders and other <code>tf.Tensors</code> is that placeholders throw an error if no value is fed to them.</p> <h2 id="datasets">Datasets</h2> <p>Placeholders work for simple experiments, but <a href="https://www.tensorflow.org/api_docs/python/tf/data">Datasets</a> are the preferred method of streaming data into a model.</p> <p>To get a runnable <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> from a Dataset you must first convert it to a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Iterator"><code>tf.data.Iterator</code></a>, and then call the Iterator's <a href="https://www.tensorflow.org/api_docs/python/tf/data/Iterator#get_next"><code>get_next</code></a> method.</p> <p>The simplest way to create an Iterator is with the <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_one_shot_iterator"><code>make_one_shot_iterator</code></a> method. For example, in the following code the <code>next_item</code> tensor will return a row from the <code>my_data</code> array on each <code>run</code> call:</p> <pre class="prettyprint lang-python" data-language="python">my_data = [
    [0, 1,],
    [2, 3,],
    [4, 5,],
    [6, 7,],
]
slices = tf.data.Dataset.from_tensor_slices(my_data)
next_item = slices.make_one_shot_iterator().get_next()
</pre> <p>Reaching the end of the data stream causes <code>Dataset</code> to throw an <a href="https://www.tensorflow.org/api_docs/python/tf/errors/OutOfRangeError"><code>OutOfRangeError</code></a>. For example, the following code reads the <code>next_item</code> until there is no more data to read:</p> <pre class="prettyprint lang-python" data-language="python">while True:
  try:
    print(sess.run(next_item))
  except tf.errors.OutOfRangeError:
    break
</pre> <p>If the <code>Dataset</code> depends on stateful operations you may need to initialize the iterator before using it, as shown below:</p> <pre class="prettyprint lang-python" data-language="python">r = tf.random_normal([10,3])
dataset = tf.data.Dataset.from_tensor_slices(r)
iterator = dataset.make_initializable_iterator()
next_row = iterator.get_next()

sess.run(iterator.initializer)
while True:
  try:
    print(sess.run(next_row))
  except tf.errors.OutOfRangeError:
    break
</pre> <p>For more details on Datasets and Iterators see: <a href="datasets">Importing Data</a>.</p> <h2 id="layers">Layers</h2> <p>A trainable model must modify the values in the graph to get new outputs with the same input. <a href="https://www.tensorflow.org/api_docs/python/tf/layers">Layers</a> are the preferred way to add trainable parameters to a graph.</p> <p>Layers package together both the variables and the operations that act on them. For example a <a href="https://developers.google.com/machine-learning/glossary/#fully_connected_layer">densely-connected layer</a> performs a weighted sum across all inputs for each output and applies an optional <a href="https://developers.google.com/machine-learning/glossary/#activation_function">activation function</a>. The connection weights and biases are managed by the layer object.</p> <h3 id="creating_layers">Creating Layers</h3> <p>The following code creates a <a href="https://www.tensorflow.org/api_docs/python/tf/layers/Dense"><code>Dense</code></a> layer that takes a batch of input vectors, and produces a single output value for each. To apply a layer to an input, call the layer as if it were a function. For example:</p> <pre class="prettyprint lang-python" data-language="python">x = tf.placeholder(tf.float32, shape=[None, 3])
linear_model = tf.layers.Dense(units=1)
y = linear_model(x)
</pre> <p>The layer inspects its input to determine sizes for its internal variables. So here we must set the shape of the <code>x</code> placeholder so that the layer can build a weight matrix of the correct size.</p> <p>Now that we have defined the calculation of the output, <code>y</code>, there is one more detail we need to take care of before we run the calculation.</p> <h3 id="initializing_layers">Initializing Layers</h3> <p>The layer contains variables that must be <strong>initialized</strong> before they can be used. While it is possible to initialize variables individually, you can easily initialize all the variables in a TensorFlow graph as follows:</p> <pre class="prettyprint lang-python" data-language="python">init = tf.global_variables_initializer()
sess.run(init)
</pre> <aside class="special"><strong>Important:</strong><span> Calling <a href="https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer"><code>tf.global_variables_initializer</code></a> only creates and returns a handle to a TensorFlow operation. That op will initialize all the global variables when we run it with <a href="https://www.tensorflow.org/api_docs/python/tf/InteractiveSession#run"><code>tf.Session.run</code></a>.</span></aside> <p>Also note that this <code>global_variables_initializer</code> only initializes variables that existed in the graph when the initializer was created. So the initializer should be one of the last things added during graph construction.</p> <h3 id="executing_layers">Executing Layers</h3> <p>Now that the layer is initialized, we can evaluate the <code>linear_model</code>'s output tensor as we would any other tensor. For example, the following code:</p> <pre class="prettyprint lang-python" data-language="python">print(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}))
</pre> <p>will generate a two-element output vector such as the following:</p> <pre class="prettyprint" data-language="cpp">[[-3.41378999]
 [-9.14999008]]
</pre> <h3 id="layer_function_shortcuts">Layer Function shortcuts</h3> <p>For each layer class (like <a href="https://www.tensorflow.org/api_docs/python/tf/layers/Dense"><code>tf.layers.Dense</code></a>) TensorFlow also supplies a shortcut function (like <a href="https://www.tensorflow.org/api_docs/python/tf/layers/dense"><code>tf.layers.dense</code></a>). The only difference is that the shortcut function versions create and run the layer in a single call. For example, the following code is equivalent to the earlier version:</p> <pre class="prettyprint lang-python" data-language="python">x = tf.placeholder(tf.float32, shape=[None, 3])
y = tf.layers.dense(x, units=1)

init = tf.global_variables_initializer()
sess.run(init)

print(sess.run(y, {x: [[1, 2, 3], [4, 5, 6]]}))
</pre> <p>While convenient, this approach allows no access to the <a href="https://www.tensorflow.org/api_docs/python/tf/layers/Layer"><code>tf.layers.Layer</code></a> object. This makes introspection and debugging more difficult, and layer reuse impossible.</p> <h2 id="feature_columns">Feature columns</h2> <p>The easiest way to experiment with feature columns is using the <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer"><code>tf.feature_column.input_layer</code></a> function. This function only accepts <a href="../get_started/feature_columns">dense columns</a> as inputs, so to view the result of a categorical column you must wrap it in an <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column"><code>tf.feature_column.indicator_column</code></a>. For example:</p> <pre class="prettyprint lang-python" data-language="python">features = {
    'sales' : [[5], [10], [8], [9]],
    'department': ['sports', 'sports', 'gardening', 'gardening']}

department_column = tf.feature_column.categorical_column_with_vocabulary_list(
        'department', ['sports', 'gardening'])
department_column = tf.feature_column.indicator_column(department_column)

columns = [
    tf.feature_column.numeric_column('sales'),
    department_column
]

inputs = tf.feature_column.input_layer(features, columns)
</pre> <p>Running the <code>inputs</code> tensor will parse the <code>features</code> into a batch of vectors.</p> <p>Feature columns can have internal state, like layers, so they often need to be initialized. Categorical columns use <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/lookup">lookup tables</a> internally and these require a separate initialization op, <a href="https://www.tensorflow.org/api_docs/python/tf/tables_initializer"><code>tf.tables_initializer</code></a>.</p> <pre class="prettyprint lang-python" data-language="python">var_init = tf.global_variables_initializer()
table_init = tf.tables_initializer()
sess = tf.Session()
sess.run((var_init, table_init))
</pre> <p>Once the internal state has been initialized you can run <code>inputs</code> like any other <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>:</p> <pre class="prettyprint lang-python" data-language="python">print(sess.run(inputs))
</pre> <p>This shows how the feature columns have packed the input vectors, with the one-hot "department" as the first two indices and "sales" as the third.</p> <pre class="prettyprint lang-None" data-language="cpp">[[  1.   0.   5.]
 [  1.   0.  10.]
 [  0.   1.   8.]
 [  0.   1.   9.]]
</pre> <h2 id="training">Training</h2> <p>Now that you're familiar with the basics of core TensorFlow, let's train a small regression model manually.</p> <h3 id="define_the_data">Define the data</h3> <p>First let's define some inputs, <code>x</code>, and the expected output for each input, <code>y_true</code>:</p> <pre class="prettyprint lang-python" data-language="python">x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)
y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)
</pre> <h3 id="define_the_model">Define the model</h3> <p>Next, build a simple linear model, with 1 output:</p> <pre class="prettyprint lang-python" data-language="python">linear_model = tf.layers.Dense(units=1)

y_pred = linear_model(x)
</pre> <p>You can evaluate the predictions as follows:</p> <pre class="prettyprint lang-python" data-language="python">sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)

print(sess.run(y_pred))
</pre> <p>The model hasn't yet been trained, so the four "predicted" values aren't very good. Here's what we got; your own output will almost certainly differ:</p> <pre class="prettyprint lang-None" data-language="cpp">[[ 0.02631879]
 [ 0.05263758]
 [ 0.07895637]
 [ 0.10527515]]
</pre> <h3 id="loss">Loss</h3> <p>To optimize a model, you first need to define the loss. We'll use the mean square error, a standard loss for regression problems.</p> <p>While you could do this manually with lower level math operations, the <a href="https://www.tensorflow.org/api_docs/python/tf/losses"><code>tf.losses</code></a> module provides a set of common loss functions. You can use it to calculate the mean square error as follows:</p> <pre class="prettyprint lang-python" data-language="python">loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)

print(sess.run(loss))
</pre> <p>This will produce a loss value, something like:</p> <pre class="prettyprint lang-None" data-language="cpp">2.23962
</pre> <h3 id="training_1">Training</h3> <p>TensorFlow provides <a href="https://developers.google.com/machine-learning/glossary/#optimizer"><strong>optimizers</strong></a> implementing standard optimization algorithms. These are implemented as sub-classes of <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"><code>tf.train.Optimizer</code></a>. They incrementally change each variable in order to minimize the loss. The simplest optimization algorithm is <a href="https://developers.google.com/machine-learning/glossary/#gradient_descent"><strong>gradient descent</strong></a>, implemented by <a href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"><code>tf.train.GradientDescentOptimizer</code></a>. It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. For example:</p> <pre class="prettyprint lang-python" data-language="python">optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
</pre> <p>This code builds all the graph components necessary for the optimization, and returns a training operation. When run, the training op will update variables in the graph. You might run it as follows:</p> <pre class="prettyprint lang-python" data-language="python">for i in range(100):
  _, loss_value = sess.run((train, loss))
  print(loss_value)
</pre> <p>Since <code>train</code> is an op, not a tensor, it doesn't return a value when run. To see the progression of the loss during training, we run the loss tensor at the same time, producing output like the following:</p> <pre class="prettyprint lang-None" data-language="cpp">1.35659
1.00412
0.759167
0.588829
0.470264
0.387626
0.329918
0.289511
0.261112
0.241046
...
</pre> <h3 id="complete_program">Complete program</h3> <pre class="prettyprint lang-python" data-language="python">x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)
y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)

linear_model = tf.layers.Dense(units=1)

y_pred = linear_model(x)
loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)

optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)
for i in range(100):
  _, loss_value = sess.run((train, loss))
  print(loss_value)

print(sess.run(y_pred))
</pre> <h2 id="next_steps">Next steps</h2> <p>To learn more about building models with TensorFlow consider the following:</p> <ul> <li>
<a href="../get_started/custom_estimators">Custom Estimators</a>, to learn how to build customized models with TensorFlow. Your knowledge of TensorFlow Core will help you understand and debug your own models.</li> </ul> <p>If you want to learn more about the inner workings of TensorFlow consider the following documents, which go into more depth on many of the topics discussed here:</p> <ul> <li><a href="graphs">Graphs and Sessions</a></li> <li><a href="tensors">Tensors</a></li> <li><a href="variables">Variables</a></li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/programmers_guide/low_level_intro" class="_attribution-link">https://www.tensorflow.org/programmers_guide/low_level_intro</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
