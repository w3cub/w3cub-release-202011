
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.linalg.LinearOperatorIdentity - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;python&#47;ops&#47;linalg&#47;linear_operator_identity.py. ">
  <meta name="keywords" content="tf, linalg, linearoperatoridentity, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tf/linalg/linearoperatoridentity.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tf.linalg.LinearOperatorIdentity </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.linalg.LinearOperatorIdentity"> <meta itemprop="path" content="r1.8"> <meta itemprop="property" content="batch_shape"> <meta itemprop="property" content="domain_dimension"> <meta itemprop="property" content="dtype"> <meta itemprop="property" content="graph_parents"> <meta itemprop="property" content="is_non_singular"> <meta itemprop="property" content="is_positive_definite"> <meta itemprop="property" content="is_self_adjoint"> <meta itemprop="property" content="is_square"> <meta itemprop="property" content="name"> <meta itemprop="property" content="range_dimension"> <meta itemprop="property" content="shape"> <meta itemprop="property" content="tensor_rank"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="add_to_tensor"> <meta itemprop="property" content="assert_non_singular"> <meta itemprop="property" content="assert_positive_definite"> <meta itemprop="property" content="assert_self_adjoint"> <meta itemprop="property" content="batch_shape_tensor"> <meta itemprop="property" content="determinant"> <meta itemprop="property" content="diag_part"> <meta itemprop="property" content="domain_dimension_tensor"> <meta itemprop="property" content="log_abs_determinant"> <meta itemprop="property" content="matmul"> <meta itemprop="property" content="matvec"> <meta itemprop="property" content="range_dimension_tensor"> <meta itemprop="property" content="shape_tensor"> <meta itemprop="property" content="solve"> <meta itemprop="property" content="solvevec"> <meta itemprop="property" content="tensor_rank_tensor"> <meta itemprop="property" content="to_dense"> <meta itemprop="property" content="trace"> </div> <h2 id="class_linearoperatoridentity">Class <code>LinearOperatorIdentity</code>
</h2> <h3 id="aliases">Aliases:</h3> <ul> <li>Class <code>tf.contrib.linalg.LinearOperatorIdentity</code>
</li> <li>Class <code>tf.linalg.LinearOperatorIdentity</code>
</li> </ul> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/linalg/linear_operator_identity.py"><code>tensorflow/python/ops/linalg/linear_operator_identity.py</code></a>.</p> <p>See the guide: <a href="https://www.tensorflow.org/api_guides/python/contrib.linalg#_LinearOperator_">Linear Algebra (contrib) &gt; <code>LinearOperator</code></a></p> <p><code>LinearOperator</code> acting like a [batch] square identity matrix.</p> <p>This operator acts like a [batch] identity matrix <code>A</code> with shape <code>[B1,...,Bb, N, N]</code> for some <code>b &gt;= 0</code>. The first <code>b</code> indices index a batch member. For every batch index <code>(i1,...,ib)</code>, <code>A[i1,...,ib, : :]</code> is an <code>N x N</code> matrix. This matrix <code>A</code> is not materialized, but for purposes of broadcasting this shape will be relevant.</p> <p><code>LinearOperatorIdentity</code> is initialized with <code>num_rows</code>, and optionally <code>batch_shape</code>, and <code>dtype</code> arguments. If <code>batch_shape</code> is <code>None</code>, this operator efficiently passes through all arguments. If <code>batch_shape</code> is provided, broadcasting may occur, which will require making copies.</p> <pre class="prettyprint lang-python" data-language="python"># Create a 2 x 2 identity matrix.
operator = LinearOperatorIdentity(num_rows=2, dtype=tf.float32)

operator.to_dense()
==&gt; [[1., 0.]
     [0., 1.]]

operator.shape
==&gt; [2, 2]

operator.log_abs_determinant()
==&gt; 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==&gt; Shape [2, 4] Tensor, same as x.

y = tf.random_normal(shape=[3, 2, 4])
# Note that y.shape is compatible with operator.shape because operator.shape
# is broadcast to [3, 2, 2].
# This broadcast does NOT require copying data, since we can infer that y
# will be passed through without changing shape.  We are always able to infer
# this if the operator has no batch_shape.
x = operator.solve(y)
==&gt; Shape [3, 2, 4] Tensor, same as y.

# Create a 2-batch of 2x2 identity matrices
operator = LinearOperatorIdentity(num_rows=2, batch_shape=[2])
operator.to_dense()
==&gt; [[[1., 0.]
      [0., 1.]],
     [[1., 0.]
      [0., 1.]]]

# Here, even though the operator has a batch shape, the input is the same as
# the output, so x can be passed through without a copy.  The operator is able
# to detect that no broadcast is necessary because both x and the operator
# have statically defined shape.
x = ... Shape [2, 2, 3]
operator.matmul(x)
==&gt; Shape [2, 2, 3] Tensor, same as x

# Here the operator and x have different batch_shape, and are broadcast.
# This requires a copy, since the output is different size than the input.
x = ... Shape [1, 2, 3]
operator.matmul(x)
==&gt; Shape [2, 2, 3] Tensor, equal to [x, x]
</pre> <h3 id="shape_compatibility">Shape compatibility</h3> <p>This operator acts on [batch] matrix with compatible shape. <code>x</code> is a batch matrix with compatible shape for <code>matmul</code> and <code>solve</code> if</p> <pre class="prettyprint" data-language="python">operator.shape = [B1,...,Bb] + [N, N],  with b &gt;= 0
x.shape =   [C1,...,Cc] + [N, R],
and [C1,...,Cc] broadcasts with [B1,...,Bb] to [D1,...,Dd]
</pre> <h3 id="performance">Performance</h3> <p>If <code>batch_shape</code> initialization arg is <code>None</code>:</p> <ul> <li>
<code>operator.matmul(x)</code> is <code>O(1)</code>
</li> <li>
<code>operator.solve(x)</code> is <code>O(1)</code>
</li> <li>
<code>operator.determinant()</code> is <code>O(1)</code>
</li> </ul> <p>If <code>batch_shape</code> initialization arg is provided, and static checks cannot rule out the need to broadcast:</p> <ul> <li>
<code>operator.matmul(x)</code> is <code>O(D1*...*Dd*N*R)</code>
</li> <li>
<code>operator.solve(x)</code> is <code>O(D1*...*Dd*N*R)</code>
</li> <li>
<code>operator.determinant()</code> is <code>O(B1*...*Bb)</code>
</li> </ul> <h4 id="matrix_property_hints">Matrix property hints</h4> <p>This <code>LinearOperator</code> is initialized with boolean flags of the form <code>is_X</code>, for <code>X = non_singular, self_adjoint, positive_definite, square</code>. These have the following meaning:</p> <ul> <li>If <code>is_X == True</code>, callers should expect the operator to have the property <code>X</code>. This is a promise that should be fulfilled, but is <em>not</em> a runtime assert. For example, finite floating point precision may result in these promises being violated.</li> <li>If <code>is_X == False</code>, callers should expect the operator to not have <code>X</code>.</li> <li>If <code>is_X == None</code> (the default), callers should have no expectation either way.</li> </ul> <h2 id="properties">Properties</h2> <h3 id="batch_shape"><code>batch_shape</code></h3> <p><code>TensorShape</code> of batch dimensions of this <code>LinearOperator</code>.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>TensorShape([B1,...,Bb])</code>, equivalent to <code>A.get_shape()[:-2]</code></p> <h4 id="returns">Returns:</h4> <p><code>TensorShape</code>, statically determined, may be undefined.</p> <h3 id="domain_dimension"><code>domain_dimension</code></h3> <p>Dimension (in the sense of vector spaces) of the domain of this operator.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>N</code>.</p> <h4 id="returns_1">Returns:</h4> <p><code>Dimension</code> object.</p> <h3 id="dtype"><code>dtype</code></h3> <p>The <code>DType</code> of <code>Tensor</code>s handled by this <code>LinearOperator</code>.</p> <h3 id="graph_parents"><code>graph_parents</code></h3> <p>List of graph dependencies of this <code>LinearOperator</code>.</p> <h3 id="is_non_singular"><code>is_non_singular</code></h3> <h3 id="is_positive_definite"><code>is_positive_definite</code></h3> <h3 id="is_self_adjoint"><code>is_self_adjoint</code></h3> <h3 id="is_square"><code>is_square</code></h3> <p>Return <code>True/False</code> depending on if this operator is square.</p> <h3 id="name"><code>name</code></h3> <p>Name prepended to all ops created by this <code>LinearOperator</code>.</p> <h3 id="range_dimension"><code>range_dimension</code></h3> <p>Dimension (in the sense of vector spaces) of the range of this operator.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>M</code>.</p> <h4 id="returns_2">Returns:</h4> <p><code>Dimension</code> object.</p> <h3 id="shape"><code>shape</code></h3> <p><code>TensorShape</code> of this <code>LinearOperator</code>.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>TensorShape([B1,...,Bb, M, N])</code>, equivalent to <code>A.get_shape()</code>.</p> <h4 id="returns_3">Returns:</h4> <p><code>TensorShape</code>, statically determined, may be undefined.</p> <h3 id="tensor_rank"><code>tensor_rank</code></h3> <p>Rank (in the sense of tensors) of matrix corresponding to this operator.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>b + 2</code>.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_4">Returns:</h4> <p>Python integer, or None if the tensor rank is undefined.</p> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    num_rows,
    batch_shape=None,
    dtype=None,
    is_non_singular=True,
    is_self_adjoint=True,
    is_positive_definite=True,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorIdentity'
)
</pre> <p>Initialize a <code>LinearOperatorIdentity</code>.</p> <p>The <code>LinearOperatorIdentity</code> is initialized with arguments defining <code>dtype</code> and shape.</p> <p>This operator is able to broadcast the leading (batch) dimensions, which sometimes requires copying data. If <code>batch_shape</code> is <code>None</code>, the operator can take arguments of any batch shape without copying. See examples.</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>num_rows</code></b>: Scalar non-negative integer <code>Tensor</code>. Number of rows in the corresponding identity matrix.</li> <li>
<b><code>batch_shape</code></b>: Optional <code>1-D</code> integer <code>Tensor</code>. The shape of the leading dimensions. If <code>None</code>, this operator has no leading dimensions.</li> <li>
<b><code>dtype</code></b>: Data type of the matrix that this operator represents.</li> <li>
<b><code>is_non_singular</code></b>: Expect that this operator is non-singular.</li> <li>
<b><code>is_self_adjoint</code></b>: Expect that this operator is equal to its hermitian transpose.</li> <li>
<b><code>is_positive_definite</code></b>: Expect that this operator is positive definite, meaning the quadratic form <code>x^H A x</code> has positive real part for all nonzero <code>x</code>. Note that we do not require the operator to be self-adjoint to be positive-definite. See: https://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices</li> <li>
<b><code>is_square</code></b>: Expect that this operator acts like square [batch] matrices.</li> <li>
<b><code>assert_proper_shapes</code></b>: Python <code>bool</code>. If <code>False</code>, only perform static checks that initialization and method arguments have proper shape. If <code>True</code>, and static checks are inconclusive, add asserts to the graph.</li> <li>
<b><code>name</code></b>: A name for this <code>LinearOperator</code>
</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If <code>num_rows</code> is determined statically to be non-scalar, or negative.</li> <li>
<b><code>ValueError</code></b>: If <code>batch_shape</code> is determined statically to not be 1-D, or negative.</li> <li>
<b><code>ValueError</code></b>: If any of the following is not <code>True</code>: <code>{is_self_adjoint, is_non_singular, is_positive_definite}</code>.</li> </ul> <h3 id="add_to_tensor"><code>add_to_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">add_to_tensor(
    mat,
    name='add_to_tensor'
)
</pre> <p>Add matrix represented by this operator to <code>mat</code>. Equiv to <code>I + mat</code>.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>mat</code></b>: <code>Tensor</code> with same <code>dtype</code> and shape broadcastable to <code>self</code>.</li> <li>
<b><code>name</code></b>: A name to give this <code>Op</code>.</li> </ul> <h4 id="returns_5">Returns:</h4> <p>A <code>Tensor</code> with broadcast shape and same <code>dtype</code> as <code>self</code>.</p> <h3 id="assert_non_singular"><code>assert_non_singular</code></h3> <pre class="prettyprint lang-python" data-language="python">assert_non_singular(name='assert_non_singular')
</pre> <p>Returns an <code>Op</code> that asserts this operator is non singular.</p> <p>This operator is considered non-singular if</p> <pre class="prettyprint" data-language="python">ConditionNumber &lt; max{100, range_dimension, domain_dimension} * eps,
eps := np.finfo(self.dtype.as_numpy_dtype).eps
</pre> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>name</code></b>: A string name to prepend to created ops.</li> </ul> <h4 id="returns_6">Returns:</h4> <p>An <code>Assert</code> <code>Op</code>, that, when run, will raise an <code>InvalidArgumentError</code> if the operator is singular.</p> <h3 id="assert_positive_definite"><code>assert_positive_definite</code></h3> <pre class="prettyprint lang-python" data-language="python">assert_positive_definite(name='assert_positive_definite')
</pre> <p>Returns an <code>Op</code> that asserts this operator is positive definite.</p> <p>Here, positive definite means that the quadratic form <code>x^H A x</code> has positive real part for all nonzero <code>x</code>. Note that we do not require the operator to be self-adjoint to be positive definite.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name to give this <code>Op</code>.</li> </ul> <h4 id="returns_7">Returns:</h4> <p>An <code>Assert</code> <code>Op</code>, that, when run, will raise an <code>InvalidArgumentError</code> if the operator is not positive definite.</p> <h3 id="assert_self_adjoint"><code>assert_self_adjoint</code></h3> <pre class="prettyprint lang-python" data-language="python">assert_self_adjoint(name='assert_self_adjoint')
</pre> <p>Returns an <code>Op</code> that asserts this operator is self-adjoint.</p> <p>Here we check that this operator is <em>exactly</em> equal to its hermitian transpose.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>name</code></b>: A string name to prepend to created ops.</li> </ul> <h4 id="returns_8">Returns:</h4> <p>An <code>Assert</code> <code>Op</code>, that, when run, will raise an <code>InvalidArgumentError</code> if the operator is not self-adjoint.</p> <h3 id="batch_shape_tensor"><code>batch_shape_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">batch_shape_tensor(name='batch_shape_tensor')
</pre> <p>Shape of batch dimensions of this operator, determined at runtime.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns a <code>Tensor</code> holding <code>[B1,...,Bb]</code>.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_9">Returns:</h4> <p><code>int32</code> <code>Tensor</code></p> <h3 id="determinant"><code>determinant</code></h3> <pre class="prettyprint lang-python" data-language="python">determinant(name='det')
</pre> <p>Determinant for every batch member.</p> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_10">Returns:</h4> <p><code>Tensor</code> with shape <code>self.batch_shape</code> and same <code>dtype</code> as <code>self</code>.</p> <h4 id="raises_1">Raises:</h4> <ul> <li>
<b><code>NotImplementedError</code></b>: If <code>self.is_square</code> is <code>False</code>.</li> </ul> <h3 id="diag_part"><code>diag_part</code></h3> <pre class="prettyprint lang-python" data-language="python">diag_part(name='diag_part')
</pre> <p>Efficiently get the [batch] diagonal part of this operator.</p> <p>If this operator has shape <code>[B1,...,Bb, M, N]</code>, this returns a <code>Tensor</code> <code>diagonal</code>, of shape <code>[B1,...,Bb, min(M, N)]</code>, where <code>diagonal[b1,...,bb, i] = self.to_dense()[b1,...,bb, i, i]</code>.</p> <pre class="prettyprint" data-language="python">my_operator = LinearOperatorDiag([1., 2.])

# Efficiently get the diagonal
my_operator.diag_part()
==&gt; [1., 2.]

# Equivalent, but inefficient method
tf.matrix_diag_part(my_operator.to_dense())
==&gt; [1., 2.]
</pre> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this <code>Op</code>.</li> </ul> <h4 id="returns_11">Returns:</h4> <ul> <li>
<b><code>diag_part</code></b>: A <code>Tensor</code> of same <code>dtype</code> as self.</li> </ul> <h3 id="domain_dimension_tensor"><code>domain_dimension_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">domain_dimension_tensor(name='domain_dimension_tensor')
</pre> <p>Dimension (in the sense of vector spaces) of the domain of this operator.</p> <p>Determined at runtime.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>N</code>.</p> <h4 id="args_9">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this <code>Op</code>.</li> </ul> <h4 id="returns_12">Returns:</h4> <p><code>int32</code> <code>Tensor</code></p> <h3 id="log_abs_determinant"><code>log_abs_determinant</code></h3> <pre class="prettyprint lang-python" data-language="python">log_abs_determinant(name='log_abs_det')
</pre> <p>Log absolute value of determinant for every batch member.</p> <h4 id="args_10">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_13">Returns:</h4> <p><code>Tensor</code> with shape <code>self.batch_shape</code> and same <code>dtype</code> as <code>self</code>.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code>NotImplementedError</code></b>: If <code>self.is_square</code> is <code>False</code>.</li> </ul> <h3 id="matmul"><code>matmul</code></h3> <pre class="prettyprint lang-python" data-language="python">matmul(
    x,
    adjoint=False,
    adjoint_arg=False,
    name='matmul'
)
</pre> <p>Transform [batch] matrix <code>x</code> with left multiplication: <code>x --&gt; Ax</code>.</p> <pre class="prettyprint lang-python" data-language="python"># Make an operator acting like batch matrix A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)
operator.shape = [..., M, N]

X = ... # shape [..., N, R], batch matrix, R &gt; 0.

Y = operator.matmul(X)
Y.shape
==&gt; [..., M, R]

Y[..., :, r] = sum_j A[..., :, j] X[j, r]
</pre> <h4 id="args_11">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code> with compatible shape and same <code>dtype</code> as <code>self</code>. See class docstring for definition of compatibility.</li> <li>
<b><code>adjoint</code></b>: Python <code>bool</code>. If <code>True</code>, left multiply by the adjoint: <code>A^H x</code>.</li> <li>
<b><code>adjoint_arg</code></b>: Python <code>bool</code>. If <code>True</code>, compute <code>A x^H</code> where <code>x^H</code> is the hermitian transpose (transposition and complex conjugation).</li> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_14">Returns:</h4> <p>A <code>Tensor</code> with shape <code>[..., M, R]</code> and same <code>dtype</code> as <code>self</code>.</p> <h3 id="matvec"><code>matvec</code></h3> <pre class="prettyprint lang-python" data-language="python">matvec(
    x,
    adjoint=False,
    name='matvec'
)
</pre> <p>Transform [batch] vector <code>x</code> with left multiplication: <code>x --&gt; Ax</code>.</p> <pre class="prettyprint lang-python" data-language="python"># Make an operator acting like batch matric A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)

X = ... # shape [..., N], batch vector

Y = operator.matvec(X)
Y.shape
==&gt; [..., M]

Y[..., :] = sum_j A[..., :, j] X[..., j]
</pre> <h4 id="args_12">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code> with compatible shape and same <code>dtype</code> as <code>self</code>. <code>x</code> is treated as a [batch] vector meaning for every set of leading dimensions, the last dimension defines a vector. See class docstring for definition of compatibility.</li> <li>
<b><code>adjoint</code></b>: Python <code>bool</code>. If <code>True</code>, left multiply by the adjoint: <code>A^H x</code>.</li> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_15">Returns:</h4> <p>A <code>Tensor</code> with shape <code>[..., M]</code> and same <code>dtype</code> as <code>self</code>.</p> <h3 id="range_dimension_tensor"><code>range_dimension_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">range_dimension_tensor(name='range_dimension_tensor')
</pre> <p>Dimension (in the sense of vector spaces) of the range of this operator.</p> <p>Determined at runtime.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>M</code>.</p> <h4 id="args_13">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this <code>Op</code>.</li> </ul> <h4 id="returns_16">Returns:</h4> <p><code>int32</code> <code>Tensor</code></p> <h3 id="shape_tensor"><code>shape_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">shape_tensor(name='shape_tensor')
</pre> <p>Shape of this <code>LinearOperator</code>, determined at runtime.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns a <code>Tensor</code> holding <code>[B1,...,Bb, M, N]</code>, equivalent to <code>tf.shape(A)</code>.</p> <h4 id="args_14">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_17">Returns:</h4> <p><code>int32</code> <code>Tensor</code></p> <h3 id="solve"><code>solve</code></h3> <pre class="prettyprint lang-python" data-language="python">solve(
    rhs,
    adjoint=False,
    adjoint_arg=False,
    name='solve'
)
</pre> <p>Solve (exact or approx) <code>R</code> (batch) systems of equations: <code>A X = rhs</code>.</p> <p>The returned <code>Tensor</code> will be close to an exact solution if <code>A</code> is well conditioned. Otherwise closeness will vary. See class docstring for details.</p> <p>Examples:</p> <pre class="prettyprint lang-python" data-language="python"># Make an operator acting like batch matrix A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)
operator.shape = [..., M, N]

# Solve R &gt; 0 linear systems for every member of the batch.
RHS = ... # shape [..., M, R]

X = operator.solve(RHS)
# X[..., :, r] is the solution to the r'th linear system
# sum_j A[..., :, j] X[..., j, r] = RHS[..., :, r]

operator.matmul(X)
==&gt; RHS
</pre> <h4 id="args_15">Args:</h4> <ul> <li>
<b><code>rhs</code></b>: <code>Tensor</code> with same <code>dtype</code> as this operator and compatible shape. <code>rhs</code> is treated like a [batch] matrix meaning for every set of leading dimensions, the last two dimensions defines a matrix. See class docstring for definition of compatibility.</li> <li>
<b><code>adjoint</code></b>: Python <code>bool</code>. If <code>True</code>, solve the system involving the adjoint of this <code>LinearOperator</code>: <code>A^H X = rhs</code>.</li> <li>
<b><code>adjoint_arg</code></b>: Python <code>bool</code>. If <code>True</code>, solve <code>A X = rhs^H</code> where <code>rhs^H</code> is the hermitian transpose (transposition and complex conjugation).</li> <li>
<b><code>name</code></b>: A name scope to use for ops added by this method.</li> </ul> <h4 id="returns_18">Returns:</h4> <p><code>Tensor</code> with shape <code>[...,N, R]</code> and same <code>dtype</code> as <code>rhs</code>.</p> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code>NotImplementedError</code></b>: If <code>self.is_non_singular</code> or <code>is_square</code> is False.</li> </ul> <h3 id="solvevec"><code>solvevec</code></h3> <pre class="prettyprint lang-python" data-language="python">solvevec(
    rhs,
    adjoint=False,
    name='solve'
)
</pre> <p>Solve single equation with best effort: <code>A X = rhs</code>.</p> <p>The returned <code>Tensor</code> will be close to an exact solution if <code>A</code> is well conditioned. Otherwise closeness will vary. See class docstring for details.</p> <p>Examples:</p> <pre class="prettyprint lang-python" data-language="python"># Make an operator acting like batch matrix A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)
operator.shape = [..., M, N]

# Solve one linear system for every member of the batch.
RHS = ... # shape [..., M]

X = operator.solvevec(RHS)
# X is the solution to the linear system
# sum_j A[..., :, j] X[..., j] = RHS[..., :]

operator.matvec(X)
==&gt; RHS
</pre> <h4 id="args_16">Args:</h4> <ul> <li>
<b><code>rhs</code></b>: <code>Tensor</code> with same <code>dtype</code> as this operator. <code>rhs</code> is treated like a [batch] vector meaning for every set of leading dimensions, the last dimension defines a vector. See class docstring for definition of compatibility regarding batch dimensions.</li> <li>
<b><code>adjoint</code></b>: Python <code>bool</code>. If <code>True</code>, solve the system involving the adjoint of this <code>LinearOperator</code>: <code>A^H X = rhs</code>.</li> <li>
<b><code>name</code></b>: A name scope to use for ops added by this method.</li> </ul> <h4 id="returns_19">Returns:</h4> <p><code>Tensor</code> with shape <code>[...,N]</code> and same <code>dtype</code> as <code>rhs</code>.</p> <h4 id="raises_4">Raises:</h4> <ul> <li>
<b><code>NotImplementedError</code></b>: If <code>self.is_non_singular</code> or <code>is_square</code> is False.</li> </ul> <h3 id="tensor_rank_tensor"><code>tensor_rank_tensor</code></h3> <pre class="prettyprint lang-python" data-language="python">tensor_rank_tensor(name='tensor_rank_tensor')
</pre> <p>Rank (in the sense of tensors) of matrix corresponding to this operator.</p> <p>If this operator acts like the batch matrix <code>A</code> with <code>A.shape = [B1,...,Bb, M, N]</code>, then this returns <code>b + 2</code>.</p> <h4 id="args_17">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this `Op.</li> </ul> <h4 id="returns_20">Returns:</h4> <p><code>int32</code> <code>Tensor</code>, determined at runtime.</p> <h3 id="to_dense"><code>to_dense</code></h3> <pre class="prettyprint lang-python" data-language="python">to_dense(name='to_dense')
</pre> <p>Return a dense (batch) matrix representing this operator.</p> <h3 id="trace"><code>trace</code></h3> <pre class="prettyprint lang-python" data-language="python">trace(name='trace')
</pre> <p>Trace of the linear operator, equal to sum of <code>self.diag_part()</code>.</p> <p>If the operator is square, this is also the sum of the eigenvalues.</p> <h4 id="args_18">Args:</h4> <ul> <li>
<b><code>name</code></b>: A name for this <code>Op</code>.</li> </ul> <h4 id="returns_21">Returns:</h4> <p>Shape <code>[B1,...,Bb]</code> <code>Tensor</code> of same <code>dtype</code> as <code>self</code>.</p>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorIdentity" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorIdentity</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
