
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>contrib.timeseries.ARModel - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;contrib&#47;timeseries&#47;python&#47;timeseries&#47;ar_model.py. ">
  <meta name="keywords" content="tf, contrib, timeseries, armodel, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tf/contrib/timeseries/armodel.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e4ebd3a2a5652ff55173659804c4390a004917f3bdd17b5bb3ba78ea5c9c46fe181cadaac34517ccd815f5bdc982bbfe67179d6f4ac2f084ef2265e2a3dc8dc5.css" integrity="sha512-5OvToqVlL/VRc2WYBMQ5CgBJF/O90Xtbs7p46lycRv4YHK2qw0UXzNgV9b3Jgrv+Zxedb0rC8ITvImXio9yNxQ==" crossorigin="anonymous">
  <script type="text/javascript" integrity="sha512-EpkDeu98lN/jPKijllzVWdRg/dUSSMCaldYZNFz6bcNoBvpWRNz0HSTRQJ3ENmQc5Cuj1zDW1vHd7b0DzpOgyA==" crossorigin="anonymous" src="/assets/application-1299037aef7c94dfe33ca8a3965cd559d460fdd51248c09a95d619345cfa6dc36806fa5644dcf41d24d1409dc436641ce42ba3d730d6d6f1ddedbd03ce93a0c8.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script data-ad-client="ca-pub-2572770204602497" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body>
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="link"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.timeseries.ARModel </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.timeseries.ARModel"> <meta itemprop="path" content="r1.8"> <meta itemprop="property" content="exogenous_feature_columns"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="define_loss"> <meta itemprop="property" content="generate"> <meta itemprop="property" content="get_batch_loss"> <meta itemprop="property" content="get_start_state"> <meta itemprop="property" content="initialize_graph"> <meta itemprop="property" content="loss_op"> <meta itemprop="property" content="predict"> <meta itemprop="property" content="prediction_ops"> <meta itemprop="property" content="random_model_parameters"> <meta itemprop="property" content="NORMAL_LIKELIHOOD_LOSS"> <meta itemprop="property" content="SQUARED_LOSS"> </div> <h2 id="class_armodel">Class <code>ARModel</code>
</h2> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/timeseries/python/timeseries/ar_model.py"><code>tensorflow/contrib/timeseries/python/timeseries/ar_model.py</code></a>.</p> <p>Auto-regressive model, both linear and non-linear.</p> <p>Features to the model include time and values of input_window_size timesteps, and times for output_window_size timesteps. These are passed through zero or more hidden layers, and then fed to a loss function (e.g. squared loss).</p> <p>Note that this class can also be used to regress against time only by setting the input_window_size to zero.</p> <h2 id="properties">Properties</h2> <h3 id="exogenous_feature_columns"><code>exogenous_feature_columns</code></h3> <p><code>tf.feature_colum</code>s for features which are not predicted.</p> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    periodicities,
    input_window_size,
    output_window_size,
    num_features,
    num_time_buckets=10,
    loss=NORMAL_LIKELIHOOD_LOSS,
    hidden_layer_sizes=None
)
</pre> <p>Constructs an auto-regressive model.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>periodicities</code></b>: periodicities of the input data, in the same units as the time feature. Note this can be a single value or a list of values for multiple periodicities.</li> <li>
<b><code>input_window_size</code></b>: Number of past time steps of data to look at when doing the regression.</li> <li>
<b><code>output_window_size</code></b>: Number of future time steps to predict. Note that setting it to &gt; 1 empirically seems to give a better fit.</li> <li>
<b><code>num_features</code></b>: number of input features per time step.</li> <li>
<b><code>num_time_buckets</code></b>: Number of buckets into which to divide (time % periodicity) for generating time based features.</li> <li>
<b><code>loss</code></b>: Loss function to use for training. Currently supported values are SQUARED_LOSS and NORMAL_LIKELIHOOD_LOSS. Note that for NORMAL_LIKELIHOOD_LOSS, we train the covariance term as well. For SQUARED_LOSS, the evaluation loss is reported based on un-scaled observations and predictions, while the training loss is computed on normalized data (if input statistics are available).</li> <li>
<b><code>hidden_layer_sizes</code></b>: list of sizes of hidden layers.</li> </ul> <h3 id="define_loss"><code>define_loss</code></h3> <pre class="prettyprint lang-python" data-language="python">define_loss(
    features,
    mode
)
</pre> <p>Default loss definition with state replicated across a batch.</p> <p>Time series passed to this model have a batch dimension, and each series in a batch can be operated on in parallel. This loss definition assumes that each element of the batch represents an independent sample conditioned on the same initial state (i.e. it is simply replicated across the batch). A batch size of one provides sequential operations on a single time series.</p> <p>More complex processing may operate instead on get_start_state() and get_batch_loss() directly.</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>features</code></b>: A dictionary (such as is produced by a chunker) with at minimum the following key/value pairs (others corresponding to the <code>exogenous_feature_columns</code> argument to <code>__init__</code> may be included representing exogenous regressors):</li> <li>
<b><code>TrainEvalFeatures.TIMES</code></b>: A [batch size x window size] integer Tensor with times for each observation. If there is no artificial chunking, the window size is simply the length of the time series.</li> <li>
<b><code>TrainEvalFeatures.VALUES</code></b>: A [batch size x window size x num features] Tensor with values for each observation.</li> <li>
<b><code>mode</code></b>: The tf.estimator.ModeKeys mode to use (TRAIN, EVAL). For INFER, see predict().</li> </ul> <h4 id="returns">Returns:</h4> <p>A ModelOutputs object.</p> <h3 id="generate"><code>generate</code></h3> <pre class="prettyprint lang-python" data-language="python">generate(
    number_of_series,
    series_length,
    model_parameters=None,
    seed=None
)
</pre> <p>Sample synthetic data from model parameters, with optional substitutions.</p> <p>Returns <code>number_of_series</code> possible sequences of future values, sampled from the generative model with each conditioned on the previous. Samples are based on trained parameters, except for those parameters explicitly overridden in <code>model_parameters</code>.</p> <p>For distributions over future observations, see predict().</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>number_of_series</code></b>: Number of time series to create.</li> <li>
<b><code>series_length</code></b>: Length of each time series.</li> <li>
<b><code>model_parameters</code></b>: A dictionary mapping model parameters to values, which replace trained parameters when generating data.</li> <li>
<b><code>seed</code></b>: If specified, return deterministic time series according to this value.</li> </ul> <h4 id="returns_1">Returns:</h4> <p>A dictionary with keys TrainEvalFeatures.TIMES (mapping to an array with shape [number_of_series, series_length]) and TrainEvalFeatures.VALUES (mapping to an array with shape [number_of_series, series_length, num_features]).</p> <h3 id="get_batch_loss"><code>get_batch_loss</code></h3> <pre class="prettyprint lang-python" data-language="python">get_batch_loss(
    features,
    mode,
    state
)
</pre> <p>Computes predictions and a loss.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>features</code></b>: A dictionary (such as is produced by a chunker) with the following key/value pairs (shapes are given as required for training): TrainEvalFeatures.TIMES: A [batch size, self.window_size] integer Tensor with times for each observation. To train on longer sequences, the data should first be chunked. TrainEvalFeatures.VALUES: A [batch size, self.window_size, self.num_features] Tensor with values for each observation. When evaluating, <code>TIMES</code> and <code>VALUES</code> must have a window size of at least self.window_size, but it may be longer, in which case the last window_size - self.input_window_size times (or fewer if this is not divisible by self.output_window_size) will be evaluated on with non-overlapping output windows (and will have associated predictions). This is primarily to support qualitative evaluation/plotting, and is not a recommended way to compute evaluation losses (since there is no overlap in the output windows, which for window-based models is an undesirable bias).</li> <li>
<b><code>mode</code></b>: The tf.estimator.ModeKeys mode to use (TRAIN or EVAL).</li> <li>
<b><code>state</code></b>: Unused</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A model.ModelOutputs object.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If <code>mode</code> is not TRAIN or EVAL, or if static shape information is incorrect.</li> </ul> <h3 id="get_start_state"><code>get_start_state</code></h3> <pre class="prettyprint lang-python" data-language="python">get_start_state()
</pre> <p>Returns a tuple of state for the start of the time series.</p> <p>For example, a mean and covariance. State should not have a batch dimension, and will often be TensorFlow Variables to be learned along with the rest of the model parameters.</p> <h3 id="initialize_graph"><code>initialize_graph</code></h3> <pre class="prettyprint lang-python" data-language="python">initialize_graph(input_statistics=None)
</pre> <p>Define ops for the model, not depending on any previously defined ops.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>input_statistics</code></b>: A math_utils.InputStatistics object containing input statistics. If None, data-independent defaults are used, which may result in longer or unstable training.</li> </ul> <h3 id="loss_op"><code>loss_op</code></h3> <pre class="prettyprint lang-python" data-language="python">loss_op(
    targets,
    prediction_ops
)
</pre> <p>Create loss_op.</p> <h3 id="predict"><code>predict</code></h3> <pre class="prettyprint lang-python" data-language="python">predict(features)
</pre> <p>Computes predictions multiple steps into the future.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>features</code></b>: A dictionary with the following key/value pairs:</li> <li>
<b><code>PredictionFeatures.TIMES</code></b>: A [batch size, predict window size] integer Tensor of times, after the window of data indicated by <code>STATE_TUPLE</code>, to make predictions for.</li> <li>
<b><code>PredictionFeatures.STATE_TUPLE</code></b>: A tuple of (times, values), times with shape [batch size, self.input_window_size], values with shape [batch size, self.input_window_size, self.num_features] representing a segment of the time series before <code>TIMES</code>. This data is used to start of the autoregressive computation. This should have data for at least self.input_window_size timesteps.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>A dictionary with keys, "mean", "covariance". The values are Tensors of shape [batch_size, predict window size, num_features] and correspond to the values passed in <code>TIMES</code>.</p> <h3 id="prediction_ops"><code>prediction_ops</code></h3> <pre class="prettyprint lang-python" data-language="python">prediction_ops(
    times,
    values
)
</pre> <p>Compute model predictions given input data.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code>times</code></b>: A [batch size, self.window_size] integer Tensor, the first self.input_window_size times in each part of the batch indicating input features, and the last self.output_window_size times indicating prediction times.</li> <li>
<b><code>values</code></b>: A [batch size, self.input_window_size, self.num_features] Tensor with input features.</li> </ul> <h4 id="returns_4">Returns:</h4> <p>Tuple (predicted_mean, predicted_covariance), where each element is a Tensor with shape [batch size, self.output_window_size, self.num_features].</p> <h3 id="random_model_parameters"><code>random_model_parameters</code></h3> <pre class="prettyprint lang-python" data-language="python">random_model_parameters(seed=None)
</pre> <h2 id="class_members">Class Members</h2> <h3 id="NORMAL_LIKELIHOOD_LOSS"><code>NORMAL_LIKELIHOOD_LOSS</code></h3> <h3 id="SQUARED_LOSS"><code>SQUARED_LOSS</code></h3>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/timeseries/ARModel" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/contrib/timeseries/ARModel</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
