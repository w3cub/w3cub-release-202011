
<!DOCTYPE HTML>

<html lang="en" class="_theme-default">

<head>
  <meta charset="utf-8">
  <title>tf.Tensor - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Defined in tensorflow&#47;python&#47;framework&#47;ops.py. ">
  <meta name="keywords" content="tf, tensor, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="https://docs.w3cub.com/tensorflow~python/tf/tensor.html">
  <link href="/favicon.png" rel="icon">
  <link rel="stylesheet" type="text/css" href="/assets/application-e498cd0ebe8746846fec95b1a53ab3bb0fb7f47f794f0a38f44c98a1f0d03b21d777ae2c583732e44a5a890f6eacb79a5333545db9d5f3616091ba21ca17d916.css">
  <script src="/assets/application-79c555f6b25481fffac2cac30a7f3e54e608ca09e9e8e42bb1790095ba6d0fcace47d6bc624ddce952c70370892f2d46864f89e6943d4f7f7ff16c8a3231a91a.js" type="text/javascript"></script>
  <script src="/json/tensorflow~python.js"></script>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R3WC07G3GB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R3WC07G3GB');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2572770204602497"
     crossorigin="anonymous"></script>
<script async custom-element="amp-auto-ads"
  src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>


</head>

<body class="docs">
	<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-2572770204602497">
	</amp-auto-ads>
	<div class="_app">
	<header class="_header">

  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="https://tools.w3cub.com/?_sp=docs" target="_blank" class="_nav-link ">W3cubTools</a>
    <a href="/cheatsheets/" class="_nav-link ">Cheatsheets</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		
		<form class="_search">
		  <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
		  <a class="_search-clear"></a>
		  <div class="_search-tag"></div>
		</form>
		
		<div class="_list-wrap">
			<div class="_list">
			
			</div>
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="6861657091"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			<div class="_page _tensorflow">
				
				
<h1 itemprop="name" class="devsite-page-title"> tf.Tensor </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.Tensor"> <meta itemprop="path" content="r1.8"> <meta itemprop="property" content="device"> <meta itemprop="property" content="dtype"> <meta itemprop="property" content="graph"> <meta itemprop="property" content="name"> <meta itemprop="property" content="op"> <meta itemprop="property" content="shape"> <meta itemprop="property" content="value_index"> <meta itemprop="property" content="__abs__"> <meta itemprop="property" content="__add__"> <meta itemprop="property" content="__and__"> <meta itemprop="property" content="__bool__"> <meta itemprop="property" content="__div__"> <meta itemprop="property" content="__eq__"> <meta itemprop="property" content="__floordiv__"> <meta itemprop="property" content="__ge__"> <meta itemprop="property" content="__getitem__"> <meta itemprop="property" content="__gt__"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="__invert__"> <meta itemprop="property" content="__iter__"> <meta itemprop="property" content="__le__"> <meta itemprop="property" content="__lt__"> <meta itemprop="property" content="__matmul__"> <meta itemprop="property" content="__mod__"> <meta itemprop="property" content="__mul__"> <meta itemprop="property" content="__neg__"> <meta itemprop="property" content="__nonzero__"> <meta itemprop="property" content="__or__"> <meta itemprop="property" content="__pow__"> <meta itemprop="property" content="__radd__"> <meta itemprop="property" content="__rand__"> <meta itemprop="property" content="__rdiv__"> <meta itemprop="property" content="__rfloordiv__"> <meta itemprop="property" content="__rmatmul__"> <meta itemprop="property" content="__rmod__"> <meta itemprop="property" content="__rmul__"> <meta itemprop="property" content="__ror__"> <meta itemprop="property" content="__rpow__"> <meta itemprop="property" content="__rsub__"> <meta itemprop="property" content="__rtruediv__"> <meta itemprop="property" content="__rxor__"> <meta itemprop="property" content="__sub__"> <meta itemprop="property" content="__truediv__"> <meta itemprop="property" content="__xor__"> <meta itemprop="property" content="consumers"> <meta itemprop="property" content="eval"> <meta itemprop="property" content="get_shape"> <meta itemprop="property" content="set_shape"> <meta itemprop="property" content="OVERLOADABLE_OPERATORS"> <meta itemprop="property" content="__array_priority__"> </div> <h2 id="class_tensor">Class <code>Tensor</code>
</h2> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/framework/ops.py"><code>tensorflow/python/framework/ops.py</code></a>.</p> <p>See the guide: <a href="https://www.tensorflow.org/api_guides/python/framework#Core_graph_data_structures">Building Graphs &gt; Core graph data structures</a></p> <p>Represents one of the outputs of an <code>Operation</code>.</p> <p>A <code>Tensor</code> is a symbolic handle to one of the outputs of an <code>Operation</code>. It does not hold the values of that operation's output, but instead provides a means of computing those values in a TensorFlow <a href="session"><code>tf.Session</code></a>.</p> <p>This class has two primary purposes:</p> <ol> <li> <p>A <code>Tensor</code> can be passed as an input to another <code>Operation</code>. This builds a dataflow connection between operations, which enables TensorFlow to execute an entire <code>Graph</code> that represents a large, multi-step computation.</p> </li> <li> <p>After the graph has been launched in a session, the value of the <code>Tensor</code> can be computed by passing it to <a href="interactivesession#run"><code>tf.Session.run</code></a>. <code>t.eval()</code> is a shortcut for calling <code>tf.get_default_session().run(t)</code>.</p> </li> </ol> <p>In the following example, <code>c</code>, <code>d</code>, and <code>e</code> are symbolic <code>Tensor</code> objects, whereas <code>result</code> is a numpy array that stores a concrete value:</p> <pre class="prettyprint lang-python" data-language="python"># Build a dataflow graph.
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)

# Construct a `Session` to execute the graph.
sess = tf.Session()

# Execute the graph and store the value that `e` represents in `result`.
result = sess.run(e)
</pre> <h2 id="properties">Properties</h2> <h3 id="device"><code>device</code></h3> <p>The name of the device on which this tensor will be produced, or None.</p> <h3 id="dtype"><code>dtype</code></h3> <p>The <code>DType</code> of elements in this tensor.</p> <h3 id="graph"><code>graph</code></h3> <p>The <code>Graph</code> that contains this tensor.</p> <h3 id="name"><code>name</code></h3> <p>The string name of this tensor.</p> <h3 id="op"><code>op</code></h3> <p>The <code>Operation</code> that produces this tensor as an output.</p> <h3 id="shape"><code>shape</code></h3> <p>Returns the <code>TensorShape</code> that represents the shape of this tensor.</p> <p>The shape is computed using shape inference functions that are registered in the Op for each <code>Operation</code>. See <a href="tensorshape"><code>tf.TensorShape</code></a> for more details of what a shape represents.</p> <p>The inferred shape of a tensor is used to provide shape information without having to launch the graph in a session. This can be used for debugging, and providing early error messages. For example:</p> <pre class="prettyprint lang-python" data-language="python">c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])

print(c.shape)
==&gt; TensorShape([Dimension(2), Dimension(3)])

d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])

print(d.shape)
==&gt; TensorShape([Dimension(4), Dimension(2)])

# Raises a ValueError, because `c` and `d` do not have compatible
# inner dimensions.
e = tf.matmul(c, d)

f = tf.matmul(c, d, transpose_a=True, transpose_b=True)

print(f.shape)
==&gt; TensorShape([Dimension(3), Dimension(4)])
</pre> <p>In some cases, the inferred shape may have unknown dimensions. If the caller has additional information about the values of these dimensions, <code>Tensor.set_shape()</code> can be used to augment the inferred shape.</p> <h4 id="returns">Returns:</h4> <p>A <code>TensorShape</code> representing the shape of this tensor.</p> <h3 id="value_index"><code>value_index</code></h3> <p>The index of this tensor in the outputs of its <code>Operation</code>.</p> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    op,
    value_index,
    dtype
)
</pre> <p>Creates a new <code>Tensor</code>.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>op</code></b>: An <code>Operation</code>. <code>Operation</code> that computes this tensor.</li> <li>
<b><code>value_index</code></b>: An <code>int</code>. Index of the operation's endpoint that produces this tensor.</li> <li>
<b><code>dtype</code></b>: A <code>DType</code>. Type of elements stored in this tensor.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: If the op is not an <code>Operation</code>.</li> </ul> <h3 id="__abs__"><code>__abs__</code></h3> <pre class="prettyprint lang-python" data-language="python">__abs__(
    x,
    name=None
)
</pre> <p>Computes the absolute value of a tensor.</p> <p>Given a tensor <code>x</code> of complex numbers, this operation returns a tensor of type <code>float32</code> or <code>float64</code> that is the absolute value of each element in <code>x</code>. All elements in <code>x</code> must be complex numbers of the form \(a + bj\). The absolute value is computed as \( \sqrt{a^2 + b^2}\). For example:</p> <pre class="prettyprint lang-python" data-language="python">x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
tf.abs(x)  # [5.25594902, 6.60492229]
</pre> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> or <code>SparseTensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code> or <code>complex128</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_1">Returns:</h4> <p>A <code>Tensor</code> or <code>SparseTensor</code> the same size and type as <code>x</code> with absolute values. Note, for <code>complex64</code> or <code>complex128</code> input, the returned <code>Tensor</code> will be of type <code>float32</code> or <code>float64</code>, respectively.</p> <h3 id="__add__"><code>__add__</code></h3> <pre class="prettyprint lang-python" data-language="python">__add__(
    x,
    y
)
</pre> <p>Returns x + y element-wise.</p> <p><em>NOTE</em>: <code>Add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h3 id="__and__"><code>__and__</code></h3> <pre class="prettyprint lang-python" data-language="python">__and__(
    x,
    y
)
</pre> <p>Returns the truth value of x AND y element-wise.</p> <p><em>NOTE</em>: <code>LogicalAnd</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_3">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__bool__"><code>__bool__</code></h3> <pre class="prettyprint lang-python" data-language="python">__bool__()
</pre> <p>Dummy method to prevent a tensor from being used as a Python <code>bool</code>.</p> <p>This overload raises a <code>TypeError</code> when the user inadvertently treats a <code>Tensor</code> as a boolean (e.g. in an <code>if</code> statement). For example:</p> <pre class="prettyprint lang-python" data-language="python">if tf.constant(True):  # Will raise.
  # ...

if tf.constant(5) &lt; tf.constant(7):  # Will raise.
  # ...
</pre> <p>This disallows ambiguities between testing the Python value vs testing the dynamic condition of the <code>Tensor</code>.</p> <h4 id="raises_1">Raises:</h4> <p><code>TypeError</code>.</p> <h3 id="__div__"><code>__div__</code></h3> <pre class="prettyprint lang-python" data-language="python">__div__(
    x,
    y
)
</pre> <p>Divide two values using Python 2 semantics. Used for Tensor.<strong>div</strong>.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li> <li>
<b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_4">Returns:</h4> <p><code>x / y</code> returns the quotient of x and y.</p> <h3 id="__eq__"><code>__eq__</code></h3> <pre class="prettyprint lang-python" data-language="python">__eq__(other)
</pre> <p>Return self==value.</p> <h3 id="__floordiv__"><code>__floordiv__</code></h3> <pre class="prettyprint lang-python" data-language="python">__floordiv__(
    x,
    y
)
</pre> <p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p> <p>The same as <code>tf.div(x,y)</code> for integers, but uses <code>tf.floor(tf.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p> <p>Note that for efficiency, <code>floordiv</code> uses C semantics for negative numbers (unlike Python and Numpy).</p> <p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li> <li>
<b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_5">Returns:</h4> <p><code>x / y</code> rounded down (except possibly towards zero for negative integers).</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: If the inputs are complex.</li> </ul> <h3 id="__ge__"><code>__ge__</code></h3> <pre class="prettyprint lang-python" data-language="python">__ge__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &gt;= y) element-wise.</p> <p><em>NOTE</em>: <code>GreaterEqual</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_6">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__getitem__"><code>__getitem__</code></h3> <pre class="prettyprint lang-python" data-language="python">__getitem__(
    tensor,
    slice_spec,
    var=None
)
</pre> <p>Overload for Tensor.<strong>getitem</strong>.</p> <p>This operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a non-scalar tensor as input is not currently allowed.</p> <p>Some useful examples:</p> <pre class="prettyprint lang-python" data-language="python"># strip leading and trailing 2 elements
foo = tf.constant([1,2,3,4,5,6])
print(foo[2:-2].eval())  # =&gt; [3,4]

# skip every row and reverse every column
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[::2,::-1].eval())  # =&gt; [[3,2,1], [9,8,7]]

# Use scalar tensors as indices on both dimensions
print(foo[tf.constant(0), tf.constant(2)].eval())  # =&gt; 3

# Insert another dimension
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval()) # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[:, tf.newaxis, :].eval()) # =&gt; [[[1,2,3]], [[4,5,6]], [[7,8,9]]]
print(foo[:, :, tf.newaxis].eval()) # =&gt; [[[1],[2],[3]], [[4],[5],[6]],
[[7],[8],[9]]]

# Ellipses (3 equivalent operations)
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis, ...].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
</pre> <p>Notes: - <a href="newaxis"><code>tf.newaxis</code></a> is <code>None</code> as in NumPy. - An implicit ellipsis is placed at the end of the <code>slice_spec</code> - NumPy advanced indexing is currently not supported.</p> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code>tensor</code></b>: An ops.Tensor object.</li> <li>
<b><code>slice_spec</code></b>: The arguments to Tensor.<strong>getitem</strong>.</li> <li>
<b><code>var</code></b>: In the case of variable slice assignment, the Variable object to slice (i.e. tensor is the read-only view of this variable).</li> </ul> <h4 id="returns_7">Returns:</h4> <p>The appropriate slice of "tensor", based on "slice_spec".</p> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If a slice range is negative size.</li> <li>
<b><code>TypeError</code></b>: If the slice indices aren't int, slice, or Ellipsis.</li> </ul> <h3 id="__gt__"><code>__gt__</code></h3> <pre class="prettyprint lang-python" data-language="python">__gt__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &gt; y) element-wise.</p> <p><em>NOTE</em>: <code>Greater</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_8">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__invert__"><code>__invert__</code></h3> <pre class="prettyprint lang-python" data-language="python">__invert__(
    x,
    name=None
)
</pre> <p>Returns the truth value of NOT x element-wise.</p> <h4 id="args_9">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_9">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__iter__"><code>__iter__</code></h3> <pre class="prettyprint lang-python" data-language="python">__iter__()
</pre> <h3 id="__le__"><code>__le__</code></h3> <pre class="prettyprint lang-python" data-language="python">__le__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &lt;= y) element-wise.</p> <p><em>NOTE</em>: <code>LessEqual</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_10">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_10">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__lt__"><code>__lt__</code></h3> <pre class="prettyprint lang-python" data-language="python">__lt__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &lt; y) element-wise.</p> <p><em>NOTE</em>: <code>Less</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_11">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_11">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__matmul__"><code>__matmul__</code></h3> <pre class="prettyprint lang-python" data-language="python">__matmul__(
    x,
    y
)
</pre> <p>Multiplies matrix <code>a</code> by matrix <code>b</code>, producing <code>a</code> * <code>b</code>.</p> <p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p> <p>Both matrices must be of the same type. The supported types are: <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code>.</p> <p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code>True</code>. These are <code>False</code> by default.</p> <p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code>a_is_sparse</code> or <code>b_is_sparse</code> flag to <code>True</code>. These are <code>False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code>bfloat16</code> or <code>float32</code>.</p> <p>For example:</p> <pre class="prettyprint lang-python" data-language="python"># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)

# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
</pre> <h4 id="args_12">Args:</h4> <ul> <li>
<b><code>a</code></b>: <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code> and rank &gt; 1.</li> <li>
<b><code>b</code></b>: <code>Tensor</code> with same type and rank as <code>a</code>.</li> <li>
<b><code>transpose_a</code></b>: If <code>True</code>, <code>a</code> is transposed before multiplication.</li> <li>
<b><code>transpose_b</code></b>: If <code>True</code>, <code>b</code> is transposed before multiplication.</li> <li>
<b><code>adjoint_a</code></b>: If <code>True</code>, <code>a</code> is conjugated and transposed before multiplication.</li> <li>
<b><code>adjoint_b</code></b>: If <code>True</code>, <code>b</code> is conjugated and transposed before multiplication.</li> <li>
<b><code>a_is_sparse</code></b>: If <code>True</code>, <code>a</code> is treated as a sparse matrix.</li> <li>
<b><code>b_is_sparse</code></b>: If <code>True</code>, <code>b</code> is treated as a sparse matrix.</li> <li>
<b><code>name</code></b>: Name for the operation (optional).</li> </ul> <h4 id="returns_12">Returns:</h4> <p>A <code>Tensor</code> of the same type as <code>a</code> and <code>b</code> where each inner-most matrix is the product of the corresponding matrices in <code>a</code> and <code>b</code>, e.g. if all transpose or adjoint attributes are <code>False</code>:</p> <p><code>output</code>[..., i, j] = sum_k (<code>a</code>[..., i, k] * <code>b</code>[..., k, j]), for all indices i, j.</p> <ul> <li>
<b><code>Note</code></b>: This is matrix product, not element-wise product.</li> </ul> <h4 id="raises_4">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li> </ul> <h3 id="__mod__"><code>__mod__</code></h3> <pre class="prettyprint lang-python" data-language="python">__mod__(
    x,
    y
)
</pre> <p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p> <p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p> <p><em>NOTE</em>: <code>FloorMod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_13">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_13">Returns:</h4> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h3 id="__mul__"><code>__mul__</code></h3> <pre class="prettyprint lang-python" data-language="python">__mul__(
    x,
    y
)
</pre> <p>Dispatches cwise mul for "Dense<em>Dense" and "Dense</em>Sparse".</p> <h3 id="__neg__"><code>__neg__</code></h3> <pre class="prettyprint lang-python" data-language="python">__neg__(
    x,
    name=None
)
</pre> <p>Computes numerical negative value element-wise.</p> <p>I.e., \(y = -x\).</p> <h4 id="args_14">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_14">Returns:</h4> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h3 id="__nonzero__"><code>__nonzero__</code></h3> <pre class="prettyprint lang-python" data-language="python">__nonzero__()
</pre> <p>Dummy method to prevent a tensor from being used as a Python <code>bool</code>.</p> <p>This is the Python 2.x counterpart to <code>__bool__()</code> above.</p> <h4 id="raises_5">Raises:</h4> <p><code>TypeError</code>.</p> <h3 id="__or__"><code>__or__</code></h3> <pre class="prettyprint lang-python" data-language="python">__or__(
    x,
    y
)
</pre> <p>Returns the truth value of x OR y element-wise.</p> <p><em>NOTE</em>: <code>LogicalOr</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_15">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_15">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__pow__"><code>__pow__</code></h3> <pre class="prettyprint lang-python" data-language="python">__pow__(
    x,
    y
)
</pre> <p>Computes the power of one value to another.</p> <p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p> <pre class="prettyprint lang-python" data-language="python">x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
</pre> <h4 id="args_16">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_16">Returns:</h4> <p>A <code>Tensor</code>.</p> <h3 id="__radd__"><code>__radd__</code></h3> <pre class="prettyprint lang-python" data-language="python">__radd__(
    y,
    x
)
</pre> <p>Returns x + y element-wise.</p> <p><em>NOTE</em>: <code>Add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_17">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_17">Returns:</h4> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h3 id="__rand__"><code>__rand__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rand__(
    y,
    x
)
</pre> <p>Returns the truth value of x AND y element-wise.</p> <p><em>NOTE</em>: <code>LogicalAnd</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_18">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_18">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__rdiv__"><code>__rdiv__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rdiv__(
    y,
    x
)
</pre> <p>Divide two values using Python 2 semantics. Used for Tensor.<strong>div</strong>.</p> <h4 id="args_19">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li> <li>
<b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_19">Returns:</h4> <p><code>x / y</code> returns the quotient of x and y.</p> <h3 id="__rfloordiv__"><code>__rfloordiv__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rfloordiv__(
    y,
    x
)
</pre> <p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p> <p>The same as <code>tf.div(x,y)</code> for integers, but uses <code>tf.floor(tf.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p> <p>Note that for efficiency, <code>floordiv</code> uses C semantics for negative numbers (unlike Python and Numpy).</p> <p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p> <h4 id="args_20">Args:</h4> <ul> <li>
<b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li> <li>
<b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_20">Returns:</h4> <p><code>x / y</code> rounded down (except possibly towards zero for negative integers).</p> <h4 id="raises_6">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: If the inputs are complex.</li> </ul> <h3 id="__rmatmul__"><code>__rmatmul__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rmatmul__(
    y,
    x
)
</pre> <p>Multiplies matrix <code>a</code> by matrix <code>b</code>, producing <code>a</code> * <code>b</code>.</p> <p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p> <p>Both matrices must be of the same type. The supported types are: <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code>.</p> <p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code>True</code>. These are <code>False</code> by default.</p> <p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code>a_is_sparse</code> or <code>b_is_sparse</code> flag to <code>True</code>. These are <code>False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code>bfloat16</code> or <code>float32</code>.</p> <p>For example:</p> <pre class="prettyprint lang-python" data-language="python"># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)

# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
</pre> <h4 id="args_21">Args:</h4> <ul> <li>
<b><code>a</code></b>: <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code> and rank &gt; 1.</li> <li>
<b><code>b</code></b>: <code>Tensor</code> with same type and rank as <code>a</code>.</li> <li>
<b><code>transpose_a</code></b>: If <code>True</code>, <code>a</code> is transposed before multiplication.</li> <li>
<b><code>transpose_b</code></b>: If <code>True</code>, <code>b</code> is transposed before multiplication.</li> <li>
<b><code>adjoint_a</code></b>: If <code>True</code>, <code>a</code> is conjugated and transposed before multiplication.</li> <li>
<b><code>adjoint_b</code></b>: If <code>True</code>, <code>b</code> is conjugated and transposed before multiplication.</li> <li>
<b><code>a_is_sparse</code></b>: If <code>True</code>, <code>a</code> is treated as a sparse matrix.</li> <li>
<b><code>b_is_sparse</code></b>: If <code>True</code>, <code>b</code> is treated as a sparse matrix.</li> <li>
<b><code>name</code></b>: Name for the operation (optional).</li> </ul> <h4 id="returns_21">Returns:</h4> <p>A <code>Tensor</code> of the same type as <code>a</code> and <code>b</code> where each inner-most matrix is the product of the corresponding matrices in <code>a</code> and <code>b</code>, e.g. if all transpose or adjoint attributes are <code>False</code>:</p> <p><code>output</code>[..., i, j] = sum_k (<code>a</code>[..., i, k] * <code>b</code>[..., k, j]), for all indices i, j.</p> <ul> <li>
<b><code>Note</code></b>: This is matrix product, not element-wise product.</li> </ul> <h4 id="raises_7">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li> </ul> <h3 id="__rmod__"><code>__rmod__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rmod__(
    y,
    x
)
</pre> <p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p> <p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p> <p><em>NOTE</em>: <code>FloorMod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_22">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_22">Returns:</h4> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h3 id="__rmul__"><code>__rmul__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rmul__(
    y,
    x
)
</pre> <p>Dispatches cwise mul for "Dense<em>Dense" and "Dense</em>Sparse".</p> <h3 id="__ror__"><code>__ror__</code></h3> <pre class="prettyprint lang-python" data-language="python">__ror__(
    y,
    x
)
</pre> <p>Returns the truth value of x OR y element-wise.</p> <p><em>NOTE</em>: <code>LogicalOr</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_23">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_23">Returns:</h4> <p>A <code>Tensor</code> of type <code>bool</code>.</p> <h3 id="__rpow__"><code>__rpow__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rpow__(
    y,
    x
)
</pre> <p>Computes the power of one value to another.</p> <p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p> <pre class="prettyprint lang-python" data-language="python">x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
</pre> <h4 id="args_24">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_24">Returns:</h4> <p>A <code>Tensor</code>.</p> <h3 id="__rsub__"><code>__rsub__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rsub__(
    y,
    x
)
</pre> <p>Returns x - y element-wise.</p> <p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_25">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_25">Returns:</h4> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h3 id="__rtruediv__"><code>__rtruediv__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rtruediv__(
    y,
    x
)
</pre> <h3 id="__rxor__"><code>__rxor__</code></h3> <pre class="prettyprint lang-python" data-language="python">__rxor__(
    y,
    x
)
</pre> <p>x ^ y = (x | y) &amp; ~(x &amp; y).</p> <h3 id="__sub__"><code>__sub__</code></h3> <pre class="prettyprint lang-python" data-language="python">__sub__(
    x,
    y
)
</pre> <p>Returns x - y element-wise.</p> <p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_26">Args:</h4> <ul> <li>
<b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li> <li>
<b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li> <li>
<b><code>name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_26">Returns:</h4> <p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p> <h3 id="__truediv__"><code>__truediv__</code></h3> <pre class="prettyprint lang-python" data-language="python">__truediv__(
    x,
    y
)
</pre> <h3 id="__xor__"><code>__xor__</code></h3> <pre class="prettyprint lang-python" data-language="python">__xor__(
    x,
    y
)
</pre> <p>x ^ y = (x | y) &amp; ~(x &amp; y).</p> <h3 id="consumers"><code>consumers</code></h3> <pre class="prettyprint lang-python" data-language="python">consumers()
</pre> <p>Returns a list of <code>Operation</code>s that consume this tensor.</p> <h4 id="returns_27">Returns:</h4> <p>A list of <code>Operation</code>s.</p> <h3 id="eval"><code>eval</code></h3> <pre class="prettyprint lang-python" data-language="python">eval(
    feed_dict=None,
    session=None
)
</pre> <p>Evaluates this tensor in a <code>Session</code>.</p> <p>Calling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.</p> <p><em>N.B.</em> Before invoking <code>Tensor.eval()</code>, its graph must have been launched in a session, and either a default session must be available, or <code>session</code> must be specified explicitly.</p> <h4 id="args_27">Args:</h4> <ul> <li>
<b><code>feed_dict</code></b>: A dictionary that maps <code>Tensor</code> objects to feed values. See <a href="interactivesession#run"><code>tf.Session.run</code></a> for a description of the valid feed values.</li> <li>
<b><code>session</code></b>: (Optional.) The <code>Session</code> to be used to evaluate this tensor. If none, the default session will be used.</li> </ul> <h4 id="returns_28">Returns:</h4> <p>A numpy array corresponding to the value of this tensor.</p> <h3 id="get_shape"><code>get_shape</code></h3> <pre class="prettyprint lang-python" data-language="python">get_shape()
</pre> <p>Alias of Tensor.shape.</p> <h3 id="set_shape"><code>set_shape</code></h3> <pre class="prettyprint lang-python" data-language="python">set_shape(shape)
</pre> <p>Updates the shape of this tensor.</p> <p>This method can be called multiple times, and will merge the given <code>shape</code> with the current shape of this tensor. It can be used to provide additional information about the shape of this tensor that cannot be inferred from the graph alone. For example, this can be used to provide additional information about the shapes of images:</p> <pre class="prettyprint lang-python" data-language="python">_, image_data = tf.TFRecordReader(...).read(...)
image = tf.image.decode_png(image_data, channels=3)

# The height and width dimensions of `image` are data dependent, and
# cannot be computed without executing the op.
print(image.shape)
==&gt; TensorShape([Dimension(None), Dimension(None), Dimension(3)])

# We know that each image in this dataset is 28 x 28 pixels.
image.set_shape([28, 28, 3])
print(image.shape)
==&gt; TensorShape([Dimension(28), Dimension(28), Dimension(3)])
</pre> <h4 id="args_28">Args:</h4> <ul> <li>
<b><code>shape</code></b>: A <code>TensorShape</code> representing the shape of this tensor, a <code>TensorShapeProto</code>, a list, a tuple, or None.</li> </ul> <h4 id="raises_8">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If <code>shape</code> is not compatible with the current shape of this tensor.</li> </ul> <h2 id="class_members">Class Members</h2> <h3 id="OVERLOADABLE_OPERATORS"><code>OVERLOADABLE_OPERATORS</code></h3> <h3 id="__array_priority__"><code>__array_priority__</code></h3>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2018 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/Tensor</a>
  </p>
</div>

				
			</div>
			<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2572770204602497"
     data-ad-slot="1992473792"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
